[{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"when-does-each-win","dir":"Articles","previous_headings":"","what":"When does each win?","title":"Automatic Algorithm Selection in bigPLSR","text":"XtX (SIMPLS): moderate pp (fits p2p^2 RAM). Fast BLAS-3; excellent n‚â´pn \\gg p. XX·µó (wide-kernel): moderate nn (fits n2n^2). Great p‚â´np\\gg n (wide problems). NIPALS / streaming: p2p^2 n2n^2 exceed budget; supports file-backed scores large-scale chunked BLAS.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"sanity-check","dir":"Articles","previous_headings":"","what":"Sanity check","title":"Automatic Algorithm Selection in bigPLSR","text":"","code":"set.seed(1) n <- 1e5; p <- 200 X <- matrix(rnorm(n*p), n, p) y <- X[,1]*0.5 + rnorm(n) bmX <- bigmemory::as.big.matrix(X) bmy <- bigmemory::as.big.matrix(matrix(y, n, 1))  options(bigPLSR.memory_budget_bytes = 2L*1024^3) fit <- pls_fit(bmX, bmy, ncomp=3, backend=\"bigmem\", scores=\"r\") fit$algorithm"},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Automatic Algorithm Selection in bigPLSR","text":"bigPLSR::pls_fit() can automatically choose algorithm based problem shape user-configurable memory budget: SIMPLS (XtX route) forming p √ó p cross-product fits memory. SIMPLS (XXt / kernel route) XtX fit XXt (n √ó n) . NIPALS (streaming) neither XtX XXt comfortably fit. selection applies algorithm = \"auto\" (default). explicit algorithm = overrides decision.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"why-these-choices","dir":"Articles","previous_headings":"Overview","what":"Why these choices?","title":"Automatic Algorithm Selection in bigPLSR","text":"SIMPLS works entirely centered cross-products, fast numerically robust target cross-product fits (either p√óp n√ón). Using XtX efficient p moderate; using XXt efficient ‚Äúwide‚Äù problems (p ‚â´ n) still bounded n^2 memory. NIPALS avoids materializing large cross-product can stream big.memory fixed working memory; safe fallback memory tight.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"the-decision-rule","dir":"Articles","previous_headings":"","what":"The decision rule","title":"Automatic Algorithm Selection in bigPLSR","text":"Let memory budget B bytes (defaults 8 GB, configurable via options(bigPLSR.mem_budget_gb = ...)). doubles (8 bytes), estimate size symmetric matrix : need_XtX = 8 * p^2 need_XXt = 8 * n^2 :","code":"if (can_XtX && shape_XtX) { algo_in <- \"simpls\"}.    # XtX   if (can_XXt && shape_XXt) { algo_in <- \"widekernelpls\"}. XXt (a.k.a. \"kernel\" route)   else                      { algorithm <- \"nipals\"}         # streaming"},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"configuring-the-memory-budget","dir":"Articles","previous_headings":"","what":"Configuring the memory budget","title":"Automatic Algorithm Selection in bigPLSR","text":"change R‚Äôs actual memory limit; controls selection.","code":"# Use 16 GB as selection budget options(bigPLSR.mem_budget_gb = 16)"},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"reproducibility-knobs","dir":"Articles","previous_headings":"","what":"Reproducibility knobs","title":"Automatic Algorithm Selection in bigPLSR","text":"tight numerical parity tests:","code":"set.seed(1) if (requireNamespace(\"RhpcBLASctl\", quietly = TRUE)) {   RhpcBLASctl::blas_set_num_threads(1L)   RhpcBLASctl::omp_set_num_threads(1L) } # otherwise, you can try environment variables: # Sys.setenv(OPENBLAS_NUM_THREADS = \"1\", OMP_NUM_THREADS = \"1\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"Automatic Algorithm Selection in bigPLSR","text":"Wide case: Big-matrix streaming:","code":"library(bigPLSR)  n <- 2e3; p <- 5e2 X <- matrix(rnorm(n*p), n, p) y <- X[,1] - 0.5*X[,2] + rnorm(n)  # Auto will likely pick SIMPLS (XtX) here fit <- pls_fit(X, y, ncomp = 10, algorithm = \"auto\") fit$algorithm  # \"simpls\" n <- 200; p <- 4000 X <- matrix(rnorm(n*p), n, p) y <- rnorm(n)  # If budget is small, auto picks kernel (XXt) or NIPALS options(bigPLSR.mem_budget_gb = 2)  # small budget fit <- pls_fit(X, y, ncomp = 5, algorithm = \"auto\") fit$algorithm  # \"kernelpls\" or \"nipals\" depending on n^2 vs budget library(bigmemory) n <- 1e6; p <- 50 # (example only; allocate according to your RAM) # bmX <- big.matrix(n, p, type = \"double\") # bmy <- big.matrix(n, 1, type = \"double\") # fit <- pls_fit(bmX, bmy, ncomp = 10, backend = \"bigmem\", algorithm = \"auto\") # fit$algorithm  # \"simpls\" or \"nipals\""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Automatic Algorithm Selection in bigPLSR","text":"de Jong, S. (1993). SIMPLS: alternative approach partial least squares regression. Chemometrics Intelligent Laboratory Systems, 18(3), 251‚Äì263. Dayal, B., & MacGregor, J. F. (1997). Improved PLS algorithms. Journal Chemometrics, 11(1), 73‚Äì85. Rosipal, R., & Trejo, L. J. (2001). Kernel Partial Least Squares Regression Reproducing Kernel Hilbert Space. Journal Machine Learning Research, 2, 97‚Äì123. Wold, H. (1966, 1985). NIPALS algorithm (original PLS formulation).","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"appendix-streaming-gram-math","dir":"Articles","previous_headings":"","what":"Appendix: streaming Gram math","title":"Automatic Algorithm Selection in bigPLSR","text":"column blocks JJ, K‚âà‚àëJX[:,J]X[:,J]‚ä§,(Kv)‚Üê(Kv)+X[:,J](X[:,J]‚ä§v). K \\approx \\sum_{J} X_{[:,J]} X_{[:,J]}^\\top,\\quad (Kv) \\leftarrow (Kv) + X_{[:,J]} \\big(X_{[:,J]}^\\top v\\big). row blocks BB, K‚âà‚àëBXBX‚ä§,(Kv)‚Üê(Kv)+XB(X‚ä§v)B. K \\approx \\sum_{B} X_B X^\\top,\\quad (Kv) \\leftarrow (Kv) + X_B \\big(X^\\top v\\big)_B. Center fly: HKHv=Kv‚àí1nùüèùüè‚ä§Kv‚àí1nKùüèùüè‚ä§v+1n2ùüèùüè‚ä§Kùüèùüè‚ä§vH K H v = K v - \\tfrac{1}{n}\\mathbf{1}\\mathbf{1}^\\top K v - \\tfrac{1}{n}K\\mathbf{1}\\mathbf{1}^\\top v + \\tfrac{1}{n^2}\\mathbf{1}\\mathbf{1}^\\top K \\mathbf{1}\\,\\mathbf{1}^\\top v. Maintain needed aggregated vectors per pass.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-kpls-streaming.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Streaming Kernel PLS in bigPLSR: XX^T and Column-Chunked Variants","text":"vignette documents bigPLSR‚Äôs kernel PLS streaming backends bigmemory::big.matrix inputs. provide two complementary streaming strategies: Column-chunked Gram (existing): updates based per-column blocks form products involving K = X X^T implicitly. Row-chunked XX^T (new): computes = X^T u scanning rows blocks, emits t = X , enabling efficient access patterns n >> p storage layout favors row-contiguous slices (e.g., file-backed subsets). strategies produce model floating point round-. Selection automatic (see ?pls_fit) can forced via option options(bigPLSR.kpls_gram = \"rows\" | \"cols\" | \"auto\").","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-kpls-streaming.html","id":"math-sketch","dir":"Articles","previous_headings":"","what":"Math sketch","title":"Streaming Kernel PLS in bigPLSR: XX^T and Column-Chunked Variants","text":"Let X R^{n x p}, Y R^{n x m} centered. component h, kernel-PLS uses NIPALS-like fixed-point update Start u R^n (e.g., column Y). Compute = X^T u. Normalize w = / ||||_2. Scores: t = X w. p = (X^T t)/(t^T t), q = (Y^T t)/(t^T t). Deflate: X <- X - t p^T, Y <- Y - t q^T, set u <- Y q. Coefficients H components beta = W (P^T W)^{-1} Q^T, yhat = 1 * mu_Y + (x - mu_X) beta. row-chunked implementation keeps X disk performs steps (2) (4) two passes row blocks: Pass (accumulate ): block B rows, update += B^T u_B. Pass B (emit t): block B, write t_B = B * . Loadings p accumulated precisely like Pass t instead u.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-kpls-streaming.html","id":"apis","dir":"Articles","previous_headings":"","what":"APIs","title":"Streaming Kernel PLS in bigPLSR: XX^T and Column-Chunked Variants","text":"cpp_kpls_stream_xxt(X_ptr, Y_ptr, ncomp, chunk_rows, chunk_cols, center, return_big) cpp_kpls_stream_cols(X_ptr, Y_ptr, ncomp, chunk_cols, center, return_big) pls_fit(..., backend = \"bigmem\", algorithm = \"kernelpls\", chunk_size, chunk_cols, ...) pls_fit() chooses variant via options(bigPLSR.kpls_gram) heuristics \"auto\" set (default).","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-kpls-streaming.html","id":"when-to-prefer-each-variant","dir":"Articles","previous_headings":"","what":"When to prefer each variant","title":"Streaming Kernel PLS in bigPLSR: XX^T and Column-Chunked Variants","text":"Column-chunked (‚Äúcols‚Äù): good default; excellent p large access columns cheap (typical bigmemory column-major backing). Row-chunked XX^T (‚Äúrows‚Äù): prefer n >> p, row access contiguous (e.g., file-backed partitions), want minimize repeated column-touching across iterations.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-kpls-streaming.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Streaming Kernel PLS in bigPLSR: XX^T and Column-Chunked Variants","text":"Dayal, B., & MacGregor, J.F. (1997). Improved PLS algorithms. Journal Chemometrics, 11(1), 73‚Äì85. Rosipal, R., & Trejo, L.J. (2001). Kernel Partial Least Squares Regression Reproducing Kernel Hilbert Space. JMLR, 2, 97‚Äì123. (kernel/logistic/sparse KPLS references kpls_review vignette)","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/double-rkhs-pls.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"double-rkhs-pls","text":"implement double RKHS variant PLS, input output spaces endowed reproducing kernels: KX‚àà‚Ñùn√ónK_X \\\\mathbb{R}^{n\\times n} entries [KX]ij=kX(xi,xj)[K_X]_{ij} = k_X(x_i, x_j), KY‚àà‚Ñùn√ónK_Y \\\\mathbb{R}^{n\\times n} entries [KY]ij=kY(yi,yj)[K_Y]_{ij} = k_Y(y_i, y_j). use centered Grams KÃÉX=HKXH\\tilde K_X = H K_X H KÃÉY=HKYH\\tilde K_Y = H K_Y H, H=‚àí1nùüèùüè‚ä§H = - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/double-rkhs-pls.html","id":"operator-and-latent-directions","dir":"Articles","previous_headings":"Overview","what":"Operator and Latent Directions","title":"double-rkhs-pls","text":"Following spirit Kernel PLS Regression II (IEEE TNNLS, 2019), avoid explicit square roots form SPD surrogate operator ‚Ñ≥v=(KX+ŒªxI)‚àí1KXKYKX(KX+ŒªxI)‚àí1v, \\mathcal{M} \\, v = (K_X+\\lambda_x )^{-1} \\; K_X \\; K_Y \\; K_X \\; (K_X+\\lambda_x )^{-1} \\, v,  small ridge Œªx>0\\lambda_x > 0 stability. compute first AA orthonormal latent directions T=[t1,‚Ä¶,tA]T = [t_1,\\dots,t_A] via power iteration Gram‚ÄìSchmidt orthogonalization ‚Ñ≥\\mathcal{M}. solve small regression latent space: C=(T‚ä§T)‚àí1(T‚ä§YÃÉ),YÃÉ=Y‚àíùüèy‚Äæ‚ä§, C = (T^\\top T)^{-1} (T^\\top \\tilde Y), \\qquad \\tilde Y = Y - \\mathbf{1} \\bar y^\\top,  form dual coefficients Œ±=UC,U=(KX+ŒªxI)‚àí1T, \\alpha \\;=\\; U \\, C, \\qquad U \\;=\\; (K_X+\\lambda_x )^{-1} T,  training predictions satisfy YÃÇ=KÃÉXŒ±+ùüèy‚Äæ‚ä§. \\hat Y \\;=\\; \\tilde K_X \\, \\alpha + \\mathbf{1}\\,\\bar y^\\top .","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/double-rkhs-pls.html","id":"centering-for-prediction","dir":"Articles","previous_headings":"Overview","what":"Centering for Prediction","title":"double-rkhs-pls","text":"Given new inputs $X_\\*$, define cross-Gram $$ K_\\* = K(X_\\*, X) . $$ apply training centering $K_\\*$, use $$ \\tilde K_\\* \\;=\\; K_\\* \\;-\\; \\mathbf{1}\\, \\bar k_X^\\top \\;-\\; \\bar k_\\* \\mathbf{1}^\\top \\;+\\; \\mu_X, $$ : - k‚ÄæX=1nùüè‚ä§KX\\bar k_X = \\frac{1}{n}\\mathbf{1}^\\top K_X column mean vector (uncentered) training Gram, - ŒºX=1n2ùüè‚ä§KXùüè\\mu_X = \\frac{1}{n^2} \\mathbf{1}^\\top K_X \\mathbf{1} grand mean, - $\\bar k_\\*$ row mean $K_\\*$ (computed prediction time). Predictions follow familiar dual form: $$ \\hat Y_\\* \\;=\\; \\tilde K_\\* \\, \\alpha + \\mathbf{1}_\\* \\, \\bar y^\\top . $$","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/double-rkhs-pls.html","id":"practical-notes","dir":"Articles","previous_headings":"Overview","what":"Practical Notes","title":"double-rkhs-pls","text":"Choose kXk_X (e.g., RBF) reflect nonlinear structure inputs. linear kYk_Y already produces numeric outputs ‚Ñùm\\mathbb{R}^m. ridge terms Œªx,Œªy\\lambda_x, \\lambda_y stabilize inversions dampen numerical noise. dual_coef =Œ±=\\alpha, scores =T=T (approximately orthonormal), intercept =y‚Äæ=\\bar y, uses centered cross-kernel formula predict().","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/double-rkhs-pls.html","id":"minimal-example","dir":"Articles","previous_headings":"Overview","what":"Minimal Example","title":"double-rkhs-pls","text":"References ‚Ä¢ Rosipal & Trejo (2001) Kernel Partial Least Squares Regression Reproducing Kernel Hilbert Space. JMLR 2:97‚Äì123. doi:10.5555/944733.944741. ‚Ä¢ Kernel PLS Regression II: Kernel Partial Least Squares Regression Projecting Independent Dependent Variables Reproducing Kernel Hilbert Space. IEEE TNNLS (2019). doi:10.1109/TNNLS.2019.2932014.","code":"library(bigPLSR) set.seed(42) n <- 60; p <- 6; m <- 2 X <- matrix(rnorm(n * p), n, p) Y <- cbind(sin(X[,1]) + 0.4 * X[,2]^2,            cos(X[,3]) - 0.3 * X[,4]^2) + matrix(rnorm(n*m, sd=.05), n, m)  op <- options(   bigPLSR.rkhs_xy.kernel_x = \"rbf\",   bigPLSR.rkhs_xy.gamma_x  = 0.5,   bigPLSR.rkhs_xy.kernel_y = \"linear\",   bigPLSR.rkhs_xy.lambda_x = 1e-6,   bigPLSR.rkhs_xy.lambda_y = 1e-6 ) on.exit(options(op), add = TRUE)  fit <- pls_fit(X, Y, ncomp = 3, algorithm = \"rkhs_xy\", backend = \"arma\") Yhat <- predict(fit, X) mean((Y - Yhat)^2) ## [1] 2.619847e-12"},{"path":"https://fbertran.github.io/bigPLSR/articles/kf-pls.html","id":"idea","dir":"Articles","previous_headings":"","what":"Idea","title":"KF-PLS: Streaming PLS with Kalman-style updates","text":"maintain exponentially-weighted cross-products ùêÇxx‚ÜêŒªùêÇxx+ùêób‚ä§ùêób+qùêà,ùêÇxy‚ÜêŒªùêÇxy+ùêób‚ä§ùêòb, \\mathbf{C}_{xx} \\leftarrow \\lambda\\,\\mathbf{C}_{xx} + \\mathbf{X}_b^\\top\\mathbf{X}_b + q\\,\\mathbf{},\\qquad \\mathbf{C}_{xy} \\leftarrow \\lambda\\,\\mathbf{C}_{xy} + \\mathbf{X}_b^\\top\\mathbf{Y}_b,  mini-batches bb rows, 0<Œª‚â§10<\\lambda\\le 1 forgetting factor q‚â•0q\\ge 0 small process-noise ridge. time extract latent components via SIMPLS (ùêÇxx,ùêÇxy)(\\mathbf{C}_{xx},\\mathbf{C}_{xy}). stable, fast, matches Kalman-style tracking slowly varying covariance structure.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kf-pls.html","id":"api","dir":"Articles","previous_headings":"","what":"API","title":"KF-PLS: Streaming PLS with Kalman-style updates","text":"bigmem, cross-products streamed row chunks; scores ùêì\\mathbf{T} produced via package‚Äôs chunked score kernel.","code":"fit <- pls_fit(X, Y, ncomp = 3,                backend   = \"arma\"  # or \"bigmem\"                ,algorithm = \"kf_pls\",                scores    = \"r\",                tol = 1e-8,                # tuning:                # options(bigPLSR.kf.lambda = 0.995,                #         bigPLSR.kf.q_proc = 1e-6) )"},{"path":"https://fbertran.github.io/bigPLSR/articles/kf-pls.html","id":"notes","dir":"Articles","previous_headings":"","what":"Notes","title":"KF-PLS: Streaming PLS with Kalman-style updates","text":"Œª‚Üí1\\lambda\\1 q‚Üí0q\\0 recovers batch SIMPLS. Smaller Œª\\lambda emphasizes recent batches (concept drift). qq stabilizes ill-conditioned ùêÇxx\\mathbf{C}_{xx} high-dimensional data.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/klogitpls.html","id":"kernel-logistic-pls-klogitpls","dir":"Articles","previous_headings":"","what":"Kernel Logistic PLS (klogitpls)","title":"klogitpls","text":"first extract latent scores Kernel PLS (KPLS): T=KcU, T = K_c U, Kc=HK(X,X)HK_c = H K(X,X) H centered Gram matrix columns UU dual score directions (KPLS deflation). fit logistic link latent space using IRLS: Œ∑=Œ≤0+TŒ≤,p=œÉ(Œ∑), \\eta = \\beta_0 + T \\beta, \\qquad p = \\sigma(\\eta), W=diag(p(1‚àíp)),z=Œ∑+y‚àípp(1‚àíp). W = \\mathrm{diag}(p (1-p)), \\qquad z = \\eta + \\frac{y - p}{p(1-p)}. iteration, solve weighted least squares system [Œ≤0,Œ≤][\\beta_0, \\beta]: (MÃÉ‚ä§MÃÉ)Œ∏=MÃÉ‚ä§zÃÉ,MÃÉ=W1/2[1,T],zÃÉ=W1/2z. (\\tilde{M}^\\top \\tilde{M}) \\theta = \\tilde{M}^\\top \\tilde{z}, \\quad \\tilde{M} = W^{1/2}[1, T], \\ \\tilde{z} = W^{1/2} z. Optionally, alternate: replace yy pp recompute KPLS refresh TT steps. Prediction new data uses centered cross-kernel $K_c(X_\\*, X)$ stored KPLS basis UU: $$ T_\\* = K_c(X_\\*, X) \\, U, \\qquad \\hat{p}_\\* = \\sigma\\!\\big(\\beta_0 + T_\\* \\beta\\big). $$","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"notation","dir":"Articles","previous_headings":"","what":"Notation","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Let X‚àà‚Ñùn√ópX \\\\mathbb{R}^{n\\times p} Y‚àà‚Ñùn√ó\\\\mathbb{R}^{n\\times m}. assume column-centered data unless stated otherwise. PLS extracts latent scores T=[t1,‚Ä¶,ta]T=[t_1,\\dots,t_a] loadings weights covariance XX YY along tat_a maximized, orthogonality constraints across components. kernel methods, let œï(‚ãÖ)\\phi(\\cdot) implicit feature map define Gram matrix KX=Œ¶XŒ¶X‚ä§K_X = \\Phi_X \\Phi_X^\\top (KX)ij=k(xi,xj)(K_X)_{ij} = k(x_i, x_j). centering operator H=‚àí1nùüèùüè‚ä§H = I_n - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top yields centered Gram KÃÉX=HKXH\\tilde K_X = H K_X H. Common kernels: Linear:k(x,z)=x‚ä§zRBF:k(x,z)=exp(‚àíŒ≥‚à•x‚àíz‚à•2)Polynomial:k(x,z)=(Œ≥x‚ä§z+c0)dSigmoid:k(x,z)=tanh(Œ≥x‚ä§z+c0). \\begin{aligned} \\text{Linear:}\\quad& k(x,z) = x^\\top z \\\\ \\text{RBF:}\\quad& k(x,z) = \\exp(-\\gamma \\|x-z\\|^2) \\\\ \\text{Polynomial:}\\quad& k(x,z) = (\\gamma\\,x^\\top z + c_0)^{d} \\\\ \\text{Sigmoid:}\\quad& k(x,z) = \\tanh(\\gamma\\,x^\\top z + c_0). \\end{aligned}","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"centering-the-gram-matrix","dir":"Articles","previous_headings":"Notation","what":"Centering the Gram matrix","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Given K‚àà‚Ñùn√ónK\\\\mathbb{R}^{n\\times n}, centered version : KÃÉ=HKH,H=‚àí1nùüèùüè‚ä§. \\tilde K = H K H, \\quad H = I_n - \\tfrac{1}{n}\\mathbf{1}\\mathbf{1}^\\top.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"klpls-kernel-pls-dayal-macgregor","dir":"Articles","previous_headings":"","what":"KLPLS / Kernel PLS (Dayal & MacGregor)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"operate dual. Consider KXK_X KXY=KXYK_{XY} = K_X Y. step aa, extract dual direction Œ±a\\alpha_a score ta=KÃÉXŒ±at_a = \\tilde K_X \\alpha_a maximizes covariance YY, subject orthogonality RKHS metric: maxŒ±cov(t,Y)s.t.t=KÃÉXŒ±,t‚ä§t=1,t‚ä§tb=0,b<. \\max_{\\alpha} \\ \\mathrm{cov}(t, Y) \\quad \\text{s.t.}\\ \\ t=\\tilde K_X \\alpha,\\ \\ t^\\top t = 1,\\ \\ t^\\top t_b = 0,\\ b<. SIMPLS-style recursion dual: Compute cross-covariance operator C=KÃÉXYC = \\tilde K_X Y. Extract direction ‚Ñùn\\mathbb{R}^n via dominant eigenvector CC‚ä§C C^\\top power iterations. Set ta=KÃÉXŒ±at_a = \\tilde K_X \\alpha_a, normalize tat_a. Regress YY tat_a: qa=(ta‚ä§ta)‚àí1ta‚ä§Yq_a = (t_a^\\top t_a)^{-1} t_a^\\top Y. Deflate Y‚ÜêY‚àítaqa‚ä§Y \\leftarrow Y - t_a q_a^\\top orthogonalize subsequent directions KÃÉX\\tilde K_X-metric. Prediction uses dual coefficients; new x‚ãÜx_\\star, k‚ãÜ=[k(x‚ãÜ,xi)]=1nk_\\star = [k(x_\\star, x_i)]_{=1}^n t‚ãÜ=kÃÉ‚ãÜ‚ä§Œ±t_\\star = \\tilde k_\\star^\\top \\alpha. YY multivariate, apply steps component-wise shared tat_a. bigPLSR - Dense path: algorithm=\"rkhs\" builds KÃÉX\\tilde K_X (approximation) runs dual SIMPLS deflation. - Big-matrix path: block-streamed Gram computations avoid materializing n√ónn\\times n.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"streaming-gram-blocks-column--and-row-chunked","dir":"Articles","previous_headings":"","what":"Streaming Gram blocks (column- and row-chunked)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"avoid forming KÃÉX\\tilde K_X explicitly accumulating blocks. Write KX=‚àëBXBX‚ä§K_X = \\sum_{B} X_B X^\\top blocks XBX_B taken rows (row-chunked/XX·µó) KX=XX‚ä§=‚àëCXC‚ä§K_X = X X^\\top = \\sum_{C} X C^\\top column chunks via XC‚ä§X C^\\top CC column submatrices (useful tall-skinny XX). Row-chunked (XX·µó): 1. blocks B‚äÇ{1,‚Ä¶,n}B \\subset \\{1,\\ldots,n\\}: compute GB=XBX‚ä§G_B = X_B X^\\top. 2. Accumulate K‚ÜêK+HGBHK \\leftarrow K + H G_B H fly needed matrix-vector products (Kv)(K v) without storing full KK. Column-chunked: 1. Partition feature dimension pp blocks JJ. 2. block JJ: GJ=X[:,J]X[:,J]‚ä§G_J = X_{[:,J]} X_{[:,J]}^\\top. 3. Use GJG_J update KvK v accumulators refresh deflation quantities (t,qt, q). Memory - Row-chunked: O(n‚ãÖchunk_rows)O(n \\cdot \\text{chunk\\_rows}). - Column-chunked: O(n‚ãÖchunk_cols)O(n \\cdot \\text{chunk\\_cols}). Pick based layout cache friendliness.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"kernel-approximations-nystr√∂m-and-random-fourier-features","dir":"Articles","previous_headings":"","what":"Kernel approximations: Nystr√∂m and Random Fourier Features","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Nystr√∂m (rank rr) Sample subset SS size rr, form KSSK_{SS} KNSK_{NS}. Define sketch Z=KNSKSS‚àí1/2Z = K_{NS} K_{SS}^{-1/2}, K‚âàZZ‚ä§K \\approx Z Z^\\top. Center ZZ subtracting row/column means. Run linear PLS ZZ. RFF (RBF kernels) Draw {œâ‚Ñì}‚Ñì=1r‚àºùí©(0,2Œ≥I)\\{\\omega_\\ell\\}_{\\ell=1}^r \\sim \\mathcal{N}(0,2\\gamma ) b‚Ñì‚àºùí∞[0,2œÄ]b_\\ell\\sim \\mathcal{U}[0,2\\pi]. Define features œÜ‚Ñì(x)=2rcos(œâ‚Ñì‚ä§x+b‚Ñì)\\varphi_\\ell(x)=\\sqrt{\\tfrac{2}{r}}\\cos(\\omega_\\ell^\\top x + b_\\ell), k(x,z)‚âàœÜ(x)‚ä§œÜ(z)k(x,z)\\approx \\varphi(x)^\\top \\varphi(z). Run linear PLS œÜ(X)\\varphi(X).","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"kernel-logistic-pls-binary-classification","dir":"Articles","previous_headings":"","what":"Kernel Logistic PLS (binary classification)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"first compute KPLS scores T‚àà‚Ñùn√ó\\\\mathbb{R}^{n\\times } XX vs labels y‚àà{0,1}y\\\\{0,1\\}, run logistic regression latent space via IRLS: Minimize ‚Ñì(Œ≤,Œ≤0)=‚àí‚àë{yiŒ∑i‚àílog(1+expŒ∑i)}\\ell(\\beta, \\beta_0) = -\\sum_i \\{ y_i\\eta_i - \\log(1+\\exp\\eta_i)\\} Œ∑=Œ≤0ùüè+TŒ≤\\eta = \\beta_0\\mathbf{1} + T \\beta. IRLS step iteration kk: W=diag(p(k)(1‚àíp(k))),z=Œ∑(k)+W‚àí1(y‚àíp(k)),[Œ≤0Œ≤]=(XŒ∑‚ä§WXŒ∑+ŒªI)‚àí1XŒ∑‚ä§Wz, W = \\mathrm{diag}(p^{(k)}(1-p^{(k)})),\\quad z = \\eta^{(k)} + W^{-1}(y - p^{(k)}),\\quad \\begin{bmatrix}\\beta_0\\\\ \\beta\\end{bmatrix} = (X_\\eta^\\top W X_\\eta + \\lambda )^{-1} X_\\eta^\\top W z, XŒ∑=[ùüè,T]X_\\eta = [\\mathbf{1}, T] p(k)=œÉ(Œ∑(k))p^{(k)} = \\sigma(\\eta^{(k)}). Optionally alternate: recompute KPLS scores current residuals re-run IRLS steps. Class weights wcw_c can injected scaling rows WW. bigPLSRalgorithm=\"klogitpls\" computes TT (dense streamed) fits IRLS latent space.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"sparse-kernel-pls-sketch","dir":"Articles","previous_headings":"","what":"Sparse Kernel PLS (sketch)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Promote sparsity dual primal weights. dual form, constrain Œ±a\\alpha_a ‚Ñì1\\ell_1 (group) penalty: maxŒ±cov(KÃÉŒ±,Y)‚àíŒª‚à•Œ±‚à•1s.t.(KÃÉŒ±)‚ä§(KÃÉŒ±)=1,ta‚ä§tb=0(b<). \\max_{\\alpha}\\ \\mathrm{cov}(\\tilde K \\alpha, Y) - \\lambda\\|\\alpha\\|_1 \\quad\\text{s.t.}\\quad (\\tilde K\\alpha)^\\top (\\tilde K\\alpha) = 1,\\ t_a^\\top t_b=0\\ (b<). practical approach uses proximal gradient coordinate descent smooth surrogate covariance, periodic orthogonalization resulting score vectors KÃÉ\\tilde K metric. Early stop explained covariance. (current release provides scaffolding API.)","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"pls-in-rkhs-for-x-and-y-double-rkhs","dir":"Articles","previous_headings":"","what":"PLS in RKHS for X and Y (double RKHS)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Let KXK_X KYK_Y centered Grams XX YY (small ridge ŒªX,ŒªY\\lambda_X,\\lambda_Y stability). cross-covariance operator =KX(KY+ŒªYI)KXA = K_X (K_Y + \\lambda_Y ) K_X. Dual SIMPLS extracts latent directions via dominant eigenspace AA orthogonalization KXK_X inner product. Prediction returns dual coefficients Œ±\\alpha XX Œ≤\\beta YY. bigPLSRalgorithm=\"rkhs_xy\" wires dense mode; streamed variant can built block Gram accumulations KXK_X KYK_Y.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"kalman-filter-pls-kf-pls-streaming","dir":"Articles","previous_headings":"","what":"Kalman-Filter PLS (KF-PLS; streaming)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"KF-PLS maintains state tracks latent parameters incoming mini-batches. Let state s={w,p,q,b}s = \\{w, p, q, b\\} current component, state transition sk+1=sk+œµks_{k+1} = s_k + \\epsilon_k (random walk) ‚Äúmeasurement‚Äù formed current block cross-covariances (X‚ä§tÃÇ,Y‚ä§tÃÇ\\widehat{X^\\top t}, \\widehat{Y^\\top t}). Kalman update: Predict: sÃÇk|k‚àí1=sÃÇk‚àí1,Pk|k‚àí1=Pk‚àí1+QInnovation: ŒΩk=zk‚àíHksÃÇk|k‚àí1,Sk=HkPk|k‚àí1Hk‚ä§+RGain: Kk=Pk|k‚àí1Hk‚ä§Sk‚àí1Update: sÃÇk=sÃÇk|k‚àí1+KkŒΩk,Pk=(‚àíKkHk)Pk|k‚àí1. \\begin{aligned} &\\text{Predict: } \\hat s_{k|k-1} = \\hat s_{k-1},\\ \\ P_{k|k-1}=P_{k-1}+Q \\\\ &\\text{Innovation: } \\nu_k = z_k - H_k \\hat s_{k|k-1},\\ \\ S_k = H_k P_{k|k-1} H_k^\\top + R \\\\ &\\text{Gain: } K_k = P_{k|k-1} H_k^\\top S_k^{-1} \\\\ &\\text{Update: } \\hat s_k = \\hat s_{k|k-1} + K_k \\nu_k,\\ \\ P_k = (- K_k H_k) P_{k|k-1}. \\end{aligned} convergence (patience stop), form t=Xwt = X w, normalize, proceed next component deflation compatible SIMPLS/NIPALS choice. bigPLSRalgorithm=\"kf_pls\" reuses existing chunked T streaming kernel updates state per block.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"api-quick-start","dir":"Articles","previous_headings":"","what":"API quick start","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"","code":"library(bigPLSR)  # Dense RKHS PLS with Nystr√∂m of rank 500 (rbf kernel) fit_rkhs <- pls_fit(X, Y, ncomp = 5,                     backend   = \"arma\",                     algorithm = \"rkhs\",                     kernel = \"rbf\", gamma = 0.5,                     approx = \"nystrom\", approx_rank = 500,                     scores = \"r\")  # Bigmemory, kernel logistic PLS (streamed scores + IRLS) fit_klog <- pls_fit(bmX, bmy, ncomp = 4,                     backend   = \"bigmem\",                     algorithm = \"klogitpls\",                     kernel = \"rbf\", gamma = 1.0,                     chunk_size = 16384L,                     scores = \"r\")  # Sparse KPLS (dense scaffold) fit_sk <- pls_fit(X, Y, ncomp = 5,                   backend = \"arma\",                   algorithm = \"sparse_kpls\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"prediction-in-rkhs-pls","dir":"Articles","previous_headings":"API quick start","what":"Prediction in RKHS PLS","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Let X‚àà‚Ñùn√ópX\\\\mathbb{R}^{n\\times p} training inputs Y‚àà‚Ñùn√ó\\\\mathbb{R}^{n\\times m} responses. kernel k(‚ãÖ,‚ãÖ)k(\\cdot,\\cdot) training Gram KK, centered Gram Kc=K‚àíùüènk‚Äæ‚ä§‚àík‚Äæùüèn‚ä§+k‚Äæ‚Äæ,k‚Äæ=1n‚àë=1nKi‚ãÖ,k‚Äæ‚Äæ=1n2‚àë,jKij. K_c = K - \\mathbf{1}_n \\bar{k}^\\top - \\bar{k}\\,\\mathbf{1}_n^\\top + \\bar{\\bar{k}}, \\quad \\bar{k} = \\frac{1}{n}\\sum_{=1}^n K_{\\cdot},\\quad \\bar{\\bar{k}} = \\frac{1}{n^2}\\sum_{,j}K_{ij}.  KPLS KcK_c yields dual coefficients ‚àà‚Ñùn√ómA\\\\mathbb{R}^{n\\times m}. new inputs $X_\\*$, cross-kernel $K_\\* \\\\mathbb{R}^{n_\\*\\times n}$ $(K_\\*)_{,j} = k(x^\\*_i, x_j)$. centered cross-Gram $$ K_{\\*,c} = K_\\* - \\mathbf{1}_{n_\\*}\\bar{k}^\\top - \\bar{k}_\\*\\mathbf{1}_n^\\top + \\bar{\\bar{k}}, \\quad \\bar{k}_\\* = \\frac{1}{n}K_\\*\\mathbf{1}_n. $$ Predictions follow $$ \\widehat{Y}_\\* = K_{\\*,c}\\,+ \\mathbf{1}_{n_\\*}\\,\\mu_Y^\\top, $$ ŒºY\\mu_Y vector training response means. bigPLSR, stored : dual_coef (AA), k_colmeans (k‚Äæ\\bar{k}), k_mean (k‚Äæ‚Äæ\\bar{\\bar{k}}), y_means (ŒºY\\mu_Y), X_ref (dense training inputs). RKHS branch predict.big_plsr() uses formula.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"dependency-overview-wrappers-c-entry-points","dir":"Articles","previous_headings":"","what":"Dependency overview (wrappers ‚Üí C++ entry points)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"","code":"pls_fit(algorithm=\"simpls\", backend=\"arma\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_simpls_from_cross\")  pls_fit(algorithm=\"simpls\", backend=\"bigmem\")   ‚îú‚îÄ .Call(\"_bigPLSR_cpp_bigmem_cross\")   ‚îú‚îÄ .Call(\"_bigPLSR_cpp_simpls_from_cross\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_stream_scores_given_W\")  pls_fit(algorithm=\"nipals\", backend=\"arma\")   ‚îî‚îÄ cpp_dense_plsr_nipals()  pls_fit(algorithm=\"nipals\", backend=\"bigmem\")   ‚îî‚îÄ big_plsr_stream_fit_nipals()  pls_fit(algorithm=\"kernelpls\"/\"widekernelpls\")   ‚îî‚îÄ .kernel_pls_core()  (R)  pls_fit(algorithm=\"rkhs\", backend=\"arma\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_kpls_rkhs_dense\")  pls_fit(algorithm=\"rkhs\", backend=\"bigmem\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_kpls_rkhs_bigmem\")  pls_fit(algorithm=\"klogitpls\", backend=\"arma\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_klogit_pls_dense\")  pls_fit(algorithm=\"klogitpls\", backend=\"bigmem\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_klogit_pls_bigmem\")  pls_fit(algorithm=\"sparse_kpls\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_sparse_kpls_dense\")  pls_fit(algorithm=\"rkhs_xy\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_rkhs_xy_dense\")  pls_fit(algorithm=\"kf_pls\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_kf_pls_stream\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Dayal, B., & MacGregor, J.F. (1997). Improved PLS algorithms. Journal Chemometrics, 11(1), 73‚Äì85, doi:10.1002/(SICI)1099-128X(199701)11:1%3C73::AID-CEM446%3E3.0.CO;2-2. Rosipal, R., & Trejo, L.J. (2001). Kernel PLS regression RKHS. Journal Machine Learning Research, 2, 97‚Äì123, doi:10.1162/153244302760200687. Tenenhaus et al., Kernel Logistic PLS. Sparse Kernel Partial Least Squares Regression. LNCS Proceedings. Rosipal et al., RKHS PLS (JMLR) http://www.jmlr.org/papers/v2/rosipal01a.html. Kernel PLS Regression II (double RKHS). IEEE Transactions Neural Networks Learning Systems, doi:10.1109/TNNLS.2019.2932014. KF-PLS (2024) doi:10.1016/j.chemolab.2024.104024","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/pls1-benchmark.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Benchmarking PLS1 Implementations","text":"vignette illustrates benchmark dense streaming implementations provided single-response partial least squares regression (PLS1). dense implementation (pls1_dense) copies predictor matrix memory therefore preferred whole dataset fits RAM. streaming implementation (pls1_stream) iterates data blocks, enabling analyses larger--memory workloads.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/pls1-benchmark.html","id":"simulated-data","dir":"Articles","previous_headings":"","what":"Simulated data","title":"Benchmarking PLS1 Implementations","text":"generate moderately sized synthetic datasets low dimensional latent structure. predictors stored bigmemory::big.matrix can reused without additional copies benchmarks. example n=4000 en p=50","code":"n <- 1000 p <- 50 ncomp <- 5  X <- bigmemory::big.matrix(nrow = n, ncol = p, type = \"double\") X[,] <- matrix(rnorm(n * p), nrow = n)  true_beta <- matrix(rnorm(p), ncol = 1) y_vec <- as.vector(scale(X[,] %*% true_beta + rnorm(n)))  y <- bigmemory::big.matrix(nrow = n, ncol = 1, type = \"double\") y[,] <- y_vec  X[1:6, 1:6] #>             [,1]        [,2]       [,3]        [,4]       [,5]       [,6] #> [1,] -0.56047565 -0.99579872 -0.5116037 -0.15030748  0.1965498 -0.4941739 #> [2,] -0.23017749 -1.03995504  0.2369379 -0.32775713  0.6501132  1.1275935 #> [3,]  1.55870831 -0.01798024 -0.5415892 -1.44816529  0.6710042 -1.1469495 #> [4,]  0.07050839 -0.13217513  1.2192276 -0.69728458 -1.2841578  1.4810186 #> [5,]  0.12928774 -2.54934277  0.1741359  2.59849023 -2.0261096  0.9161912 #> [6,]  1.71506499  1.04057346 -0.6152683 -0.03741501  2.2053261  0.3351310 y[1:6,] #> [1] -1.30750385 -0.31942974  0.78534055  0.95482632 -0.64407070 -0.04507161"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls1-benchmark.html","id":"benchmark-results","dir":"Articles","previous_headings":"","what":"Benchmark results","title":"Benchmarking PLS1 Implementations","text":"bench package provides convenient framework compare runtime characteristics solvers. following benchmark uses chunk size 1024 rows streaming variant, offers good trade-speed memory usage dataset size.","code":"bench_res_simpls <- press(   n = c(500, 1000, 5000),   p = c(100, 500, 1000),   {     ncomp = 5     rep = 1     algorithm = \"simpls\"     X <- bigmemory::big.matrix(nrow = n, ncol = p, type = \"double\")     X[,] <- matrix(rnorm(n * p), nrow = n)      true_beta <- matrix(rnorm(p), ncol = 1)     y_vec <- as.vector(scale(X[,] %*% true_beta + rnorm(n)))      y <- bigmemory::big.matrix(nrow = n, ncol = 1, type = \"double\")     y[,] <- y_vec     bench::mark(       dense = pls1_dense(X, y_vec, ncomp = ncomp, algorithm = algorithm),       streaming = pls1_stream(X, y_vec, ncomp = ncomp, chunk_size = 1024L,                                algorithm = algorithm),       dense_a = pls1_dense_a(X, y, ncomp = ncomp, algorithm = algorithm),       streaming_a = pls1_stream_a(X, y, ncomp = ncomp, chunk_size = 1024L,                                    algorithm = algorithm),       dense_ya = pls1_dense_ya(X, y, ncomp = ncomp, algorithm = algorithm),       streaming_ya = pls1_stream_ya(X, y, ncomp = ncomp, chunk_size = 1024L,                                      algorithm = algorithm),       iterations = 10,       check = FALSE     )     } ) bench_res_nipals <- press(   n = c(500, 1000, 5000),   p = c(100, 500, 1000),   {     ncomp = 5     rep = 1     algorithm = \"nipals\"     X <- bigmemory::big.matrix(nrow = n, ncol = p, type = \"double\")     X[,] <- matrix(rnorm(n * p), nrow = n)      true_beta <- matrix(rnorm(p), ncol = 1)     y_vec <- as.vector(scale(X[,] %*% true_beta + rnorm(n)))      y <- bigmemory::big.matrix(nrow = n, ncol = 1, type = \"double\")     y[,] <- y_vec     bench::mark(       dense = pls1_dense(X, y_vec, ncomp = ncomp, algorithm = algorithm),       streaming = pls1_stream(X, y_vec, ncomp = ncomp, chunk_size = 1024L,                                algorithm = algorithm),       dense_a = pls1_dense_a(X, y, ncomp = ncomp, algorithm = algorithm),       streaming_a = pls1_stream_a(X, y, ncomp = ncomp, chunk_size = 1024L,                                    algorithm = algorithm),       dense_ya = pls1_dense_ya(X, y, ncomp = ncomp, algorithm = algorithm),       streaming_ya = pls1_stream_ya(X, y, ncomp = ncomp, chunk_size = 1024L,                                      algorithm = algorithm),       iterations = 10,       check = FALSE     )     } )"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls1-benchmark.html","id":"interpreting-the-output","dir":"Articles","previous_headings":"","what":"Interpreting the output","title":"Benchmarking PLS1 Implementations","text":"table summarises timing memory footprint solver. practice can adjust number components, chunk size data generation process mimic workload. working file-backed matrices streaming solver often viable option, whereas dense solver remains fastest choice datasets comfortably fit memory.","code":"bench_res_simpls #> # A tibble: 54 √ó 8 #>    expression       n     p      min   median `itr/sec` mem_alloc `gc/sec` #>    <bch:expr>   <dbl> <dbl> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> #>  1 dense          500   100   5.11ms   5.28ms     189.    199.4KB     0    #>  2 streaming      500   100  26.68ms  26.74ms      37.3      49KB     0    #>  3 dense_a        500   100   4.67ms   4.71ms     212.     42.1KB     0    #>  4 streaming_a    500   100  26.25ms  26.38ms      37.7    47.7KB     0    #>  5 dense_ya       500   100   4.67ms   4.71ms     212.     43.5KB     0    #>  6 streaming_ya   500   100  26.22ms  26.32ms      37.9    43.8KB     0    #>  7 dense         1000   100   8.57ms   8.73ms     114.     18.4KB     0    #>  8 streaming     1000   100  51.21ms  51.31ms      19.2    18.4KB     2.13 #>  9 dense_a       1000   100   8.23ms   8.29ms     120.     11.2KB     0    #> 10 streaming_a   1000   100  50.79ms  50.89ms      19.6    11.6KB     0    #> # ‚Ñπ 44 more rows bench_res_nipals #> # A tibble: 54 √ó 8 #>    expression       n     p      min   median `itr/sec` mem_alloc `gc/sec` #>    <bch:expr>   <dbl> <dbl> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> #>  1 dense          500   100   8.53ms   8.54ms     117.     34.7KB     0    #>  2 streaming      500   100   8.53ms   8.53ms     117.     34.9KB     0    #>  3 dense_a        500   100   6.56ms   6.58ms     152.     33.5KB     0    #>  4 streaming_a    500   100   3.54ms   3.54ms     281.     53.4KB     0    #>  5 dense_ya       500   100   8.55ms   8.56ms     117.     62.7KB     0    #>  6 streaming_ya   500   100   8.55ms   8.56ms     117.     35.2KB     0    #>  7 dense         1000   100  16.99ms     17ms      58.8    50.8KB     0    #>  8 streaming     1000   100  16.98ms     17ms      58.8    50.8KB     6.53 #>  9 dense_a       1000   100  13.04ms  13.07ms      72.9    49.5KB     0    #> 10 streaming_a   1000   100   7.08ms   7.13ms     138.       89KB     0    #> # ‚Ñπ 44 more rows plot(bench_res_simpls, type=\"jitter\") plot(bench_res_nipals, type=\"jitter\") plot(bench_res_simpls, type=\"boxplot\") plot(bench_res_nipals, type=\"boxplot\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls2-benchmark.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Benchmarking PLS2 Implementations","text":"package offers dense (pls2_dense) streaming (pls2_stream) solvers multi-response partial least squares regression (PLS2). vignette demonstrates benchmark variants synthetic dataset featuring three correlated response variables.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/pls2-benchmark.html","id":"simulated-data","dir":"Articles","previous_headings":"","what":"Simulated data","title":"Benchmarking PLS2 Implementations","text":"","code":"n <- 3500 p <- 40 q <- 3 ncomp <- 4  X <- bigmemory::big.matrix(nrow = n, ncol = p, type = \"double\") X[,] <- matrix(rnorm(n * p), nrow = n)  loading_matrix <- matrix(rnorm(p * q), nrow = p) latent_scores <- matrix(rnorm(n * q), nrow = n) Y_mat <- scale(latent_scores %*% t(loading_matrix[1:q, , drop = FALSE]) + matrix(rnorm(n * q, sd = 0.5), nrow = n))  Y <- bigmemory::big.matrix(nrow = n, ncol = q, type = \"double\") Y[,] <- Y_mat  X[1:6, 1:6] #>            [,1]        [,2]       [,3]       [,4]       [,5]        [,6] #> [1,] -1.3435214 -1.26820867 -1.1037112  0.7617503 -0.7401395  0.61024868 #> [2,]  0.6217756  1.11508975 -1.2768613 -1.1255784 -0.6586119  0.10081629 #> [3,]  0.8008747 -0.75753597  0.3151891 -0.2539973  1.0197950  0.19342423 #> [4,] -1.3888924  0.04744551 -0.1255208  0.2758486  1.2427898  0.26696049 #> [5,] -0.7143569  0.42990602 -1.0605240  1.5142303 -0.2104338  0.01490636 #> [6,] -0.3240611  1.76766712 -0.8403403 -0.5756483  1.0435582 -0.53748062 Y[1:6, 1:min(6,q)] #>             [,1]       [,2]        [,3] #> [1,]  0.04209719  0.1434869  0.68332640 #> [2,]  0.62385647  1.0324674 -0.23792375 #> [3,] -0.64518653 -0.9586347  1.24808892 #> [4,] -0.73590774 -0.2234672 -0.05677271 #> [5,] -0.38412792  0.3147141 -1.03324920 #> [6,]  1.98359362  0.4768693 -0.64380685"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls2-benchmark.html","id":"benchmark-results","dir":"Articles","previous_headings":"","what":"Benchmark results","title":"Benchmarking PLS2 Implementations","text":"rely bench package compare implementations. block size 1024 rows used streaming solver, balances throughput memory requirements example dataset.","code":"bench_res_q_5 <- press(   n = c(500, 1000, 5000),   p = c(100, 500, 1000),   {     ncomp = 5     rep = 1     q = 5     X <- bigmemory::big.matrix(nrow = n, ncol = p, type = \"double\")     X[,] <- matrix(rnorm(n * p), nrow = n)          loading_matrix <- matrix(rnorm(p * q), nrow = p)     latent_scores <- matrix(rnorm(n * q), nrow = n)     Y_mat <- scale(latent_scores %*% t(loading_matrix[1:q, , drop = FALSE]) + matrix(rnorm(n * q, sd = 0.5), nrow = n))          Y <- bigmemory::big.matrix(nrow = n, ncol = q, type = \"double\")     Y[,] <- Y_mat          bench::mark(       dense_simpls = pls2_dense(X, Y, ncomp = ncomp, algorithm = \"simpls\"),       dense_nipals = pls2_dense(X, Y, ncomp = ncomp, algorithm = \"nipals\"),       streaming_simpls = pls2_stream(X, Y, ncomp = ncomp, chunk_size = 1024L,                                       algorithm = \"simpls\"),       streaming_nipals = pls2_stream(X, Y, ncomp = ncomp, chunk_size = 1024L,                                       algorithm = \"nipals\"),       iterations = 10,       check = FALSE     )   } ) bench_res_q_50 <- press(   n = c(500, 1000, 5000),   p = c(100, 500, 1000),   {     ncomp = 5     rep = 1     q = 50     X <- bigmemory::big.matrix(nrow = n, ncol = p, type = \"double\")     X[,] <- matrix(rnorm(n * p), nrow = n)          loading_matrix <- matrix(rnorm(p * q), nrow = p)     latent_scores <- matrix(rnorm(n * q), nrow = n)     Y_mat <- scale(latent_scores %*% t(loading_matrix[1:q, , drop = FALSE]) + matrix(rnorm(n * q, sd = 0.5), nrow = n))          Y <- bigmemory::big.matrix(nrow = n, ncol = q, type = \"double\")     Y[,] <- Y_mat          bench::mark(       dense_simpls = pls2_dense(X, Y, ncomp = ncomp, algorithm = \"simpls\"),       dense_nipals = pls2_dense(X, Y, ncomp = ncomp, algorithm = \"nipals\"),       streaming_simpls = pls2_stream(X, Y, ncomp = ncomp, chunk_size = 1024L,                                       algorithm = \"simpls\"),       streaming_nipals = pls2_stream(X, Y, ncomp = ncomp, chunk_size = 1024L,                                       algorithm = \"nipals\"),       iterations = 10,       check = FALSE     )   } )"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls2-benchmark.html","id":"interpreting-the-output","dir":"Articles","previous_headings":"","what":"Interpreting the output","title":"Benchmarking PLS2 Implementations","text":"benchmark summary reports average worst-case execution time well memory allocations. Users working file-backed matrices large datasets prefer streaming implementation, dense solver remains strong choice data comfortably fits RAM.","code":"bench_res_q_5 #> # A tibble: 36 √ó 8 #>    expression           n     p      min   median `itr/sec` mem_alloc `gc/sec` #>    <bch:expr>       <dbl> <dbl> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> #>  1 dense_simpls       500   100 950.22¬µs  958.7¬µs   1026.      50.5KB        0 #>  2 dense_nipals       500   100     16ms  16.07ms     62.1     36.6KB        0 #>  3 streaming_simpls   500   100   4.08ms   4.11ms    243.      31.9KB        0 #>  4 streaming_nipals   500   100 424.75ms 425.73ms      2.34    44.7KB        0 #>  5 dense_simpls      1000   100    1.9ms   2.03ms    497.      52.9KB        0 #>  6 dense_nipals      1000   100  40.01ms  40.19ms     24.8     52.9KB        0 #>  7 streaming_simpls  1000   100   8.12ms   8.13ms    123.      13.8KB        0 #>  8 streaming_nipals  1000   100 965.84ms  969.9ms      1.01    60.8KB        0 #>  9 dense_simpls      5000   100   9.73ms   9.87ms    101.     209.1KB        0 #> 10 dense_nipals      5000   100 197.99ms 198.43ms      5.00   209.1KB        0 #> # ‚Ñπ 26 more rows bench_res_q_50 #> # A tibble: 36 √ó 8 #>    expression           n     p      min   median `itr/sec` mem_alloc `gc/sec` #>    <bch:expr>       <dbl> <dbl> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> #>  1 dense_simpls       500   100   6.96ms   7.33ms   138.       71.6KB        0 #>  2 dense_nipals       500   100  107.6ms 107.69ms     9.28     71.6KB        0 #>  3 streaming_simpls   500   100   14.6ms  15.13ms    66.1        52KB        0 #>  4 streaming_nipals   500   100     2.8s    2.81s     0.356   114.6KB        0 #>  5 dense_simpls      1000   100  10.92ms  11.05ms    89.8      91.1KB        0 #>  6 dense_nipals      1000   100 270.61ms 277.01ms     3.52     91.1KB        0 #>  7 streaming_simpls  1000   100  25.37ms  25.38ms    39.3        52KB        0 #>  8 streaming_nipals  1000   100    7.89s     7.9s     0.126   134.2KB        0 #>  9 dense_simpls      5000   100  42.03ms  42.14ms    23.7     247.3KB        0 #> 10 dense_nipals      5000   100    1.64s    1.65s     0.607   247.3KB        0 #> # ‚Ñπ 26 more rows plot(bench_res_q_5, type=\"jitter\") plot(bench_res_q_50, type=\"jitter\") plot(bench_res_q_5, type=\"boxplot\") plot(bench_res_q_50, type=\"boxplot\")"},{"path":"https://fbertran.github.io/bigPLSR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Frederic Bertrand. Maintainer, author. Myriam Maumy. Author.","code":""},{"path":"https://fbertran.github.io/bigPLSR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Frederic Bertrand Myriam Maumy (2025). Partial Least Squares Regression Models Big Matrices, R package version 0.6.8. Maumy, M. Bertrand, F. (2023). PLS models extension big data. Conference presentation Joint Statistical Meetings (JSM 2023), Toronto, Ontario, Canada, Aug 5‚Äì10, 2023. Maumy, M. Bertrand, F. (2023). bigPLS: Fitting cross-validating PLS-based Cox models censored big data. Poster BioC2023: Bioconductor Annual Conference, Dana-Farber Cancer Institute, Boston, MA, USA, Aug 2‚Äì4, 2023. doi:10.7490/f1000research.1119546.1.","code":"@Manual{,   title = {Partial Least Squares Regression Models with Big Matrices},   author = {Frederic Bertrand and Myriam Maumy},   publisher = {manual},   year = {2025},   note = {R package version 0.6.8},   url = {https://fbertran.github.io/bigPLSR/}, } @Misc{,   title = {PLS models and their extension for big data},   author = {Myriam Maumy and Fr√©d√©ric Bertrand},   year = {2023},   howpublished = {Conference presentation at the Joint Statistical Meetings (JSM 2023)},   address = {Toronto, Ontario, Canada},   note = {Aug 5‚Äì10, 2023}, } @Misc{,   title = {bigPLS: Fitting and cross-validating PLS-based Cox models to censored big data},   author = {Myriam Maumy and Fr√©d√©ric Bertrand},   year = {2023},   howpublished = {Conference presentation at BioC2023: The Bioconductor Annual Conference},   address = {Dana-Farber Cancer Institute, Boston, MA, USA},   note = {Aug 2‚Äì4, 2023},   doi = {10.7490/f1000research.1119546.1},   url = {https://doi.org/10.7490/f1000research.1119546.1}, }"},{"path":[]},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"fr√©d√©ric-bertrand-and-myriam-maumy","dir":"","previous_headings":"","what":"Fr√©d√©ric Bertrand and Myriam Maumy","title":"Partial Least Squares Regression Models with Big Matrices","text":"bigPLSR provides fast, scalable Partial Least Squares (PLS) two execution backends: Dense (backend = \"arma\"): -memory Armadillo/BLAS speed. Big-matrix (backend = \"bigmem\"): chunked streaming bigmemory::big.matrix large data. PLS1 (single response) PLS2 (multi-response) supported. PLS2 uses SIMPLS cross-products backends numerical parity. Recent updates bring additional solvers tooling: Kernel PLS wide kernel PLS available alongside SIMPLS/NIPALS. Plot helpers now include correlation circles, loading arrows VIP bar charts. New wrappers simplify prediction, information-criteria based component selection, cross-validation bootstrapping workflows. package set CRAN-friendly: optional CBLAS fast path default. Support parallel computation GPU developed. website examples created F. Bertrand M. Maumy.","code":""},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Partial Least Squares Regression Models with Big Matrices","text":"can install released version bigPLSR CRAN : can install development version bigPLSR github :","code":"install.packages(\"bigPLSR\") devtools::install_github(\"fbertran/bigPLSR\")"},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick start","title":"Partial Least Squares Regression Models with Big Matrices","text":"","code":"library(bigPLSR)  set.seed(1) n <- 200; p <- 50 X <- matrix(rnorm(n*p), n, p) y <- X[,1]*2 - X[,2] + rnorm(n)  # Dense PLS1 (fast) fit <- pls_fit(X, y, ncomp = 3, backend = \"arma\", scores = \"r\") str(list(   coef=dim(fit$coefficients),   scores=dim(fit$scores),   ncomp=fit$ncomp ))"},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"big-matrix-pls1-with-file-backed-scores","dir":"","previous_headings":"Quick start","what":"Big-matrix PLS1 with file-backed scores","title":"Partial Least Squares Regression Models with Big Matrices","text":"","code":"options_val_before <- options(\"bigmemory.allow.dimnames\") options(bigmemory.allow.dimnames=TRUE)  bmX <- bigmemory::as.big.matrix(X) bmy <- bigmemory::as.big.matrix(matrix(y, n, 1))  tmp=tempdir() if(file.exists(paste(tmp,\"scores.desc\",sep=\"/\"))){unlink(paste(tmp,\"scores.desc\",sep=\"/\"))} if(file.exists(paste(tmp,\"scores.bin\",sep=\"/\"))){unlink(paste(tmp,\"scores.bin\",sep=\"/\"))} sink <- bigmemory::filebacked.big.matrix(   nrow=n, ncol=3, type=\"double\",   backingfile=\"scores.bin\",   backingpath=tmp,   descriptorfile=\"scores.desc\" )  fit_b <- pls_fit(   bmX, bmy, ncomp=3, backend=\"bigmem\", scores=\"big\",   scores_target=\"existing\", scores_bm=sink,   scores_colnames = c(\"t1\",\"t2\",\"t3\"),   return_scores_descriptor = TRUE )  fit_b$scores_descriptor  # big.matrix.descriptor options(bigmemory.allow.dimnames=options_val_before)"},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"pls2-multi-response","dir":"","previous_headings":"Quick start","what":"PLS2 (multi-response)","title":"Partial Least Squares Regression Models with Big Matrices","text":"","code":"set.seed(2) m <- 3 B <- matrix(rnorm(p*m), p, m) Y <- scale(X, scale = FALSE) %*% B + matrix(rnorm(n*m, sd = 0.1), n, m)  # Dense PLS2 ‚Äì SIMPLS on cross-products (parity with bigmem) fit2 <- pls_fit(X, Y, ncomp = 2, backend = \"arma\", mode = \"pls2\", scores = \"none\") str(list(coef=dim(fit2$coefficients), ncomp=fit2$ncomp))"},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"api","dir":"","previous_headings":"","what":"API","title":"Partial Least Squares Regression Models with Big Matrices","text":"Auto selection backend = \"auto\" ‚Üí \"bigmem\" X big.matrix (descriptor), else \"arma\". mode = \"auto\" ‚Üí \"pls1\" y one column, else \"pls2\". Return values PLS1: coefficients (p), intercept (scalar), x_weights, x_loadings, y_loadings, scores (optional), x_means, y_mean, ncomp. PLS2: coefficients (p√óm), intercept (length m), x_weights (p√óncomp), x_loadings (p√óncomp), y_loadings (m√óncomp), scores (optional), x_means, y_means, ncomp.","code":"pls_fit(   X, y, ncomp,   tol = 1e-8,   backend = c(\"auto\", \"arma\", \"bigmem\"),   scores  = c(\"none\", \"r\", \"big\"),   chunk_size = 10000L,   scores_name = \"scores\",   mode = c(\"auto\",\"pls1\",\"pls2\"),   scores_target = c(\"auto\",\"new\",\"existing\"),   scores_bm = NULL,   scores_backingfile = NULL,   scores_backingpath = NULL,   scores_descriptorfile = NULL,   scores_colnames = NULL,   return_scores_descriptor = FALSE )"},{"path":[]},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"dense-path-backend--arma","dir":"","previous_headings":"Backends & algorithms","what":"Dense path (backend = \"arma\")","title":"Partial Least Squares Regression Models with Big Matrices","text":"PLS1: fast dense solver (BLAS). Center X, Y, build XtX = Xc·µÄXc, XtY = Xc·µÄYc. Cholesky-whitened symmetric eigen solve; enforce symmetry add tiny ridge stabilize. Optional scores: T = Xc %*% W.","code":""},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"big-matrix-path-backend--bigmem","dir":"","previous_headings":"Backends & algorithms","what":"Big-matrix path (backend = \"bigmem\")","title":"Partial Least Squares Regression Models with Big Matrices","text":"Chunked /O big.matrix preallocated buffers. PLS1: streaming cross-products deflation; optional scores streamed chunk-wise sink. PLS2: chunked cross-products (XtX += B·µÄB, XtY += B·µÄY) + SIMPLS solver parity; optional score streaming: T = (X ‚àí Œº) %*% W. paths enforce symmetry (0.5*(M+M·µÄ)) eigen use small ridge XtX stability.","code":""},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"scores-sinks-and-descriptors","dir":"","previous_headings":"","what":"Scores, sinks, and descriptors","title":"Partial Least Squares Regression Models with Big Matrices","text":"scores = \"none\" ‚Äì don‚Äôt compute scores. scores = \"r\" ‚Äì return -memory matrix. Provide sink via scores_target = \"existing\" + scores_bm (big.matrix descriptor), Let function create file-backed storage via scores_backingfile (+ optional scores_backingpath, scores_descriptorfile). scores_colnames ‚Äì set column names scores. return_scores_descriptor = TRUE ‚Äì adds fit$scores_descriptor scores big.matrix.","code":""},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"determinism-tests--reproducibility","dir":"","previous_headings":"","what":"Determinism (tests & reproducibility)","title":"Partial Least Squares Regression Models with Big Matrices","text":"tight parity tests, force 1 BLAS thread fix RNG:","code":"set.seed(1) if (requireNamespace(\"RhpcBLASctl\", quietly = TRUE)) {   RhpcBLASctl::blas_set_num_threads(1L) } else {   # Use env vars before BLAS loads in the session   Sys.setenv(     OMP_NUM_THREADS=\"1\",     OPENBLAS_NUM_THREADS=\"1\",     MKL_NUM_THREADS=\"1\",     VECLIB_MAXIMUM_THREADS=\"1\",     BLIS_NUM_THREADS=\"1\"   ) }"},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"performance-tuning","dir":"","previous_headings":"","what":"Performance tuning","title":"Partial Least Squares Regression Models with Big Matrices","text":"chunk_size: default 10000L. Apple Silicon, internal default larger (e.g., 16384) chunk_size == 0. Tune per dataset best GEMM throughput. Scores streaming: scores=\"big\", streaming avoids holding T fully RAM. Multi-thread BLAS: production, allow multi-thread BLAS; tests, use 1 thread.","code":""},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"optional-cblas-fast-path-in-place-gemm","dir":"","previous_headings":"","what":"Optional CBLAS fast path (in-place GEMM)","title":"Partial Least Squares Regression Models with Big Matrices","text":"Default: (CRAN-safe). optional -place accumulation (true beta = 1 CBLAS dgemm) available guarded compile-time checks. available enabled, package falls back automatically portable Armadillo path. Enable locally (Unix/macOS): src/Makevars, link BLAS/LAPACK R uses: macOS: code attempts <vecLib/cblas.h>; Linux/others: <cblas.h>. headers aren‚Äôt present, build silently falls back portable GEMM path. hardcode -lopenblas -framework Accelerate; use R‚Äôs variables. Windows: leave macro unless ‚Äôve explicitly provided CBLAS headers/libs.","code":"R CMD INSTALL .   --configure-vars=\"PKG_CPPFLAGS='-DBIGPLSR_USE_CBLAS'\" PKG_LIBS += $(LAPACK_LIBS) $(BLAS_LIBS) $(FLIBS)"},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"development","dir":"","previous_headings":"","what":"Development","title":"Partial Least Squares Regression Models with Big Matrices","text":"Unit tests compare dense vs big-matrix backends PLS1/PLS2 tight tolerances. Vignettes examples keep datasets small; file-backed output uses tempdir().","code":""},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Partial Least Squares Regression Models with Big Matrices","text":"use bigPLSR academic work, please cite package SIMPLS method de Jong (1993).","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigPLSR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"bigPLSR-package ‚Äî bigPLSR-package","title":"bigPLSR-package ‚Äî bigPLSR-package","text":"Provides Partial least squares Regression big data. allows missing data explanatory variables. Repeated k-fold cross-validation models using various criteria. Bootstrap confidence intervals constructions also available.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigPLSR-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"bigPLSR-package ‚Äî bigPLSR-package","text":"Maumy, M., Bertrand, F. (2023). PLS models extension big data. Joint Statistical Meetings (JSM 2023), Toronto, , Canada. Maumy, M., Bertrand, F. (2023). bigPLS: Fitting cross-validating PLS-based Cox models censored big data. BioC2023 ‚Äî Bioconductor Annual Conference, Dana-Farber Cancer Institute, Boston, MA, USA. Poster. https://doi.org/10.7490/f1000research.1119546.1","code":""},{"path":[]},{"path":"https://fbertran.github.io/bigPLSR/reference/bigPLSR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"bigPLSR-package ‚Äî bigPLSR-package","text":"Maintainer: Frederic Bertrand frederic.bertrand@lecnam.net (ORCID) Authors: Myriam Maumy myriam.maumy@ehesp.fr (ORCID)","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigPLSR-package.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"bigPLSR-package ‚Äî bigPLSR-package","text":"","code":"set.seed(314) library(bigPLSR) data(sim_data) head(sim_data) #>                    status         X1         X2         X3        X4         X5 #> 0.0013236229370777      1  0.5448667 -0.9205711  1.1017160 1.3558567  1.4346174 #> 0.193665925040523       1 -0.5641483  0.2733279  0.9731780 1.1232252  0.2652977 #> 0.0167866701431944      1  1.4921118  0.2598002 -1.5436997 0.1165158  1.2208183 #> 0.0584127055299712      1 -0.6430141 -0.9807448 -1.2294945 0.8006227  1.5492078 #> 0.732960708716205       1  0.1876928 -1.2571263  0.9016827 1.3562191 -1.6809553 #> 0.508483386474255       0 -0.6141516 -0.8162560  0.2633415 0.4188066  0.2791399 #>                            X6         X7         X8          X9        X10 #> 0.0013236229370777 -0.8727406  1.5161252  0.7801527 -0.53617252 -0.6990319 #> 0.193665925040523   1.5046047  0.9096495 -1.2200395 -1.57280359  0.8347194 #> 0.0167866701431944 -0.6451659  1.2515692  0.5867273 -0.20080821  0.7492891 #> 0.0584127055299712  1.2557210  0.6188920  0.7123894 -0.67379538 -1.2377412 #> 0.732960708716205   0.7304366 -1.1223302  0.9633307  0.14016470 -0.9996676 #> 0.508483386474255  -0.0538974 -0.1410697 -0.8637916  0.01669784  1.5589135"},{"path":"https://fbertran.github.io/bigPLSR/reference/bigmatrix-operations.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix and arithmetic operations for big.matrix objects ‚Äî bigmatrix-operations","title":"Matrix and arithmetic operations for big.matrix objects ‚Äî bigmatrix-operations","text":"methods extend base matrix multiplication operator (%*%) group generic Arithmetic big.matrix objects can interoperate base R matrices numeric scalars using high-performance routines provided bigalgebra.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigmatrix-operations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matrix and arithmetic operations for big.matrix objects ‚Äî bigmatrix-operations","text":"","code":"# S4 method for class 'big.matrix,big.matrix' x %*% y  # S4 method for class 'matrix,big.matrix' x %*% y  # S4 method for class 'big.matrix,matrix' x %*% y  # S4 method for class 'big.matrix,big.matrix' Arith(e1, e2)  # S4 method for class 'big.matrix,matrix' Arith(e1, e2)  # S4 method for class 'matrix,big.matrix' Arith(e1, e2)  # S4 method for class 'numeric,big.matrix' Arith(e1, e2)  # S4 method for class 'big.matrix,numeric' Arith(e1, e2)"},{"path":"https://fbertran.github.io/bigPLSR/reference/bigmatrix-operations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matrix and arithmetic operations for big.matrix objects ‚Äî bigmatrix-operations","text":"x, y Matrix operands supplied either big.matrix instances base R matrices, depending method signature. e1, e2 Numeric operands, may big.matrix objects, base R matrices, numeric scalars depending method signature.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigmatrix-operations.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Matrix and arithmetic operations for big.matrix objects ‚Äî bigmatrix-operations","text":"Matrix multiplications dispatch bigalgebra::dgemm(), mixed arithmetic matrices relies bigalgebra::daxpy(), scalar/matrix combinations use bigalgebra::dadd() appropriate.","code":""},{"path":[]},{"path":"https://fbertran.github.io/bigPLSR/reference/bigmatrix-operations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Matrix and arithmetic operations for big.matrix objects ‚Äî bigmatrix-operations","text":"","code":"if (requireNamespace(\"bigmemory\", quietly = TRUE) &&     requireNamespace(\"bigalgebra\", quietly = TRUE)) {   x <- bigmemory::big.matrix(2, 2, init = 1)   y <- bigmemory::big.matrix(2, 2, init = 2)   x %*% y   x + y   x * 3 } #> An object of class \"big.matrix\" #> Slot \"address\": #> <pointer: 0x10f8ee840> #>"},{"path":"https://fbertran.github.io/bigPLSR/reference/bigscale.html","id":null,"dir":"Reference","previous_headings":"","what":"Title ‚Äî bigscale","title":"Title ‚Äî bigscale","text":"Title","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigscale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Title ‚Äî bigscale","text":"","code":"bigscale(   formula = Surv(time = time, status = status) ~ .,   data,   norm.method = \"standardize\",   strata.size = 20,   batch.size = 1,   features.mean = NULL,   features.sd = NULL,   parallel.flag = FALSE,   num.cores = NULL,   bigmemory.flag = FALSE,   num.rows.chunk = 1e+06,   col.names = NULL,   type = \"short\" )"},{"path":"https://fbertran.github.io/bigPLSR/reference/bigscale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Title ‚Äî bigscale","text":"formula  data  norm.method  strata.size  batch.size  features.mean  features.sd  parallel.flag  num.cores  bigmemory.flag  num.rows.chunk  col.names  type","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigscale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Title ‚Äî bigscale","text":"object scaler class time.indices: indices time variable cens.indices: indices censored variables features.indices: indices features time.sd: standard deviation time variable time.mean: mean time variable features.sd: standard deviation features features.mean: mean features nr: number rows nc: number columns col.names: columns names","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigscale.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Title ‚Äî bigscale","text":"","code":"1+1 #> [1] 2"},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_irls_binomial.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast IRLS for binomial logit with class weights ‚Äî cpp_irls_binomial","title":"Fast IRLS for binomial logit with class weights ‚Äî cpp_irls_binomial","text":"Fast IRLS binomial logit class weights","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_irls_binomial.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast IRLS for binomial logit with class weights ‚Äî cpp_irls_binomial","text":"","code":"cpp_irls_binomial(T, ybin, w_class = NULL, maxit = 50L, tol = 1e-08)"},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_irls_binomial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast IRLS for binomial logit with class weights ‚Äî cpp_irls_binomial","text":"T n x numeric matrix latent scores (intercept column) ybin integer vector 0,1 labels (length n) w_class optional length-2 numeric vector: weights classes c( w0, w1 ) maxit max IRLS iterations tol relative tolerance parameter change","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_irls_binomial.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast IRLS for binomial logit with class weights ‚Äî cpp_irls_binomial","text":"list(beta = -vector, b = scalar intercept, fitted = n-vector, iter = integer, converged = logical)","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_kernel_pls.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal kernel and wide-kernel PLS solver ‚Äî cpp_kernel_pls","title":"Internal kernel and wide-kernel PLS solver ‚Äî cpp_kernel_pls","text":"Internal kernel wide-kernel PLS solver","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_kernel_pls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal kernel and wide-kernel PLS solver ‚Äî cpp_kernel_pls","text":"","code":"cpp_kernel_pls(X, Y, ncomp, tol, wide)"},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_kernel_pls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal kernel and wide-kernel PLS solver ‚Äî cpp_kernel_pls","text":"X Centered design matrix. Y Centered response matrix. ncomp Maximum number components. tol Numerical tolerance. wide Whether use wide-kernel update.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_kernel_pls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal kernel and wide-kernel PLS solver ‚Äî cpp_kernel_pls","text":"list containing kernel PLS factors.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/dot-resolve_training_ref.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal: resolve training reference for RKHS predictions ‚Äî .resolve_training_ref","title":"Internal: resolve training reference for RKHS predictions ‚Äî .resolve_training_ref","text":"Accepts: dense matrix (returned -) big.matrix (returned -) big.matrix.descriptor (attached returned)","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/dot-resolve_training_ref.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal: resolve training reference for RKHS predictions ‚Äî .resolve_training_ref","text":"","code":".resolve_training_ref(obj, dots)"},{"path":"https://fbertran.github.io/bigPLSR/reference/dot-resolve_training_ref.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal: resolve training reference for RKHS predictions ‚Äî .resolve_training_ref","text":"Sources (priority): object$X, object$Xtrain, ...$Xtrain, ...$X_ref, object$X_ref","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Stateful PLS Kalman Filter ‚Äî kf_pls_state_fit","title":"Stateful PLS Kalman Filter ‚Äî kf_pls_state_fit","text":"Stateful PLS Kalman Filter","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stateful PLS Kalman Filter ‚Äî kf_pls_state_fit","text":"","code":"kf_pls_state_fit(state, tol = 1e-08)"},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stateful PLS Kalman Filter ‚Äî kf_pls_state_fit","text":"","code":"n <- 1000; p <- 30; m <- 2; A <- 3 X <- matrix(rnorm(n*p), n, p) Y <- X[,1:2] %*% matrix(c(0.7, -0.3, 0.2, 0.9), 2, m) + matrix(rnorm(n*m, sd=0.2), n, m)  state <- kf_pls_state_new(p, m, A, lambda = 0.99, q_proc = 1e-6)  # stream in mini-batches bs <- 128 for (i in seq(1, n, by = bs)) {   idx <- i:min(i+bs-1, n)   kf_pls_state_update(state, X[idx, , drop=FALSE], Y[idx, , drop=FALSE]) }  fit <- kf_pls_state_fit(state)  # returns a big_plsr-compatible list # predict via your existing predict.big_plsr (linear case) Yhat <- cbind(1, scale(X, center = fit$x_means, scale = FALSE)) %*%   rbind(fit$intercept, fit$coefficients)"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_biplot.html","id":null,"dir":"Reference","previous_headings":"","what":"PLS biplot ‚Äî plot_pls_biplot","title":"PLS biplot ‚Äî plot_pls_biplot","text":"PLS biplot","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_biplot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PLS biplot ‚Äî plot_pls_biplot","text":"","code":"plot_pls_biplot(   object,   comps = c(1L, 2L),   scale_variables = 1,   circle = TRUE,   circle_col = \"grey85\",   arrow_col = \"firebrick\",   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_biplot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"PLS biplot ‚Äî plot_pls_biplot","text":"object fitted PLS model scores loadings. comps Components display. scale_variables Scaling factor applied variable loadings. circle Logical; draw correlation circle behind loadings. circle_col Colour circle guide. arrow_col Colour loading arrows. ... Additional arguments passed graphics::plot().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_biplot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"PLS biplot ‚Äî plot_pls_biplot","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") plot_pls_biplot(fit)"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_individuals.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot individual scores ‚Äî plot_pls_individuals","title":"Plot individual scores ‚Äî plot_pls_individuals","text":"Plot individual scores","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_individuals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot individual scores ‚Äî plot_pls_individuals","text":"","code":"plot_pls_individuals(object, comps = c(1L, 2L), labels = NULL, ...)"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_individuals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot individual scores ‚Äî plot_pls_individuals","text":"object fitted PLS model scores. comps Components plot (length two). labels Optional character vector point labels. ... Additional plotting parameters passed graphics::plot().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_individuals.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot individual scores ‚Äî plot_pls_individuals","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") plot_pls_individuals(fit)"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_variables.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot variable loadings ‚Äî plot_pls_variables","title":"Plot variable loadings ‚Äî plot_pls_variables","text":"Plot variable loadings","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_variables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot variable loadings ‚Äî plot_pls_variables","text":"","code":"plot_pls_variables(   object,   comps = c(1L, 2L),   circle = TRUE,   circle_col = \"grey80\",   arrow_col = \"steelblue\",   arrow_scale = 1,   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_variables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot variable loadings ‚Äî plot_pls_variables","text":"object fitted PLS model. comps Components display (length two). circle Logical; draw correlation circle. circle_col Colour correlation circle. arrow_col Colour variable arrows. arrow_scale Scaling applied variable vectors. ... Additional plotting parameters passed graphics::plot().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_variables.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot variable loadings ‚Äî plot_pls_variables","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") plot_pls_variables(fit)"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_vip.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Variable Importance in Projection (VIP) ‚Äî plot_pls_vip","title":"Plot Variable Importance in Projection (VIP) ‚Äî plot_pls_vip","text":"Plot Variable Importance Projection (VIP)","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_vip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Variable Importance in Projection (VIP) ‚Äî plot_pls_vip","text":"","code":"plot_pls_vip(   object,   comps = NULL,   threshold = 1,   palette = c(\"#4575b4\", \"#d73027\"),   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_vip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Variable Importance in Projection (VIP) ‚Äî plot_pls_vip","text":"object fitted PLS model. comps Components aggregate. Defaults available. threshold Optional threshold highlight influential variables. palette Colour palette used bars. ... Additional parameters passed graphics::barplot().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_vip.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Variable Importance in Projection (VIP) ‚Äî plot_pls_vip","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") plot_pls_vip(fit)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense.html","id":null,"dir":"Reference","previous_headings":"","what":"Single-response partial least squares regression (PLS1) ‚Äî pls1_dense","title":"Single-response partial least squares regression (PLS1) ‚Äî pls1_dense","text":"helpers expose optimised dense streaming solvers tailored partial least squares regression problems response consists single column. wrap high performance C++ routines shipped package provide user friendly entry point benchmarking available implementations.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Single-response partial least squares regression (PLS1) ‚Äî pls1_dense","text":"","code":".harmonize_pls_result(res)  pls1_stream(   X,   y,   ncomp = 2L,   chunk_size = 1024L,   center = TRUE,   scale = FALSE,   center_y = TRUE,   scale_y = FALSE,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Single-response partial least squares regression (PLS1) ‚Äî pls1_dense","text":"X bigmemory::big.matrix storing design matrix. y Numeric vector responses length nrow(X). ncomp Number latent components compute. chunk_size Number rows process per chunk. Must strictly positive. Smaller chunks reduce peak memory usage larger chunks may improve speed. center columns X centered? Defaults TRUE. scale columns X scaled unit variance? Defaults FALSE. center_y response centered? Defaults TRUE. scale_y response scaled unit variance? Defaults FALSE. algorithm Algorithm use fit. Either \"simpls\" \"nipals\". choosing \"simpls\", preprocessing options must remain default values. return_big Logical; TRUE, coefficients, scores loadings returned bigmemory::big.matrix objects. Defaults FALSE.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Single-response partial least squares regression (PLS1) ‚Äî pls1_dense","text":"list containing regression coefficients, intercept, latent scores, loadings weights.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Single-response partial least squares regression (PLS1) ‚Äî pls1_dense","text":"","code":"# \\donttest{ library(bigmemory) X <- as.big.matrix(matrix(rnorm(2000), nrow = 100)) y <- matrix(rnorm(100), ncol = 1) fit <- pls1_dense(X, y, ncomp = 3) str(fit) #> List of 15 #>  $ coefficients: num [1:20, 1] 0.1145 0.1132 -0.0814 -0.1989 0.1312 ... #>  $ intercept   : num -0.167 #>  $ x_weights   : num [1:20, 1:3] 0.0944 0.093 -0.2491 -0.1897 0.1397 ... #>  $ x_loadings  : num [1:20, 1:3] 0.03071 0.09135 -0.34936 -0.00304 0.07533 ... #>  $ y_loadings  : num [1:3, 1] 0.439 0.224 0.1 #>  $ x_means     : num [1:20] -0.0032 -0.07841 -0.16905 -0.08206 -0.00219 ... #>  $ y_mean      : num -0.091 #>  $ ncomp       : int 3 #>  $ weights     : num [1:20, 1:3] 0.0944 0.093 -0.2491 -0.1897 0.1397 ... #>  $ loadings    : num [1:20, 1:3] 0.03071 0.09135 -0.34936 -0.00304 0.07533 ... #>  $ x_center    : num [1:20] -0.0032 -0.07841 -0.16905 -0.08206 -0.00219 ... #>  $ y_center    : num -0.091 #>  $ x_scale     : NULL #>  $ y_scale     : NULL #>  $ scores      : NULL # }  # \\donttest{ library(bigmemory) X <- as.big.matrix(matrix(rnorm(2000), nrow = 100)) y <- matrix(rnorm(100), ncol = 1) fit <- pls1_stream(X, y, ncomp = 3) str(fit) #> List of 15 #>  $ coefficients: num [1:20, 1] -0.0634 -0.0947 0.0649 0.1416 -0.0312 ... #>  $ intercept   : num -0.0348 #>  $ x_weights   : num [1:20, 1:3] -0.0193 -0.2892 0.0764 0.0932 -0.0772 ... #>  $ x_loadings  : num [1:20, 1:3] 0.069 -0.3726 0.0123 -0.0112 -0.0616 ... #>  $ y_loadings  : num [1:3, 1] 0.405 0.194 0.074 #>  $ x_means     : num [1:20] -0.0804 -0.0878 -0.1322 0.104 0.1434 ... #>  $ y_mean      : num -0.0538 #>  $ ncomp       : int 3 #>  $ weights     : num [1:20, 1:3] -0.0193 -0.2892 0.0764 0.0932 -0.0772 ... #>  $ loadings    : num [1:20, 1:3] 0.069 -0.3726 0.0123 -0.0112 -0.0616 ... #>  $ x_center    : num [1:20] -0.0804 -0.0878 -0.1322 0.104 0.1434 ... #>  $ y_center    : num -0.0538 #>  $ x_scale     : NULL #>  $ y_scale     : NULL #>  $ scores      : NULL # }"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_a.html","id":null,"dir":"Reference","previous_headings":"","what":"Single-response partial least squares regression (PLS1) another implementation ‚Äî pls1_dense_a","title":"Single-response partial least squares regression (PLS1) another implementation ‚Äî pls1_dense_a","text":"helpers wrap high-performance C++ routines built top bigmemory bigalgebra infrastructure. pls1_dense_ya function performs standard PLS regression using NIPALS-style algorithm without copying data memory. pls1_stream_ya variant iterates data blocks makes possible handle --core datasets efficiently.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_a.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Single-response partial least squares regression (PLS1) another implementation ‚Äî pls1_dense_a","text":"","code":"pls1_dense_a(   X,   y,   ncomp = 2L,   center = TRUE,   scale = FALSE,   tol = 1e-08,   max_iter = 100L,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )  pls1_stream_a(   X,   y,   ncomp = 2L,   chunk_size = 1024L,   center = TRUE,   scale = FALSE,   tol = 1e-08,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_a.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Single-response partial least squares regression (PLS1) another implementation ‚Äî pls1_dense_a","text":"X big.matrix object containing predictors. y Either big.matrix single column numeric vector response values. ncomp Number latent components extract. center Logical; predictors response centered. scale Logical; predictors response scaled unit variance fitting model. tol Numerical tolerance used detect convergence breakdown. max_iter Maximum number iterations internal solver (kept compatibility; solver adapts automatically convergence issues detected). algorithm Algorithm used compute PLS fit. Either \"simpls\" \"nipals\". SIMPLS backend supports default centering scaling configuration. return_big Logical; TRUE, coefficients, scores loadings returned bigmemory::big.matrix objects. Defaults FALSE. chunk_size Number rows processed time streaming backend.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_a.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Single-response partial least squares regression (PLS1) another implementation ‚Äî pls1_dense_a","text":"list regression coefficients, intercept, latent scores, weights additional metadata.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_a.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Single-response partial least squares regression (PLS1) another implementation ‚Äî pls1_dense_a","text":"","code":"# \\donttest{ library(bigmemory) X <- as.big.matrix(matrix(rnorm(2000), nrow = 100)) y <- as.big.matrix(matrix(rnorm(100), ncol = 1)) fit <- pls1_dense_a(X, y, ncomp = 3) str(fit) #> List of 16 #>  $ coefficients: num [1:20, 1] -0.0168 -0.1308 0.1621 0.1499 0.0418 ... #>  $ intercept   : num 0.222 #>  $ x_weights   : num [1:20, 1:3] -0.0027 -0.27846 0.35683 0.41143 0.00574 ... #>  $ x_loadings  : num [1:20, 1:3] 0.0337 -0.2535 0.3214 0.4333 -0.0755 ... #>  $ y_loadings  : num [1:3, 1] 0.3218 0.1069 0.0493 #>  $ x_means     : num [1:20] 0.0265 -0.0546 -0.0189 0.0167 -0.1192 ... #>  $ y_mean      : num 0.192 #>  $ ncomp       : int 3 #>  $ weights     : num [1:20, 1:3] -0.0027 -0.27846 0.35683 0.41143 0.00574 ... #>  $ loadings    : num [1:20, 1:3] 0.0337 -0.2535 0.3214 0.4333 -0.0755 ... #>  $ x_center    : num [1:20] 0.0265 -0.0546 -0.0189 0.0167 -0.1192 ... #>  $ y_center    : num 0.192 #>  $ x_scale     : NULL #>  $ y_scale     : NULL #>  $ scores      : NULL #>  $ call        : language pls1_dense_a(X = X, y = y, ncomp = 3) #>  - attr(*, \"class\")= chr [1:2] \"big_plsr\" \"list\" # }  # \\donttest{ library(bigmemory) X <- as.big.matrix(matrix(rnorm(2000), nrow = 100)) y <- as.big.matrix(matrix(rnorm(100), ncol = 1)) fit <- pls1_stream_a(X, y, ncomp = 3) str(fit) #> List of 17 #>  $ coefficients: num [1:20, 1] -0.2457 0.0752 0.1336 -0.2089 0.1301 ... #>  $ intercept   : num 0.0927 #>  $ x_weights   : num [1:20, 1:3] -0.2551 0.0453 0.2238 -0.2733 0.097 ... #>  $ x_loadings  : num [1:20, 1:3] -0.1273 -0.0333 0.2768 -0.195 -0.0221 ... #>  $ y_loadings  : num [1:3, 1] 0.579 0.197 0.1 #>  $ x_means     : num [1:20] -0.0705 -0.0676 0.0993 0.218 -0.1831 ... #>  $ y_mean      : num -0.0159 #>  $ ncomp       : int 3 #>  $ weights     : num [1:20, 1:3] -0.2551 0.0453 0.2238 -0.2733 0.097 ... #>  $ loadings    : num [1:20, 1:3] -0.1273 -0.0333 0.2768 -0.195 -0.0221 ... #>  $ x_center    : num [1:20] -0.0705 -0.0676 0.0993 0.218 -0.1831 ... #>  $ y_center    : num -0.0159 #>  $ x_scale     : NULL #>  $ y_scale     : NULL #>  $ scores      : NULL #>  $ chunk_size  : int 1024 #>  $ call        : language pls1_stream_a(X = X, y = y, ncomp = 3) #>  - attr(*, \"class\")= chr [1:2] \"big_plsr\" \"list\" # }"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_ya.html","id":null,"dir":"Reference","previous_headings":"","what":"Single-response partial least squares regression (PLS1) yet another implementation ‚Äî pls1_dense_ya","title":"Single-response partial least squares regression (PLS1) yet another implementation ‚Äî pls1_dense_ya","text":"Single-response partial least squares regression (PLS1) yet another implementation","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_ya.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Single-response partial least squares regression (PLS1) yet another implementation ‚Äî pls1_dense_ya","text":"","code":"pls1_dense_ya(   x,   y,   ncomp,   tol = 1e-08,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )  pls1_stream_ya(   x,   y,   ncomp,   chunk_size = 4096,   tol = 1e-08,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_ya.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Single-response partial least squares regression (PLS1) yet another implementation ‚Äî pls1_dense_ya","text":"x, y Predictor response objects stored double precision bigmemory::big.matrix instances. response must contain single column. dense helper also accepts numeric vectors y converts transparently. dense routine copies predictors R matrix, streaming version accesses blocks. ncomp Number latent components extract. tol Convergence tolerance used estimating component. relevant dense variant. algorithm Algorithm used compute PLS fit. Either \"simpls\" \"nipals\". SIMPLS backend generally faster data fits memory. return_big Logical; TRUE, coefficients, scores loadings returned bigmemory::big.matrix objects. Defaults FALSE. chunk_size Number rows processed per block streaming variant.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_ya.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Single-response partial least squares regression (PLS1) yet another implementation ‚Äî pls1_dense_ya","text":"list containing regression coefficients, intercept, loadings preprocessing statistics. structure matches output underlying C++ routines.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_ya.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Single-response partial least squares regression (PLS1) yet another implementation ‚Äî pls1_dense_ya","text":"","code":"# \\donttest{ library(bigmemory) X <- as.big.matrix(matrix(rnorm(2000), nrow = 100)) y <- as.big.matrix(matrix(rnorm(100), ncol = 1)) fit <- pls1_dense_ya(X, y, ncomp = 3) str(fit) #> List of 16 #>  $ coefficients: num [1:20, 1] 0.032576 0.063625 -0.000905 -0.055482 0.097963 ... #>  $ intercept   : num -0.135 #>  $ x_weights   : num [1:20, 1:3] -0.03604 0.16865 -0.00802 -0.01691 0.16181 ... #>  $ x_loadings  : num [1:20, 1:3] -0.108728 0.196599 0.000502 0.065538 0.092192 ... #>  $ y_loadings  : num [1:3, 1] 0.3892 0.119 0.0509 #>  $ x_means     : num [1:20] 4.29e-02 -6.20e-02 5.56e-05 -4.11e-04 1.79e-01 ... #>  $ y_mean      : num -0.1 #>  $ ncomp       : int 3 #>  $ weights     : num [1:20, 1:3] -0.03604 0.16865 -0.00802 -0.01691 0.16181 ... #>  $ loadings    : num [1:20, 1:3] -0.108728 0.196599 0.000502 0.065538 0.092192 ... #>  $ x_center    : num [1:20] 4.29e-02 -6.20e-02 5.56e-05 -4.11e-04 1.79e-01 ... #>  $ y_center    : num -0.1 #>  $ x_scale     : NULL #>  $ y_scale     : NULL #>  $ scores      : NULL #>  $ call        : language pls1_dense_ya(x = X, y = y, ncomp = 3) #>  - attr(*, \"class\")= chr [1:2] \"big_plsr\" \"list\" # }  # \\donttest{ library(bigmemory) X <- as.big.matrix(matrix(rnorm(2000), nrow = 100)) y <- as.big.matrix(matrix(rnorm(100), ncol = 1)) fit <- pls1_stream_ya(X, y, ncomp = 3) str(fit) #> List of 16 #>  $ coefficients: num [1:20, 1] 0.1189 0.0449 -0.0474 -0.0133 0.0545 ... #>  $ intercept   : num -0.215 #>  $ x_weights   : num [1:20, 1:3] 0.2465 0.1987 -0.2812 0.1544 0.0542 ... #>  $ x_loadings  : num [1:20, 1:3] 0.1625 0.2773 -0.3115 0.2517 0.0276 ... #>  $ y_loadings  : num [1:3, 1] 0.3138 0.1438 0.0843 #>  $ x_means     : num [1:20] 0.0192 0.1354 -0.0974 -0.19 -0.0393 ... #>  $ y_mean      : num -0.222 #>  $ ncomp       : int 3 #>  $ weights     : num [1:20, 1:3] 0.2465 0.1987 -0.2812 0.1544 0.0542 ... #>  $ loadings    : num [1:20, 1:3] 0.1625 0.2773 -0.3115 0.2517 0.0276 ... #>  $ x_center    : num [1:20] 0.0192 0.1354 -0.0974 -0.19 -0.0393 ... #>  $ y_center    : num -0.222 #>  $ x_scale     : NULL #>  $ y_scale     : NULL #>  $ scores      : NULL #>  $ call        : language pls1_stream_ya(x = X, y = y, ncomp = 3) #>  - attr(*, \"class\")= chr [1:2] \"big_plsr\" \"list\" # }"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls2_dense.html","id":null,"dir":"Reference","previous_headings":"","what":"Partial least squares regression for multi-response problems (PLS2) ‚Äî pls2_dense","title":"Partial least squares regression for multi-response problems (PLS2) ‚Äî pls2_dense","text":"Partial least squares regression multi-response problems (PLS2)","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls2_dense.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Partial least squares regression for multi-response problems (PLS2) ‚Äî pls2_dense","text":"","code":"pls2_dense(   X,   Y,   ncomp,   center = TRUE,   scale = FALSE,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )  pls2_stream(   X,   Y,   ncomp,   center = TRUE,   scale = FALSE,   chunk_size = 1024L,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls2_dense.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Partial least squares regression for multi-response problems (PLS2) ‚Äî pls2_dense","text":"X bigmemory::big.matrix containing predictor variables. Y bigmemory::big.matrix storing multi-dimensional response. ncomp Number latent components compute. center inputs centered prior fitting? scale inputs scaled unit variance prior fitting? algorithm PLS backend use. Either \"simpls\" (default) \"nipals\". return_big Logical; TRUE, coefficients, scores loadings returned bigmemory::big.matrix objects. Defaults FALSE. chunk_size Number rows processed per block streaming variant.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls2_dense.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Partial least squares regression for multi-response problems (PLS2) ‚Äî pls2_dense","text":"list regression coefficients, intercept, weights, loadings preprocessing metadata.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_bootstrap.html","id":null,"dir":"Reference","previous_headings":"","what":"Bootstrap confidence intervals for coefficients ‚Äî pls_bootstrap","title":"Bootstrap confidence intervals for coefficients ‚Äî pls_bootstrap","text":"Bootstrap confidence intervals coefficients","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_bootstrap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bootstrap confidence intervals for coefficients ‚Äî pls_bootstrap","text":"","code":"pls_bootstrap(   X,   Y,   ncomp,   R = 100L,   algorithm = c(\"simpls\", \"nipals\", \"kernelpls\", \"widekernelpls\"),   backend = \"arma\",   conf = 0.95,   seed = NULL )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_bootstrap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bootstrap confidence intervals for coefficients ‚Äî pls_bootstrap","text":"X Predictor matrix. Y Response matrix vector. ncomp Number components. R Number bootstrap replications. algorithm Backend algorithm (\"simpls\", \"nipals\", \"kernelpls\" \"widekernelpls\"). backend Backend argument passed fitting routine. conf Confidence level. seed Optional seed.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_bootstrap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bootstrap confidence intervals for coefficients ‚Äî pls_bootstrap","text":"list bootstrap samples confidence intervals.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_bootstrap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bootstrap confidence intervals for coefficients ‚Äî pls_bootstrap","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) pls_bootstrap(X, y, ncomp = 2, R = 20) #> $mean #>            [,1] #> [1,]  0.9913106 #> [2,] -0.4513641 #> [3,] -0.0364948 #>  #> $lower #>            [,1] #> [1,]  0.9008479 #> [2,] -0.5554312 #> [3,] -0.1578291 #>  #> $upper #>             [,1] #> [1,]  1.04643103 #> [2,] -0.29747912 #> [3,]  0.03218767 #>  #> $samples #> $samples[[1]] #>             [,1] #> [1,]  1.01370235 #> [2,] -0.47362869 #> [3,] -0.01871238 #>  #> $samples[[2]] #>             [,1] #> [1,]  1.00805220 #> [2,] -0.29373547 #> [3,] -0.03438166 #>  #> $samples[[3]] #>             [,1] #> [1,]  1.02482494 #> [2,] -0.43981055 #> [3,] -0.06598034 #>  #> $samples[[4]] #>            [,1] #> [1,]  0.9217837 #> [2,] -0.4195667 #> [3,] -0.1675766 #>  #> $samples[[5]] #>             [,1] #> [1,]  0.90088847 #> [2,] -0.56898923 #> [3,] -0.06228952 #>  #> $samples[[6]] #>             [,1] #> [1,]  1.03738707 #> [2,] -0.48335047 #> [3,]  0.02431306 #>  #> $samples[[7]] #>             [,1] #> [1,]  1.03665669 #> [2,] -0.41794678 #> [3,] -0.01365786 #>  #> $samples[[8]] #>             [,1] #> [1,]  0.90081111 #> [2,] -0.53015677 #> [3,] -0.06947427 #>  #> $samples[[9]] #>              [,1] #> [1,]  1.046587228 #> [2,] -0.420771928 #> [3,]  0.006859411 #>  #> $samples[[10]] #>            [,1] #> [1,]  1.0205916 #> [2,] -0.3137173 #> [3,] -0.0397945 #>  #> $samples[[11]] #>             [,1] #> [1,]  1.04625839 #> [2,] -0.46446341 #> [3,]  0.00655102 #>  #> $samples[[12]] #>            [,1] #> [1,]  0.9487480 #> [2,] -0.5126467 #> [3,] -0.1244670 #>  #> $samples[[13]] #>             [,1] #> [1,]  0.96695471 #> [2,] -0.52643057 #> [3,] -0.01118588 #>  #> $samples[[14]] #>             [,1] #> [1,]  1.00591617 #> [2,] -0.51301573 #> [3,]  0.03150559 #>  #> $samples[[15]] #>             [,1] #> [1,]  1.00430562 #> [2,] -0.49498888 #> [3,] -0.03036961 #>  #> $samples[[16]] #>              [,1] #> [1,]  1.006820717 #> [2,] -0.490124440 #> [3,]  0.006203915 #>  #> $samples[[17]] #>            [,1] #> [1,]  1.0193070 #> [2,] -0.3016168 #> [3,] -0.1470557 #>  #> $samples[[18]] #>            [,1] #> [1,]  0.9601683 #> [2,] -0.5404460 #> [3,] -0.0334001 #>  #> $samples[[19]] #>             [,1] #> [1,]  0.99306219 #> [2,] -0.51709589 #> [3,] -0.01978846 #>  #> $samples[[20]] #>             [,1] #> [1,]  0.96338505 #> [2,] -0.30477937 #> [3,]  0.03280478 #>  #>"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cross_validate.html","id":null,"dir":"Reference","previous_headings":"","what":"K-fold or leave-one-out cross validation for PLS models ‚Äî pls_cross_validate","title":"K-fold or leave-one-out cross validation for PLS models ‚Äî pls_cross_validate","text":"K-fold leave-one-cross validation PLS models","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cross_validate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"K-fold or leave-one-out cross validation for PLS models ‚Äî pls_cross_validate","text":"","code":"pls_cross_validate(   X,   Y,   ncomp,   folds = 5L,   type = c(\"kfold\", \"loo\"),   algorithm = c(\"simpls\", \"nipals\", \"kernelpls\", \"widekernelpls\"),   backend = \"arma\",   metrics = c(\"rmse\", \"mae\", \"r2\"),   seed = NULL )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cross_validate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"K-fold or leave-one-out cross validation for PLS models ‚Äî pls_cross_validate","text":"X Predictor matrix. Y Response matrix vector. ncomp Number components evaluate. folds Number folds (ignored type = \"loo\"). type Either \"kfold\" (default) \"loo\". algorithm Backend algorithm: \"simpls\", \"nipals\", \"kernelpls\" \"widekernelpls\". backend Backend passed pls_fit(). metrics Metrics compute (subset \"rmse\", \"mae\", \"r2\"). seed Optional seed reproducibility.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cross_validate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"K-fold or leave-one-out cross validation for PLS models ‚Äî pls_cross_validate","text":"list containing per-fold metrics summary across folds.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cross_validate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"K-fold or leave-one-out cross validation for PLS models ‚Äî pls_cross_validate","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) pls_cross_validate(X, y, ncomp = 2, folds = 3) #> $details #>    fold ncomp metric      value #> 1     1     1   rmse 0.16328299 #> 2     1     1    mae 0.12805868 #> 3     1     1     r2 0.97080073 #> 4     1     2   rmse 0.07303034 #> 5     1     2    mae 0.05597072 #> 6     1     2     r2 0.99415887 #> 7     2     1   rmse 0.24657144 #> 8     2     1    mae 0.20919627 #> 9     2     1     r2 0.95049915 #> 10    2     2   rmse 0.11001802 #> 11    2     2    mae 0.08976574 #> 12    2     2     r2 0.99014504 #> 13    3     1   rmse 0.36542334 #> 14    3     1    mae 0.34056432 #> 15    3     1     r2 0.78937830 #> 16    3     2   rmse 0.38837465 #> 17    3     2    mae 0.37082266 #> 18    3     2     r2 0.76209023 #>  #> $summary #>   ncomp metric     value #> 1     1    mae 0.2259398 #> 2     2    mae 0.1721864 #> 3     1     r2 0.9035594 #> 4     2     r2 0.9154647 #> 5     1   rmse 0.2584259 #> 6     2   rmse 0.1904743 #>"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cv_select.html","id":null,"dir":"Reference","previous_headings":"","what":"Select components from cross-validation results ‚Äî pls_cv_select","title":"Select components from cross-validation results ‚Äî pls_cv_select","text":"Select components cross-validation results","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cv_select.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select components from cross-validation results ‚Äî pls_cv_select","text":"","code":"pls_cv_select(cv_result, metric = c(\"rmse\", \"mae\", \"r2\"), minimise = NULL)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cv_select.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select components from cross-validation results ‚Äî pls_cv_select","text":"cv_result Result returned pls_cross_validate(). metric Metric optimise. minimise Logical; whether metric minimised.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cv_select.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select components from cross-validation results ‚Äî pls_cv_select","text":"Selected number components.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cv_select.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select components from cross-validation results ‚Äî pls_cv_select","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) cv <- pls_cross_validate(X, y, ncomp = 2, folds = 3) pls_cv_select(cv, metric = \"rmse\") #> [1] 2"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Unified PLS fit with auto backend and selectable algorithm ‚Äî pls_fit","title":"Unified PLS fit with auto backend and selectable algorithm ‚Äî pls_fit","text":"Dispatches dense (Arm/BLAS) backend -memory matrices streaming big.matrix backend X (Y) big.matrix. Algorithm can chosen : \"simpls\" (default), \"nipals\", \"kernelpls\", \"widekernelpls\", \"rkhs\" (Rosipal & Trejo), \"klogitpls\", \"sparse_kpls\", \"rkhs_xy\" (double RKHS), \"kf_pls\" (Kalman-filter PLS, streaming). \"kernelpls\" paths now include streaming XX' variant big.matrix inputs, optional row-chunking loop controlled chunk_cols.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unified PLS fit with auto backend and selectable algorithm ‚Äî pls_fit","text":"","code":"pls_fit(   X,   y,   ncomp,   tol = 1e-08,   backend = c(\"auto\", \"arma\", \"bigmem\"),   mode = c(\"auto\", \"pls1\", \"pls2\"),   algorithm = c(\"auto\", \"simpls\", \"nipals\", \"kernelpls\", \"widekernelpls\", \"rkhs\",     \"klogitpls\", \"sparse_kpls\", \"rkhs_xy\", \"kf_pls\"),   scores = c(\"none\", \"r\", \"big\"),   chunk_size = 10000L,   chunk_cols = NULL,   scores_name = \"scores\",   scores_target = c(\"auto\", \"new\", \"existing\"),   scores_bm = NULL,   scores_backingfile = NULL,   scores_backingpath = NULL,   scores_descriptorfile = NULL,   scores_colnames = NULL,   return_scores_descriptor = FALSE,   coef_threshold = NULL,   kernel = c(\"linear\", \"rbf\", \"poly\", \"sigmoid\"),   gamma = 1,   degree = 3L,   coef0 = 0,   approx = c(\"none\", \"nystrom\", \"rff\"),   approx_rank = NULL,   class_weights = NULL )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unified PLS fit with auto backend and selectable algorithm ‚Äî pls_fit","text":"X numeric matrix bigmemory::big.matrix y numeric vector/matrix big.matrix ncomp number latent components tol numeric tolerance used core solver backend one \"auto\", \"arma\", \"bigmem\" mode one \"auto\", \"pls1\", \"pls2\" algorithm one \"auto\", \"simpls\", \"nipals\", \"kernelpls\", \"widekernelpls\", \"rkhs\", \"klogitpls\", \"sparse_kpls\", \"rkhs_xy\", \"kf_pls\" scores one \"none\", \"r\", \"big\" chunk_size chunk size bigmem backend chunk_cols columns chunk size bigmem backend scores_name name dense scores (output big.matrix) scores_target one \"auto\", \"new\", \"existing\" scores_bm optional existing big.matrix descriptor scores scores_colnames optional character vector score column names return_scores_descriptor logical; TRUE scores big.matrix, add $scores_descriptor coef_threshold Optional non-negative value used hard-threshold fitted coefficients model estimation. supplied, absolute coefficients strictly threshold set zero via pls_threshold(). kernel kernel name RKHS/KPLS (\"linear\", \"rbf\", \"poly\", \"sigmoid\") gamma RBF/sigmoid/poly scale parameter degree polynomial degree coef0 polynomial/sigmoid bias approx kernel approximation: \"none\", \"nystrom\", \"rff\" approx_rank rank (columns / features) approximation class_weights optional numeric weights classes klogitpls scores_backingfile/backingpath/descriptorfile file-backed sink args","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unified PLS fit with auto backend and selectable algorithm ‚Äî pls_fit","text":"list coefficients, intercept, weights, loadings, means, optionally $scores.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unified PLS fit with auto backend and selectable algorithm ‚Äî pls_fit","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\", algorithm = \"simpls\") head(pls_predict_response(fit, X, ncomp = 2)) #> [1] -0.2557041 -0.3103345  1.8935717  0.1961492  0.2217772  2.3503614"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_information_criteria.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute information criteria for component selection ‚Äî pls_information_criteria","title":"Compute information criteria for component selection ‚Äî pls_information_criteria","text":"Compute information criteria component selection","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_information_criteria.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute information criteria for component selection ‚Äî pls_information_criteria","text":"","code":"pls_information_criteria(object, X, Y, max_comp = NULL)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_information_criteria.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute information criteria for component selection ‚Äî pls_information_criteria","text":"object fitted PLS model. X Training design matrix. Y Training response matrix vector. max_comp Maximum number components consider. criteria Character vector specifying criteria compute.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_information_criteria.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute information criteria for component selection ‚Äî pls_information_criteria","text":"data frame RSS, RMSE, AIC BIC per component.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_information_criteria.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute information criteria for component selection ‚Äî pls_information_criteria","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") pls_information_criteria(fit, X, y) #>   ncomp       rss      rmse       aic       bic #> 1     1 1.1527765 0.2400809 -53.07118 -51.07971 #> 2     2 0.7192385 0.1896363 -60.50589 -57.51869"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict responses from a PLS fit ‚Äî pls_predict_response","title":"Predict responses from a PLS fit ‚Äî pls_predict_response","text":"Predict responses PLS fit","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict responses from a PLS fit ‚Äî pls_predict_response","text":"","code":"pls_predict_response(object, newdata, ncomp = NULL)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict responses from a PLS fit ‚Äî pls_predict_response","text":"object fitted PLS model. newdata Predictor matrix scoring. ncomp Number components use.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict responses from a PLS fit ‚Äî pls_predict_response","text":"numeric matrix vector predictions.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict responses from a PLS fit ‚Äî pls_predict_response","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") pls_predict_response(fit, X, ncomp = 2) #>  [1] -1.0141790 -0.5820463  1.1675424  0.1269469  0.4234421  0.5985047 #>  [7]  0.3557169 -0.3717287 -1.1921817 -0.2849629"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_scores.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict latent scores from a PLS fit ‚Äî pls_predict_scores","title":"Predict latent scores from a PLS fit ‚Äî pls_predict_scores","text":"Predict latent scores PLS fit","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_scores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict latent scores from a PLS fit ‚Äî pls_predict_scores","text":"","code":"pls_predict_scores(object, newdata, ncomp = NULL)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_scores.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict latent scores from a PLS fit ‚Äî pls_predict_scores","text":"object fitted PLS model. newdata Predictor matrix scoring. ncomp Number components use.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_scores.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict latent scores from a PLS fit ‚Äî pls_predict_scores","text":"Matrix component scores.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_scores.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict latent scores from a PLS fit ‚Äî pls_predict_scores","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") pls_predict_scores(fit, X, ncomp = 2) #>                t1          t2 #>  [1,] -0.13544977 -0.52708945 #>  [2,] -0.17098259 -0.14902942 #>  [3,]  0.50972517  0.24632134 #>  [4,]  0.08135113  0.04355323 #>  [5,]  0.08487733  0.26452562 #>  [6,]  0.55636410 -0.25130464 #>  [7,]  0.08732598  0.20957085 #>  [8,] -0.44910850  0.39409855 #>  [9,] -0.26198780 -0.48844206 #> [10,] -0.30211506  0.25779599"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_select_components.html","id":null,"dir":"Reference","previous_headings":"","what":"Component selection via information criteria ‚Äî pls_select_components","title":"Component selection via information criteria ‚Äî pls_select_components","text":"Component selection via information criteria","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_select_components.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Component selection via information criteria ‚Äî pls_select_components","text":"","code":"pls_select_components(   object,   X,   Y,   criteria = c(\"aic\", \"bic\"),   max_comp = NULL )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_select_components.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component selection via information criteria ‚Äî pls_select_components","text":"object fitted PLS model. X Training design matrix. Y Training response matrix vector. criteria Character vector specifying criteria compute. max_comp Maximum number components consider.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_select_components.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Component selection via information criteria ‚Äî pls_select_components","text":"list per-component table selected components.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_select_components.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Component selection via information criteria ‚Äî pls_select_components","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") pls_select_components(fit, X, y) #> $table #>   ncomp       rss      rmse       aic       bic #> 1     1 1.1527765 0.2400809 -53.07118 -51.07971 #> 2     2 0.7192385 0.1896363 -60.50589 -57.51869 #>  #> $best #> $best$aic #> [1] 2 #>  #> $best$bic #> [1] 2 #>  #>"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_threshold.html","id":null,"dir":"Reference","previous_headings":"","what":"Naive sparsity control by coefficient thresholding ‚Äî pls_threshold","title":"Naive sparsity control by coefficient thresholding ‚Äî pls_threshold","text":"Naive sparsity control coefficient thresholding","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_threshold.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Naive sparsity control by coefficient thresholding ‚Äî pls_threshold","text":"","code":"pls_threshold(object, threshold)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_threshold.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Naive sparsity control by coefficient thresholding ‚Äî pls_threshold","text":"object fitted PLS model. threshold Values absolute magnitude set zero.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_threshold.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Naive sparsity control by coefficient thresholding ‚Äî pls_threshold","text":"modified copy object thresholded coefficients.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_threshold.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Naive sparsity control by coefficient thresholding ‚Äî pls_threshold","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2) pls_threshold(fit, threshold = 0.05) #> $coefficients #>            [,1] #> [1,]  0.7945827 #> [2,] -0.3874556 #> [3,]  0.1205194 #> [4,]  0.3708827 #>  #> $intercept #> [1] -0.07729456 #>  #> $x_weights #>              [,1]       [,2] #> [1,]  0.278021666  0.2224039 #> [2,]  0.002259737 -0.2982192 #> [3,] -0.039744939  0.1465173 #> [4,]  0.126996697  0.1076294 #>  #> $x_loadings #>           [,1]       [,2] #> [1,]  2.803684  0.1385679 #> [2,]  1.818750 -2.3265396 #> [3,] -1.468176  1.6845482 #> [4,]  1.244542  0.2652412 #>  #> $y_loadings #>          [,1]     [,2] #> [1,] 1.807709 1.312928 #>  #> $x_means #> [1]  0.07462564  0.20862196 -0.42455887  0.32204455 #>  #> $y_means #> [1] -0.03055689 #>  #> $ncomp #> [1] 2 #>  #> $mode #> [1] \"pls1\" #>  #> $algorithm #> [1] \"simpls\" #>  #> $x_center #> [1]  0.07462564  0.20862196 -0.42455887  0.32204455 #>  #> $y_center #> [1] -0.03055689 #>  #> $X #>              [,1]       [,2]       [,3]        [,4] #>  [1,] -0.56047565  1.2240818 -1.0678237  0.42646422 #>  [2,] -0.23017749  0.3598138 -0.2179749 -0.29507148 #>  [3,]  1.55870831  0.4007715 -1.0260044  0.89512566 #>  [4,]  0.07050839  0.1106827 -0.7288912  0.87813349 #>  [5,]  0.12928774 -0.5558411 -0.6250393  0.82158108 #>  [6,]  1.71506499  1.7869131 -1.6866933  0.68864025 #>  [7,]  0.46091621  0.4978505  0.8377870  0.55391765 #>  [8,] -1.26506123 -1.9666172  0.1533731 -0.06191171 #>  [9,] -0.68685285  0.7013559 -1.1381369 -0.30596266 #> [10,] -0.44566197 -0.4727914  1.2538149 -0.38047100 #>  #> $coef_threshold #> [1] 0.05 #>  #> attr(,\"class\") #> [1] \"big_plsr\" \"list\""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_vip.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable importance in projection (VIP) scores ‚Äî pls_vip","title":"Variable importance in projection (VIP) scores ‚Äî pls_vip","text":"Variable importance projection (VIP) scores","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_vip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable importance in projection (VIP) scores ‚Äî pls_vip","text":"","code":"pls_vip(object, comps = NULL)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_vip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable importance in projection (VIP) scores ‚Äî pls_vip","text":"object fitted PLS model. comps Components used compute VIP scores. Defaults available components.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_vip.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable importance in projection (VIP) scores ‚Äî pls_vip","text":"named numeric vector VIP scores.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_vip.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable importance in projection (VIP) scores ‚Äî pls_vip","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") pls_vip(fit) #> [1] 0.3679251 0.2478539 0.1299815 0.1706395"},{"path":"https://fbertran.github.io/bigPLSR/reference/predict.big_plsr.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict method for big_plsr objects ‚Äî predict.big_plsr","title":"Predict method for big_plsr objects ‚Äî predict.big_plsr","text":"Predict method big_plsr objects","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/predict.big_plsr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict method for big_plsr objects ‚Äî predict.big_plsr","text":"","code":"# S3 method for class 'big_plsr' predict(   object,   newdata,   ncomp = NULL,   type = c(\"response\", \"scores\", \"prob\", \"class\"),   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/predict.big_plsr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict method for big_plsr objects ‚Äî predict.big_plsr","text":"object fitted PLS model produced pls_fit(). newdata Matrix bigmemory::big.matrix predictor values. ncomp Number components use prediction. type Either \"response\" (default) \"scores\". ... Unused, compatibility generic.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/predict.big_plsr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict method for big_plsr objects ‚Äî predict.big_plsr","text":"Predicted responses component scores.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/predict.big_plsr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict method for big_plsr objects ‚Äî predict.big_plsr","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") predict(fit, X, ncomp = 2) #>  [1] -1.0141790 -0.5820463  1.1675424  0.1269469  0.4234421  0.5985047 #>  [7]  0.3557169 -0.3717287 -1.1921817 -0.2849629"},{"path":"https://fbertran.github.io/bigPLSR/reference/sim_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated dataset ‚Äî sim_data","title":"Simulated dataset ‚Äî sim_data","text":"dataset provides explantory variables simulations censoring status.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/sim_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated dataset ‚Äî sim_data","text":"data frame 1000 observations following 11 variables. status binary vector X1 numeric vector X2 numeric vector X3 numeric vector X4 numeric vector X5 numeric vector X6 numeric vector X7 numeric vector X8 numeric vector X9 numeric vector X10 numeric vector","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/sim_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simulated dataset ‚Äî sim_data","text":"TODO.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/sim_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulated dataset ‚Äî sim_data","text":"","code":"# \\donttest{ data(sim_data) X_sim_data_train <- sim_data[1:800,2:11] C_sim_data_train <- sim_data$status[1:800] X_sim_data_test <- sim_data[801:1000,2:11] C_sim_data_test <- sim_data$status[801:1000] rm(X_sim_data_train,C_sim_data_train,X_sim_data_test,C_sim_data_test) # }"},{"path":"https://fbertran.github.io/bigPLSR/reference/summary.big_plsr.html","id":null,"dir":"Reference","previous_headings":"","what":"Summaries for big_plsr objects ‚Äî summary.big_plsr","title":"Summaries for big_plsr objects ‚Äî summary.big_plsr","text":"Summaries big_plsr objects","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/summary.big_plsr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summaries for big_plsr objects ‚Äî summary.big_plsr","text":"","code":"# S3 method for class 'big_plsr' summary(object, X = NULL, Y = NULL, ...)  print.summary.big_plsr(x, ...)"},{"path":"https://fbertran.github.io/bigPLSR/reference/summary.big_plsr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summaries for big_plsr objects ‚Äî summary.big_plsr","text":"object fitted PLS model. X Optional design matrix recompute reconstruction metrics. Y Optional response matrix/vector. ... Unused.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/summary.big_plsr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summaries for big_plsr objects ‚Äî summary.big_plsr","text":"object class summary.big_plsr.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/summary.big_plsr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summaries for big_plsr objects ‚Äî summary.big_plsr","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") summary(fit) #> $call #> NULL #>  #> $algorithm #> [1] \"simpls\" #>  #> $mode #> [1] \"pls1\" #>  #> $ncomp #> [1] 2 #>  #> $intercept #> [1] -0.07729456 #>  #> $coefficients #>            [,1] #> [1,]  0.7945827 #> [2,] -0.3874556 #> [3,]  0.1205194 #> [4,]  0.3708827 #>  #> $score_variance #> [1] 0.1111111 0.1111111 #>  #> $explained_variance #> [1] 0.5 0.5 #>  #> $vip #> [1] 0.3679251 0.2478539 0.1299815 0.1706395 #>  #> attr(,\"class\") #> [1] \"summary.big_plsr\" set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") print(summary(fit)) #> $call #> NULL #>  #> $algorithm #> [1] \"simpls\" #>  #> $mode #> [1] \"pls1\" #>  #> $ncomp #> [1] 2 #>  #> $intercept #> [1] -0.07729456 #>  #> $coefficients #>            [,1] #> [1,]  0.7945827 #> [2,] -0.3874556 #> [3,]  0.1205194 #> [4,]  0.3708827 #>  #> $score_variance #> [1] 0.1111111 0.1111111 #>  #> $explained_variance #> [1] 0.5 0.5 #>  #> $vip #> [1] 0.3679251 0.2478539 0.1299815 0.1706395 #>  #> attr(,\"class\") #> [1] \"summary.big_plsr\""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-067","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.7","title":"bigPLSR 0.6.7","text":"kernelpls backend=‚Äòbigmem‚Äô now uses streaming XX·µó/column paths; previous dense fallback removed. Control options(bigPLSR.kpls_gram = ‚Äòrows‚Äô|‚Äòcols‚Äô|‚Äòauto‚Äô) bigPLSR.chunk_rows, bigPLSR.chunk_cols.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-066","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.6","title":"bigPLSR 0.6.6","text":"Vignettes: Kernel Streaming PLS Methods, Automatic Algorithm Selection. Stub C++ entry points RKHS / kernel logistic / sparse KPLS / KF-PLS.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-065","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.5","title":"bigPLSR 0.6.5","text":"XtX SIMPLS (standard cross-product SIMPLS), XXt (‚Äúwidekernelpls‚Äù) n << p, NIPALS memory tight rank low. Tuned options(bigPLSR.mem_budget_gb = 8). Users can override algorithm=. Kernel-style PLS routes: algorithm = \"kernelpls\" algorithm = \"widekernelpls\" implementing Dayal & MacGregor‚Äìstyle (1997) kernel PLS X-space wide-X (XX·µó) space. Implemented high-performance kernel wide-kernel PLS algorithms pls_fit() dense bigmemory backends using RcppArmadillo. Introduced optional coefficient thresholding. Added fast-running examples exported functions improve documentation usability CRAN.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-064","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.4","title":"bigPLSR 0.6.4","text":"Added kernel PLS wide-kernel PLS algorithms pls_fit() dense bigmemory backends. Refreshed plotting helpers correlation circles, arrow-based loadings dedicated VIP bar plot. Introduced convenience prediction wrappers, information-criteria helpers, expanded cross-validation/bootstrapping utilities support new algorithms. Improved summaries explained-variance reporting updated package documentation.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-062","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.2","title":"bigPLSR 0.6.2","text":"Added cross validation bootstrap plsR.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-061","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.1","title":"bigPLSR 0.6.1","text":"Added plots summaries pls_fit().","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-060","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.0","title":"bigPLSR 0.6.0","text":"Added unified path pls_fit() plsR regression features : dense bigmemory, simpls nipals.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-050","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.5.0","title":"bigPLSR 0.5.0","text":"Added several plsR implementations. Benchmarks.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-040","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.4.0","title":"bigPLSR 0.4.0","text":"Maintainer email update Added unit tests","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-030","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.3.0","title":"bigPLSR 0.3.0","text":"Code update","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-020","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.2.0","title":"bigPLSR 0.2.0","text":"Improving code help pages","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-010","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.1.0","title":"bigPLSR 0.1.0","text":"Implementing gpls, sgpls based models","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-001","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.0.1","title":"bigPLSR 0.0.1","text":"Package creation","code":""}]
