[{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"when-does-each-win","dir":"Articles","previous_headings":"","what":"When does each win?","title":"Automatic Algorithm Selection in bigPLSR","text":"XtX (SIMPLS): moderate pp (fits p2p^2 RAM). Fast BLAS-3; excellent n‚â´pn \\gg p. XX·µó (wide-kernel): moderate nn (fits n2n^2). Great p‚â´np\\gg n (wide problems). NIPALS / streaming: p2p^2 n2n^2 exceed budget; supports file-backed scores large-scale chunked BLAS.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"sanity-check","dir":"Articles","previous_headings":"","what":"Sanity check","title":"Automatic Algorithm Selection in bigPLSR","text":"","code":"set.seed(1) n <- 1e5; p <- 200 X <- matrix(rnorm(n*p), n, p) y <- X[,1]*0.5 + rnorm(n) bmX <- bigmemory::as.big.matrix(X) bmy <- bigmemory::as.big.matrix(matrix(y, n, 1))  options(bigPLSR.memory_budget_bytes = 2L*1024^3) fit <- pls_fit(bmX, bmy, ncomp=3, backend=\"bigmem\", scores=\"r\") fit$algorithm"},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Automatic Algorithm Selection in bigPLSR","text":"bigPLSR::pls_fit() can automatically choose algorithm based problem shape user-configurable memory budget: SIMPLS (XtX route) forming p √ó p cross-product fits memory. SIMPLS (XXt / kernel route) XtX fit XXt (n √ó n) . NIPALS (streaming) neither XtX XXt comfortably fit. selection applies algorithm = \"auto\" (default). explicit algorithm = overrides decision.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"why-these-choices","dir":"Articles","previous_headings":"Overview","what":"Why these choices?","title":"Automatic Algorithm Selection in bigPLSR","text":"SIMPLS works entirely centered cross-products, fast numerically robust target cross-product fits (either p√óp n√ón). Using XtX efficient p moderate; using XXt efficient ‚Äúwide‚Äù problems (p ‚â´ n) still bounded n^2 memory. NIPALS avoids materializing large cross-product can stream big.memory fixed working memory; safe fallback memory tight.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"the-decision-rule","dir":"Articles","previous_headings":"","what":"The decision rule","title":"Automatic Algorithm Selection in bigPLSR","text":"Let memory budget B bytes (defaults 8 GB, configurable via options(bigPLSR.mem_budget_gb = ...)). doubles (8 bytes), estimate size symmetric matrix : need_XtX = 8 * p^2 need_XXt = 8 * n^2 :","code":"if (can_XtX && shape_XtX) { algo_in <- \"simpls\"}.    # XtX   if (can_XXt && shape_XXt) { algo_in <- \"widekernelpls\"}. XXt (a.k.a. \"kernel\" route)   else                      { algorithm <- \"nipals\"}         # streaming"},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"configuring-the-memory-budget","dir":"Articles","previous_headings":"","what":"Configuring the memory budget","title":"Automatic Algorithm Selection in bigPLSR","text":"change R‚Äôs actual memory limit; controls selection.","code":"# Use 16 GB as selection budget options(bigPLSR.mem_budget_gb = 16)"},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"reproducibility-knobs","dir":"Articles","previous_headings":"","what":"Reproducibility knobs","title":"Automatic Algorithm Selection in bigPLSR","text":"tight numerical parity tests:","code":"set.seed(1) if (requireNamespace(\"RhpcBLASctl\", quietly = TRUE)) {   RhpcBLASctl::blas_set_num_threads(1L)   RhpcBLASctl::omp_set_num_threads(1L) } # otherwise, you can try environment variables: # Sys.setenv(OPENBLAS_NUM_THREADS = \"1\", OMP_NUM_THREADS = \"1\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"Automatic Algorithm Selection in bigPLSR","text":"Wide case: Big-matrix streaming:","code":"library(bigPLSR)  n <- 2e3; p <- 5e2 X <- matrix(rnorm(n*p), n, p) y <- X[,1] - 0.5*X[,2] + rnorm(n)  # Auto will likely pick SIMPLS (XtX) here fit <- pls_fit(X, y, ncomp = 10, algorithm = \"auto\") fit$algorithm  # \"simpls\" n <- 200; p <- 4000 X <- matrix(rnorm(n*p), n, p) y <- rnorm(n)  # If budget is small, auto picks kernel (XXt) or NIPALS options(bigPLSR.mem_budget_gb = 2)  # small budget fit <- pls_fit(X, y, ncomp = 5, algorithm = \"auto\") fit$algorithm  # \"kernelpls\" or \"nipals\" depending on n^2 vs budget library(bigmemory) n <- 1e6; p <- 50 # (example only; allocate according to your RAM) # bmX <- big.matrix(n, p, type = \"double\") # bmy <- big.matrix(n, 1, type = \"double\") # fit <- pls_fit(bmX, bmy, ncomp = 10, backend = \"bigmem\", algorithm = \"auto\") # fit$algorithm  # \"simpls\" or \"nipals\""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Automatic Algorithm Selection in bigPLSR","text":"de Jong, S. (1993). SIMPLS: alternative approach partial least squares regression. Chemometrics Intelligent Laboratory Systems, 18(3), 251‚Äì263. Dayal, B., & MacGregor, J. F. (1997). Improved PLS algorithms. Journal Chemometrics, 11(1), 73‚Äì85. Rosipal, R., & Trejo, L. J. (2001). Kernel Partial Least Squares Regression Reproducing Kernel Hilbert Space. Journal Machine Learning Research, 2, 97‚Äì123. Wold, H. (1966, 1985). NIPALS algorithm (original PLS formulation).","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-auto-selection.html","id":"appendix-streaming-gram-math","dir":"Articles","previous_headings":"","what":"Appendix: streaming Gram math","title":"Automatic Algorithm Selection in bigPLSR","text":"column blocks JJ, K‚âà‚àëJX[:,J]X[:,J]‚ä§,(Kv)‚Üê(Kv)+X[:,J](X[:,J]‚ä§v). K \\approx \\sum_{J} X_{[:,J]} X_{[:,J]}^\\top,\\quad (Kv) \\leftarrow (Kv) + X_{[:,J]} \\big(X_{[:,J]}^\\top v\\big). row blocks BB, K‚âà‚àëBXBX‚ä§,(Kv)‚Üê(Kv)+XB(X‚ä§v)B. K \\approx \\sum_{B} X_B X^\\top,\\quad (Kv) \\leftarrow (Kv) + X_B \\big(X^\\top v\\big)_B. Center fly: HKHv=Kv‚àí1nùüèùüè‚ä§Kv‚àí1nKùüèùüè‚ä§v+1n2ùüèùüè‚ä§Kùüèùüè‚ä§vH K H v = K v - \\tfrac{1}{n}\\mathbf{1}\\mathbf{1}^\\top K v - \\tfrac{1}{n}K\\mathbf{1}\\mathbf{1}^\\top v + \\tfrac{1}{n^2}\\mathbf{1}\\mathbf{1}^\\top K \\mathbf{1}\\,\\mathbf{1}^\\top v. Maintain needed aggregated vectors per pass.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-kpls-streaming.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Streaming Kernel PLS in bigPLSR: XX^T and Column-Chunked Variants","text":"vignette documents bigPLSR‚Äôs kernel PLS streaming backends bigmemory::big.matrix inputs. provide two complementary streaming strategies: Column-chunked Gram (existing): updates based per-column blocks form products involving K = X X^T implicitly. Row-chunked XX^T (new): computes = X^T u scanning rows blocks, emits t = X , enabling efficient access patterns n >> p storage layout favors row-contiguous slices (e.g., file-backed subsets). strategies produce model floating point round-. Selection automatic (see ?pls_fit) can forced via option options(bigPLSR.kpls_gram = \"rows\" | \"cols\" | \"auto\").","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-kpls-streaming.html","id":"math-sketch","dir":"Articles","previous_headings":"","what":"Math sketch","title":"Streaming Kernel PLS in bigPLSR: XX^T and Column-Chunked Variants","text":"Let X R^{n x p}, Y R^{n x m} centered. component h, kernel-PLS uses NIPALS-like fixed-point update Start u R^n (e.g., column Y). Compute = X^T u. Normalize w = / ||||_2. Scores: t = X w. p = (X^T t)/(t^T t), q = (Y^T t)/(t^T t). Deflate: X <- X - t p^T, Y <- Y - t q^T, set u <- Y q. Coefficients H components beta = W (P^T W)^{-1} Q^T, yhat = 1 * mu_Y + (x - mu_X) beta. row-chunked implementation keeps X disk performs steps (2) (4) two passes row blocks: Pass (accumulate ): block B rows, update += B^T u_B. Pass B (emit t): block B, write t_B = B * . Loadings p accumulated precisely like Pass t instead u.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-kpls-streaming.html","id":"apis","dir":"Articles","previous_headings":"","what":"APIs","title":"Streaming Kernel PLS in bigPLSR: XX^T and Column-Chunked Variants","text":"cpp_kpls_stream_xxt(X_ptr, Y_ptr, ncomp, chunk_rows, chunk_cols, center, return_big) cpp_kpls_stream_cols(X_ptr, Y_ptr, ncomp, chunk_cols, center, return_big) pls_fit(..., backend = \"bigmem\", algorithm = \"kernelpls\", chunk_size, chunk_cols, ...) pls_fit() chooses variant via options(bigPLSR.kpls_gram) heuristics \"auto\" set (default).","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-kpls-streaming.html","id":"when-to-prefer-each-variant","dir":"Articles","previous_headings":"","what":"When to prefer each variant","title":"Streaming Kernel PLS in bigPLSR: XX^T and Column-Chunked Variants","text":"Column-chunked (‚Äúcols‚Äù): good default; excellent p large access columns cheap (typical bigmemory column-major backing). Row-chunked XX^T (‚Äúrows‚Äù): prefer n >> p, row access contiguous (e.g., file-backed partitions), want minimize repeated column-touching across iterations.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bigPLSR-kpls-streaming.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Streaming Kernel PLS in bigPLSR: XX^T and Column-Chunked Variants","text":"Dayal, B., & MacGregor, J.F. (1997). Improved PLS algorithms. Journal Chemometrics, 11(1), 73‚Äì85. Rosipal, R., & Trejo, L.J. (2001). Kernel Partial Least Squares Regression Reproducing Kernel Hilbert Space. JMLR, 2, 97‚Äì123. (kernel/logistic/sparse KPLS references kpls_review vignette)","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/bootstrap-strategies.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Bootstrap strategies for bigPLSR","text":"bigPLSR now provides two complementary bootstrap procedures: (X, Y) bootstrap refits full regression model resampled pairs. (X, T) bootstrap keeps latent components original fit resamples score structure, delivering fast updates regression coefficients. approaches expose percentile BCa confidence intervals, numerical summaries plotting helpers. rely small multivariate example illustrate workflow.","code":"library(bigPLSR) n <- 100; p <- 6; m <- 2 X <- matrix(rnorm(n * p), n, p) eta1 <- X[, 1] + 0.4 * X[, 2] - 0.6 * X[, 3] eta2 <- -0.5 * X[, 2] + 0.7 * X[, 4] + 0.5 * X[, 5] Y <- cbind(eta1, eta2) + matrix(rnorm(n * m, sd = 0.5), n, m)"},{"path":"https://fbertran.github.io/bigPLSR/articles/bootstrap-strategies.html","id":"baseline-fit","dir":"Articles","previous_headings":"","what":"Baseline fit","title":"Bootstrap strategies for bigPLSR","text":"","code":"fit <- pls_fit(X, Y, ncomp = 3, scores = \"r\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/bootstrap-strategies.html","id":"x-y-bootstrap","dir":"Articles","previous_headings":"","what":"(X, Y) bootstrap","title":"Bootstrap strategies for bigPLSR","text":"quick visual inspection coefficient distributions:","code":"boot_xy <- pls_bootstrap(X, Y, ncomp = 3, R = 50, type = \"xy\",                          parallel = \"none\", return_scores = TRUE) head(summarise_pls_bootstrap(boot_xy)) #>   variable response        mean         sd percentile_lower percentile_upper #> 1       X1       Y1  1.12720380 0.05483111       1.03357126       1.22790239 #> 2       X2       Y1  0.31715126 0.07184301       0.16076652       0.43387511 #> 3       X3       Y1 -0.58032760 0.07402063      -0.71756064      -0.44816272 #> 4       X4       Y1  0.01629040 0.07403399      -0.13117394       0.11615478 #> 5       X5       Y1  0.03428052 0.08010332      -0.09078452       0.17987746 #> 6       X6       Y1 -0.02468821 0.05931838      -0.14013792       0.07848342 #>     bca_lower  bca_upper #> 1  1.07662505  1.2870309 #> 2  0.11687183  0.4120490 #> 3 -0.71386459 -0.3935652 #> 4 -0.18829055  0.1576646 #> 5 -0.09981499  0.3233262 #> 6 -0.14153533  0.0540529 plot_pls_bootstrap_coefficients(boot_xy, variables = colnames(X))"},{"path":"https://fbertran.github.io/bigPLSR/articles/bootstrap-strategies.html","id":"x-t-bootstrap","dir":"Articles","previous_headings":"","what":"(X, T) bootstrap","title":"Bootstrap strategies for bigPLSR","text":"conditional bootstrap operates latent score representation extracted baseline fit.","code":"boot_xt <- pls_bootstrap(X, Y, ncomp = 3, R = 50, type = \"xt\",                          parallel = \"none\", return_scores = TRUE) head(summarise_pls_bootstrap(boot_xt)) #>   variable response        mean         sd percentile_lower percentile_upper #> 1       X1     eta1  1.14632807 0.04497402        1.0672858      1.216945852 #> 2       X2     eta1  0.31388701 0.03309151        0.2577740      0.376551867 #> 3       X3     eta1 -0.57958235 0.02422587       -0.6339318     -0.539038171 #> 4       X4     eta1  0.01363128 0.03222957       -0.0448380      0.069097207 #> 5       X5     eta1  0.01377627 0.03246635       -0.0569404      0.072171472 #> 6       X6     eta1 -0.04337170 0.02979694       -0.1021070     -0.004940322 #>     bca_lower   bca_upper #> 1  1.06165429  1.22675518 #> 2  0.25417756  0.38356573 #> 3 -0.62915894 -0.51626603 #> 4 -0.05084027  0.07010002 #> 5 -0.03361595  0.07498808 #> 6 -0.12270539  0.02059054 plot_pls_bootstrap_coefficients(boot_xt, responses = colnames(Y))"},{"path":"https://fbertran.github.io/bigPLSR/articles/bootstrap-strategies.html","id":"exploring-bootstrap-scores","dir":"Articles","previous_headings":"","what":"Exploring bootstrap scores","title":"Bootstrap strategies for bigPLSR","text":"return_scores = TRUE, bootstrap result stores score matrices replicate. allows custom diagnostics dispersion first two latent variables: can feed individual score matrices plot_pls_individuals() overlay confidence ellipses obtained bootstrap draws.","code":"score_mats <- boot_xt$score_samples score_means <- sapply(score_mats, function(M) colMeans(M)[1:2]) apply(score_means, 1, summary) #>                 [,1]          [,2] #> Min.    -0.023265400 -0.0177317599 #> 1st Qu. -0.010183238 -0.0067989105 #> Median  -0.002768175 -0.0013845304 #> Mean    -0.002329194 -0.0006864936 #> 3rd Qu.  0.004474621  0.0059661965 #> Max.     0.021550597  0.0225250130"},{"path":"https://fbertran.github.io/bigPLSR/articles/bootstrap-strategies.html","id":"parallel-execution","dir":"Articles","previous_headings":"","what":"Parallel execution","title":"Bootstrap strategies for bigPLSR","text":"bootstrap flavours honour parallel = \"future\" option. Configure preferred plan calling helper:","code":"future::plan(future::multisession, workers = 2) boot_xy_parallel <- pls_bootstrap(X, Y, ncomp = 3, R = 100, type = \"xy\",                                   parallel = \"future\") future::plan(future::sequential)"},{"path":"https://fbertran.github.io/bigPLSR/articles/bootstrap-strategies.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Bootstrap strategies for bigPLSR","text":"Use two bootstrap strategies quantify uncertainty PLS models. (X, Y) variant mirrors classic non-parametric bootstrap (X, T) option keeps latent structure fixed computational efficiency. supplied summaries plotting helpers provide starting points elaborate diagnostic workflows.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/cross-validation-ic.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Cross-validation and Information Criteria in bigPLSR","text":"vignette illustrates evaluate partial least squares (PLS) models repeated cross-validation information criteria using new parallel helpers available bigPLSR. generate small synthetic data set examples run quickly even vignette built package installation.","code":"library(bigPLSR) n <- 120; p <- 8 X <- matrix(rnorm(n * p), n, p) eta <- X[, 1] - 0.8 * X[, 2] + 0.5 * X[, 3] y <- eta + rnorm(n, sd = 0.4)"},{"path":"https://fbertran.github.io/bigPLSR/articles/cross-validation-ic.html","id":"cross-validation","dir":"Articles","previous_headings":"","what":"Cross-validation","title":"Cross-validation and Information Criteria in bigPLSR","text":"pls_cross_validate() function now accepts parallel argument. Setting parallel = \"future\" evaluates folds concurrently relying future ecosystem. free configure execution plan like calling helper. keep sequential default avoid introducing run-time dependencies build process. Aggregating metrics provides quick overview predictive performance per number components: cross-validation table convenient downstream selection. example, can pick component count minimises RMSE:","code":"cv_res <- pls_cross_validate(X, y, ncomp = 4, folds = 6,                              metrics = c(\"rmse\", \"r2\"),                              parallel = \"none\") head(cv_res$details) #>   fold ncomp metric     value #> 1    1     1   rmse 0.4673779 #> 2    1     1     r2 0.8877468 #> 3    1     2   rmse 0.4176394 #> 4    1     2     r2 0.9103676 #> 5    1     3   rmse 0.3397565 #> 6    1     3     r2 0.9406804 cv_res$summary #>   ncomp metric     value #> 1     1     r2 0.8263996 #> 2     2     r2 0.8928828 #> 3     3     r2 0.9039359 #> 4     4     r2 0.9039186 #> 5     1   rmse 0.5430639 #> 6     2   rmse 0.4294906 #> 7     3   rmse 0.4038882 #> 8     4   rmse 0.4038991 pls_cv_select(cv_res, metric = \"rmse\") #> [1] 3"},{"path":"https://fbertran.github.io/bigPLSR/articles/cross-validation-ic.html","id":"information-criteria","dir":"Articles","previous_headings":"","what":"Information criteria","title":"Cross-validation and Information Criteria in bigPLSR","text":"Information criteria complement cross-validation trading goodness fit model complexity. helper pls_information_criteria() computes RSS, RMSE, AIC BIC across components: convenience wrapper pls_select_components() selects best components according requested criteria:","code":"fit <- pls_fit(X, y, ncomp = 4, scores = \"r\") ic_tbl <- pls_information_criteria(fit, X, y) ic_tbl #>   ncomp      rss      rmse       aic       bic #> 1     1 28.81873 0.4900572 -167.1760 -161.6010 #> 2     2 18.63632 0.3940846 -217.4856 -209.1231 #> 3     3 17.49284 0.3818032 -223.0840 -211.9340 #> 4     4 17.39255 0.3807071 -221.7740 -207.8365 pls_select_components(fit, X, y, criteria = c(\"aic\", \"bic\")) #> $table #>   ncomp      rss      rmse       aic       bic #> 1     1 28.81873 0.4900572 -167.1760 -161.6010 #> 2     2 18.63632 0.3940846 -217.4856 -209.1231 #> 3     3 17.49284 0.3818032 -223.0840 -211.9340 #> 4     4 17.39255 0.3807071 -221.7740 -207.8365 #>  #> $best #> $best$aic #> [1] 3 #>  #> $best$bic #> [1] 3"},{"path":"https://fbertran.github.io/bigPLSR/articles/cross-validation-ic.html","id":"parallel-execution-with-future","dir":"Articles","previous_headings":"","what":"Parallel execution with future","title":"Cross-validation and Information Criteria in bigPLSR","text":"wish parallelise cross-validation, configure plan calling helper. example assumes multicore environment therefore run vignette building: future_seed argument ensures reproducible bootstrap samples even multiple workers used.","code":"future::plan(future::multisession, workers = 2) cv_parallel <- pls_cross_validate(X, y, ncomp = 4, folds = 6,                                   metrics = c(\"rmse\", \"mae\"),                                   parallel = \"future\",                                   future_seed = TRUE) future::plan(future::sequential)"},{"path":"https://fbertran.github.io/bigPLSR/articles/cross-validation-ic.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Cross-validation and Information Criteria in bigPLSR","text":"refreshed cross-validation workflow exposes consistent interface sequential parallel execution, information-criteria helpers offer another perspective component selection. combination lets systematically tune PLS models accuracy parsimony.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/double-rkhs-pls.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Double RKHS PLS (rkhs_xy): Theory and Usage","text":"implement double RKHS variant PLS, input output spaces endowed reproducing kernels: KX‚àà‚Ñùn√ónK_X \\\\mathbb{R}^{n\\times n} entries [KX]ij=kX(xi,xj)[K_X]_{ij} = k_X(x_i, x_j), KY‚àà‚Ñùn√ónK_Y \\\\mathbb{R}^{n\\times n} entries [KY]ij=kY(yi,yj)[K_Y]_{ij} = k_Y(y_i, y_j). use centered Grams KÃÉX=HKXH\\tilde K_X = H K_X H KÃÉY=HKYH\\tilde K_Y = H K_Y H, H=‚àí1nùüèùüè‚ä§H = - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/double-rkhs-pls.html","id":"operator-and-latent-directions","dir":"Articles","previous_headings":"Overview","what":"Operator and Latent Directions","title":"Double RKHS PLS (rkhs_xy): Theory and Usage","text":"Following spirit Kernel PLS Regression II (IEEE TNNLS, 2019), avoid explicit square roots form SPD surrogate operator ‚Ñ≥v=(KX+ŒªxI)‚àí1KXKYKX(KX+ŒªxI)‚àí1v, \\mathcal{M} \\, v = (K_X+\\lambda_x )^{-1} \\; K_X \\; K_Y \\; K_X \\; (K_X+\\lambda_x )^{-1} \\, v,  small ridge Œªx>0\\lambda_x > 0 stability. compute first AA orthonormal latent directions T=[t1,‚Ä¶,tA]T = [t_1,\\dots,t_A] via power iteration Gram‚ÄìSchmidt orthogonalization ‚Ñ≥\\mathcal{M}. solve small regression latent space: C=(T‚ä§T)‚àí1(T‚ä§YÃÉ),YÃÉ=Y‚àíùüèy‚Äæ‚ä§, C = (T^\\top T)^{-1} (T^\\top \\tilde Y), \\qquad \\tilde Y = Y - \\mathbf{1} \\bar y^\\top,  form dual coefficients Œ±=UC,U=(KX+ŒªxI)‚àí1T, \\alpha \\;=\\; U \\, C, \\qquad U \\;=\\; (K_X+\\lambda_x )^{-1} T,  training predictions satisfy YÃÇ=KÃÉXŒ±+ùüèy‚Äæ‚ä§. \\hat Y \\;=\\; \\tilde K_X \\, \\alpha + \\mathbf{1}\\,\\bar y^\\top .","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/double-rkhs-pls.html","id":"centering-for-prediction","dir":"Articles","previous_headings":"Overview","what":"Centering for Prediction","title":"Double RKHS PLS (rkhs_xy): Theory and Usage","text":"Given new inputs $X_\\*$, define cross-Gram $$ K_\\* = K(X_\\*, X) . $$ apply training centering $K_\\*$, use $$ \\tilde K_\\* \\;=\\; K_\\* \\;-\\; \\mathbf{1}\\, \\bar k_X^\\top \\;-\\; \\bar k_\\* \\mathbf{1}^\\top \\;+\\; \\mu_X, $$ : - k‚ÄæX=1nùüè‚ä§KX\\bar k_X = \\frac{1}{n}\\mathbf{1}^\\top K_X column mean vector (uncentered) training Gram, - ŒºX=1n2ùüè‚ä§KXùüè\\mu_X = \\frac{1}{n^2} \\mathbf{1}^\\top K_X \\mathbf{1} grand mean, - $\\bar k_\\*$ row mean $K_\\*$ (computed prediction time). Predictions follow familiar dual form: $$ \\hat Y_\\* \\;=\\; \\tilde K_\\* \\, \\alpha + \\mathbf{1}_\\* \\, \\bar y^\\top . $$","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/double-rkhs-pls.html","id":"practical-notes","dir":"Articles","previous_headings":"Overview","what":"Practical Notes","title":"Double RKHS PLS (rkhs_xy): Theory and Usage","text":"Choose kXk_X (e.g., RBF) reflect nonlinear structure inputs. linear kYk_Y already produces numeric outputs ‚Ñùm\\mathbb{R}^m. ridge terms Œªx,Œªy\\lambda_x, \\lambda_y stabilize inversions dampen numerical noise. dual_coef =Œ±=\\alpha, scores =T=T (approximately orthonormal), intercept =y‚Äæ=\\bar y, uses centered cross-kernel formula predict().","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/double-rkhs-pls.html","id":"minimal-example","dir":"Articles","previous_headings":"Overview","what":"Minimal Example","title":"Double RKHS PLS (rkhs_xy): Theory and Usage","text":"References ‚Ä¢ Rosipal & Trejo (2001) Kernel Partial Least Squares Regression Reproducing Kernel Hilbert Space. JMLR 2:97‚Äì123. doi:10.5555/944733.944741. ‚Ä¢ Kernel PLS Regression II: Kernel Partial Least Squares Regression Projecting Independent Dependent Variables Reproducing Kernel Hilbert Space. IEEE TNNLS (2019). doi:10.1109/TNNLS.2019.2932014.","code":"library(bigPLSR) set.seed(42) n <- 60; p <- 6; m <- 2 X <- matrix(rnorm(n * p), n, p) Y <- cbind(sin(X[,1]) + 0.4 * X[,2]^2,            cos(X[,3]) - 0.3 * X[,4]^2) + matrix(rnorm(n*m, sd=.05), n, m)  op <- options(   bigPLSR.rkhs_xy.kernel_x = \"rbf\",   bigPLSR.rkhs_xy.gamma_x  = 0.5,   bigPLSR.rkhs_xy.kernel_y = \"linear\",   bigPLSR.rkhs_xy.lambda_x = 1e-6,   bigPLSR.rkhs_xy.lambda_y = 1e-6 ) on.exit(options(op), add = TRUE)  fit <- pls_fit(X, Y, ncomp = 3, algorithm = \"rkhs_xy\", backend = \"arma\") Yhat <- predict(fit, X) mean((Y - Yhat)^2) #> [1] 2.619847e-12"},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-long.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"External PLS benchmarks for bigPLSR: detailed analysis","text":"vignette presents detailed analysis external benchmarks stored external_pls_benchmarks. dataset compares bigPLSR dense streaming implementations reference PLS implementations available R packages. analysis organised follows. Section benchmark design structure dataset. Section PLS1 behaviour, emphasis runtime memory. Section PLS2 behaviour, including wide response settings. Section comparing kernel based algorithms wide kernel PLS. Short discussion take home messages. code chunks meant reproducible can adapted update extend benchmark dataset.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-long.html","id":"benchmark-design-and-data-structure","dir":"Articles","previous_headings":"","what":"Benchmark design and data structure","title":"External PLS benchmarks for bigPLSR: detailed analysis","text":"key factors : task: PLS1 versus PLS2, algorithm: SIMPLS, NIPALS, kernel PLS wide kernel PLS, package: implementation provider, n, p, q: data dimensions, ncomp: number extracted components. performance measures : median_time_s itr_per_sec runtime, mem_alloc_bytes memory consumption reported bench::mark. analyses convenient add helper variables. often work conditionally task, n, p, q ncomp order compare implementations exactly configurations.","code":"library(bigPLSR) library(ggplot2) library(dplyr) library(tidyr) library(forcats)  data(\"external_pls_benchmarks\", package = \"bigPLSR\")  head(external_pls_benchmarks) #>   task algorithm            package median_time_s itr_per_sec mem_alloc_bytes #> 1 pls1    simpls      bigPLSR_dense   0.006045184   165.15626          104720 #> 2 pls1    simpls bigPLSR_big.memory   0.003586762   275.94176          895872 #> 3 pls1    simpls                pls   0.002086736   469.94316         7479024 #> 4 pls1    simpls           mixOmics   0.004478861   223.44869         7550384 #> 5 pls1 kernelpls      bigPLSR_dense   0.044123503    22.66366        42461256 #> 6 pls1 kernelpls bigPLSR_big.memory   0.000521520  1733.62898          821328 #>      n   p q ncomp                                     notes #> 1 1000 100 1     1      Run via pls_fit() with dense backend #> 2 1000 100 1     1 Run via pls_fit() with big.memory backend #> 3 1000 100 1     1                  Requires the pls package #> 4 1000 100 1     1             Requires the mixOmics package #> 5 1000 100 1     1      Run via pls_fit() with dense backend #> 6 1000 100 1     1 Run via pls_fit() with big.memory backend bench <- external_pls_benchmarks %>%   mutate(     mem_mib      = mem_alloc_bytes / 1024^2,     log_time     = log10(median_time_s),     log_mem_mib  = log10(pmax(mem_mib, 1e-6)),     impl         = paste(package, algorithm, sep = \"::\")   )"},{"path":[]},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-long.html","id":"fixed-size-varying-number-of-components","dir":"Articles","previous_headings":"PLS1: dense versus streaming","what":"Fixed size, varying number of components","title":"External PLS benchmarks for bigPLSR: detailed analysis","text":"first focus PLS1 problems fix representative data size. can adjust selection depending contents benchmark runs. illustration take frequent (n, p, q) triple look runtime function ncomp.   plots allow compare dense bigPLSR backends competitors scale number latent components increases, without mixing problem sizes.","code":"pls1_sizes <- bench %>%   filter(task == \"pls1\") %>%   count(n, p, q, sort = TRUE)  pls1_sizes #>       n     p q nn #> 1  1000   100 1 48 #> 2 10000  1000 1 48 #> 3   100  5000 1 16 #> 4  1000 50000 1 16 size_pls1 <- pls1_sizes %>% slice(1L) %>% select(n, p, q)  pls1_subset <- bench %>%   semi_join(size_pls1, by = c(\"n\", \"p\", \"q\")) %>%   filter(task == \"pls1\") ggplot(pls1_subset,        aes(x = ncomp, y = median_time_s,            colour = package, linetype = algorithm)) +   geom_line() +   geom_point() +   scale_y_log10() +   labs(     x = \"Number of components\",     y = \"Median runtime (seconds, log scale)\",     title = \"PLS1: fixed data size, varying components\"   ) +   theme_minimal() ggplot(pls1_subset,        aes(x = ncomp, y = mem_mib,            colour = package, linetype = algorithm)) +   geom_line() +   geom_point() +   labs(     x = \"Number of components\",     y = \"Memory allocated (MiB)\",     title = \"PLS1: fixed data size, memory behaviour\"   ) +   theme_minimal()"},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-long.html","id":"relative-speed-and-memory-ratios","dir":"Articles","previous_headings":"PLS1: dense versus streaming","what":"Relative speed and memory ratios","title":"External PLS benchmarks for bigPLSR: detailed analysis","text":"better understand relative behaviour compute ratios respect chosen reference implementation, example pls::simpls available dataset.   figures help answer questions : close dense bigPLSR SIMPLS classical implementations terms runtime given problem regime. domains big memory streaming backends give substantial memory savings relative fully dense algorithms.","code":"reference_impl <- \"pls::simpls\"  ## 1) Reference rows for pls1 refs_pls1 <- bench %>%   filter(task == \"pls1\", impl == reference_impl) %>%   select(     n, p, q, ncomp,     time_ref = median_time_s,     mem_ref  = mem_mib   )  ## 2) Join and compute ratios (only where a reference exists) ratios_pls1 <- bench %>%   filter(task == \"pls1\") %>%   left_join(refs_pls1, by = c(\"n\", \"p\", \"q\", \"ncomp\")) %>%   filter(!is.na(time_ref), !is.na(mem_ref)) %>%   mutate(     rel_time = median_time_s / time_ref,     rel_mem  = mem_mib      / mem_ref   )  ggplot(ratios_pls1,        aes(x = impl, y = rel_time)) +   geom_hline(yintercept = 1, linetype = \"dashed\", colour = \"grey50\") +   geom_boxplot() +   coord_flip() +   scale_y_log10() +   labs(     x = \"Implementation\",     y = \"Runtime ratio vs reference (log scale)\",     title = \"PLS1: runtime ratios relative to pls::simpls\"   ) +   theme_minimal() ggplot(ratios_pls1,        aes(x = impl, y = rel_mem)) +   geom_hline(yintercept = 1, linetype = \"dashed\", colour = \"grey50\") +   geom_boxplot() +   coord_flip() +   scale_y_log10() +   labs(     x = \"Implementation\",     y = \"Memory ratio vs reference (log scale)\",     title = \"PLS1: memory ratios relative to pls::simpls\"   ) +   theme_minimal()"},{"path":[]},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-long.html","id":"fixed-size-varying-number-of-components-1","dir":"Articles","previous_headings":"PLS2: multiple responses","what":"Fixed size, varying number of components","title":"External PLS benchmarks for bigPLSR: detailed analysis","text":"now repeat type analysis PLS2 configurations.","code":"pls2_sizes <- bench %>%   filter(task == \"pls2\") %>%   count(n, p, q, sort = TRUE)  pls2_sizes #>       n     p   q nn #> 1  1000   100  10 48 #> 2  1000   100 100 48 #> 3 10000  1000  10 48 #> 4 10000  1000 100 48 #> 5   100  5000  10 16 #> 6   100  5000 100 16 #> 7  1000 50000  10 16 #> 8  1000 50000 100 16 size_pls2 <- pls2_sizes %>% slice(1L) %>% select(n, p, q)  pls2_subset <- bench %>%   semi_join(size_pls2, by = c(\"n\", \"p\", \"q\")) %>%   filter(task == \"pls2\") ggplot(pls2_subset,        aes(x = ncomp, y = median_time_s,            colour = package, linetype = algorithm)) +   geom_line() +   geom_point() +   scale_y_log10() +   labs(     x = \"Number of components\",     y = \"Median runtime (seconds, log scale)\",     title = \"PLS2: fixed data size, varying components\"   ) +   theme_minimal() ggplot(pls2_subset,        aes(x = ncomp, y = mem_mib,            colour = package, linetype = algorithm)) +   geom_line() +   geom_point() +   labs(     x = \"Number of components\",     y = \"Memory allocated (MiB)\",     title = \"PLS2: fixed data size, memory behaviour\"   ) +   theme_minimal()"},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-long.html","id":"influence-of-the-number-of-responses","dir":"Articles","previous_headings":"PLS2: multiple responses","what":"Influence of the number of responses","title":"External PLS benchmarks for bigPLSR: detailed analysis","text":"q grows, implementations may move predictor limited regime response limited regime. dataset allows explore fixing n p varying q. can adapt following code one several grids interest.","code":"pls2_q_grid <- bench %>%   filter(task == \"pls2\") %>%   count(n, p, q, sort = TRUE)  head(pls2_q_grid) #>       n    p   q nn #> 1  1000  100  10 48 #> 2  1000  100 100 48 #> 3 10000 1000  10 48 #> 4 10000 1000 100 48 #> 5   100 5000  10 16 #> 6   100 5000 100 16 grid_example <- pls2_q_grid %>%   slice(1L) %>%   select(n, p)  pls2_q_subset <- bench %>%   semi_join(grid_example, by = c(\"n\", \"p\")) %>%   filter(task == \"pls2\", ncomp == max(ncomp))  ggplot(pls2_q_subset,        aes(x = q, y = median_time_s,            colour = package, linetype = algorithm)) +   geom_line() +   geom_point() +   scale_y_log10() +   labs(     x = \"Number of responses q\",     y = \"Median runtime (seconds, log scale)\",     title = \"PLS2: influence of q at fixed n and p\"   ) +   theme_minimal()"},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-long.html","id":"kernel-and-wide-kernel-pls","dir":"Articles","previous_headings":"","what":"Kernel and wide kernel PLS","title":"External PLS benchmarks for bigPLSR: detailed analysis","text":"Kernel based algorithms available dense wide kernel flavours. often sensitive number observations linear PLS, rely Gram matrices size n x n. restrict dataset kernel based algorithms visual clarity. Example plot PLS1:  can build similar plots memory PLS2 task. make easy check whether kernel implementations scale expected way data regimes.","code":"bench_kernel <- bench %>%   filter(algorithm %in% c(\"kernelpls\", \"widekernelpls\")) kern_pls1 <- bench_kernel %>% filter(task == \"pls1\")  ggplot(kern_pls1,        aes(x = n, y = median_time_s,            colour = package, linetype = algorithm)) +   geom_line() +   geom_point() +   scale_y_log10() +   labs(     x = \"Number of observations n\",     y = \"Median runtime (seconds, log scale)\",     title = \"Kernel PLS1: scaling with n\"   ) +   theme_minimal()"},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-long.html","id":"discussion-and-practical-guidance","dir":"Articles","previous_headings":"","what":"Discussion and practical guidance","title":"External PLS benchmarks for bigPLSR: detailed analysis","text":"figures vignette illustrate practical messages. moderate dense problems n p fit comfortably memory, bigPLSR dense SIMPLS can used drop alternative classical PLS implementations. Runtime usually close practice code keeps possibility switching streaming required later. large sample sizes, big memory streaming backends avoid allocating dense score matrices scores = \"none\" keep memory bounded respect n. particularly relevant PLS2 kernel based methods. Kernel PLS algorithms may benefit careful choice kernel parameters backends reduce memory footprint n x n Gram matrices. design bigPLSR keeps extensions mind. benchmark dataset part package evidence behind statements remains transparent reproducible. can extend benchmarks tasks regenerate figures simply adding rows external_pls_benchmarks.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-short.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Benchmarking bigPLSR against external PLS implementations","text":"vignette documents bigPLSR implementations compare external partial least squares (PLS) libraries terms runtime memory use, using pre computed dataset external_pls_benchmarks. goals : summarise benchmark design, visualise runtime memory behaviour comparable problem sizes, provide short comments can reused papers reports.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-short.html","id":"benchmark-design","dir":"Articles","previous_headings":"","what":"Benchmark design","title":"Benchmarking bigPLSR against external PLS implementations","text":"dataset external_pls_benchmarks data frame contains benchmark results PLS1 PLS2 problems several algorithms. main columns : task: \"pls1\" \"pls2\", algorithm: one \"simpls\", \"nipals\", \"kernelpls\", \"widekernelpls\", package: implementation provider (example \"bigPLSR\", \"pls\", \"mixOmics\"), median_time_s: median runtime seconds reported bench::mark, itr_per_sec: iterations per second, mem_alloc_bytes: memory allocated bytes, n, p, q: number observations, predictors responses, ncomp: number components, notes: optional free text description. plots vignette focus configurations directly comparable, namely fixed task, n, p, q ncomp.","code":"library(bigPLSR) library(ggplot2) library(dplyr) library(tidyr)  data(\"external_pls_benchmarks\", package = \"bigPLSR\")  str(external_pls_benchmarks) #> 'data.frame':    384 obs. of  11 variables: #>  $ task           : chr  \"pls1\" \"pls1\" \"pls1\" \"pls1\" ... #>  $ algorithm      : chr  \"simpls\" \"simpls\" \"simpls\" \"simpls\" ... #>  $ package        : chr  \"bigPLSR_dense\" \"bigPLSR_big.memory\" \"pls\" \"mixOmics\" ... #>  $ median_time_s  : num  0.00605 0.00359 0.00209 0.00448 0.04412 ... #>  $ itr_per_sec    : num  165.2 275.9 469.9 223.4 22.7 ... #>  $ mem_alloc_bytes: num  104720 895872 7479024 7550384 42461256 ... #>  $ n              : num  1000 1000 1000 1000 1000 1000 1000 1000 100 100 ... #>  $ p              : num  100 100 100 100 100 100 100 100 5000 5000 ... #>  $ q              : num  1 1 1 1 1 1 1 1 1 1 ... #>  $ ncomp          : num  1 1 1 1 1 1 1 1 1 1 ... #>  $ notes          : chr  \"Run via pls_fit() with dense backend\" \"Run via pls_fit() with big.memory backend\" \"Requires the pls package\" \"Requires the mixOmics package\" ..."},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-short.html","id":"helper-summaries","dir":"Articles","previous_headings":"","what":"Helper summaries","title":"Benchmarking bigPLSR against external PLS implementations","text":"start compact summary reports best implementation configuration terms runtime memory. two tables indicate many configurations given combination package + algorithm comes fastest memory efficient.","code":"summ_best <- external_pls_benchmarks %>%   group_by(task, n, p, q, ncomp) %>%   mutate(     rank_time = rank(median_time_s, ties.method = \"min\"),     rank_mem  = rank(mem_alloc_bytes, ties.method = \"min\")   ) %>%   ungroup()  best_time <- summ_best %>%   filter(rank_time == 1L) %>%   count(task, package, algorithm, name = \"n_best_time\")  best_mem <- summ_best %>%   filter(rank_mem == 1L) %>%   count(task, package, algorithm, name = \"n_best_mem\")  best_time #> # A tibble: 4 √ó 4 #>   task  package            algorithm     n_best_time #>   <chr> <chr>              <chr>               <int> #> 1 pls1  bigPLSR_big.memory kernelpls               7 #> 2 pls1  bigPLSR_big.memory widekernelpls           7 #> 3 pls2  bigPLSR_big.memory kernelpls              12 #> 4 pls2  bigPLSR_big.memory widekernelpls          12 best_mem #> # A tibble: 4 √ó 4 #>   task  package            algorithm     n_best_mem #>   <chr> <chr>              <chr>              <int> #> 1 pls1  bigPLSR_big.memory widekernelpls          8 #> 2 pls1  bigPLSR_dense      nipals                 8 #> 3 pls2  bigPLSR_big.memory widekernelpls         16 #> 4 pls2  bigPLSR_dense      nipals                16"},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-short.html","id":"example-pls1-fixed-size-varying-components","dir":"Articles","previous_headings":"","what":"Example: PLS1, fixed size, varying components","title":"Benchmarking bigPLSR against external PLS implementations","text":"order avoid mixing problem sizes, select single PLS1 configuration plot runtime memory functions number components. can adjust filters match sizes interest work. Runtime comparison fixed size:  Memory use configuration:  figures exported SVG default, can included directly LaTeX HTML documents.","code":"example_pls1 <- external_pls_benchmarks %>%   filter(task == \"pls1\") %>%   group_by(n, p, q) %>%   filter(n == first(n), p == first(p), q == first(q)) %>%   ungroup()  example_pls1_size <- example_pls1 %>%   count(n, p, q, sort = TRUE) %>%   slice(1L) %>%   select(n, p, q)  example_pls1 <- external_pls_benchmarks %>%   semi_join(example_pls1_size, by = c(\"n\", \"p\", \"q\")) %>%   filter(task == \"pls1\") ggplot(example_pls1,        aes(x = ncomp, y = median_time_s,            colour = package, linetype = algorithm)) +   geom_line() +   geom_point() +   scale_y_log10() +   labs(     x = \"Number of components\",     y = \"Median runtime (seconds, log scale)\",     title = \"PLS1 benchmark, fixed (n, p, q)\",     subtitle = \"Comparison across packages and algorithms\"   ) +   theme_minimal() ggplot(example_pls1,        aes(x = ncomp, y = mem_alloc_bytes / 1024^2,            colour = package, linetype = algorithm)) +   geom_line() +   geom_point() +   labs(     x = \"Number of components\",     y = \"Memory allocated (MiB)\",     title = \"PLS1 benchmark, fixed (n, p, q)\"   ) +   theme_minimal()"},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-short.html","id":"example-pls2-fixed-size-varying-components","dir":"Articles","previous_headings":"","what":"Example: PLS2, fixed size, varying components","title":"Benchmarking bigPLSR against external PLS implementations","text":"repeat idea PLS2 setting.","code":"example_pls2 <- external_pls_benchmarks %>%   filter(task == \"pls2\") %>%   group_by(n, p, q) %>%   filter(n == first(n), p == first(p), q == first(q)) %>%   ungroup()  example_pls2_size <- example_pls2 %>%   count(n, p, q, sort = TRUE) %>%   slice(1L) %>%   select(n, p, q)  example_pls2 <- external_pls_benchmarks %>%   semi_join(example_pls2_size, by = c(\"n\", \"p\", \"q\")) %>%   filter(task == \"pls2\") ggplot(example_pls2,        aes(x = ncomp, y = median_time_s,            colour = package, linetype = algorithm)) +   geom_line() +   geom_point() +   scale_y_log10() +   labs(     x = \"Number of components\",     y = \"Median runtime (seconds, log scale)\",     title = \"PLS2 benchmark, fixed (n, p, q)\",     subtitle = \"Comparison across packages and algorithms\"   ) +   theme_minimal() ggplot(example_pls2,        aes(x = ncomp, y = mem_alloc_bytes / 1024^2,            colour = package, linetype = algorithm)) +   geom_line() +   geom_point() +   labs(     x = \"Number of components\",     y = \"Memory allocated (MiB)\",     title = \"PLS2 benchmark, fixed (n, p, q)\"   ) +   theme_minimal()"},{"path":"https://fbertran.github.io/bigPLSR/articles/external-pls-benchmarks-short.html","id":"short-commentary","dir":"Articles","previous_headings":"","what":"Short commentary","title":"Benchmarking bigPLSR against external PLS implementations","text":"plots summary tables can usually observe following patterns. small moderate PLS1 problems, dense bigPLSR SIMPLS backend typically close pls::simpls terms runtime, favouring explicit memory control. larger PLS1 PLS2 configurations, big memory streaming backends trade small runtime penalty bounded memory footprint depend number observations. Kernel based algorithms tend react strongly increases n ncomp underlying Gram matrices scale quadratically n. benchmarks stored regular data frame, can easily produce additional figures adapted application areas, example fixing n q varying p, comparing one two algorithms time.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kf-pls.html","id":"idea","dir":"Articles","previous_headings":"","what":"Idea","title":"KF-PLS: Streaming PLS with Kalman-style updates","text":"maintain exponentially-weighted cross-products ùêÇxx‚ÜêŒªùêÇxx+ùêób‚ä§ùêób+qùêà,ùêÇxy‚ÜêŒªùêÇxy+ùêób‚ä§ùêòb, \\mathbf{C}_{xx} \\leftarrow \\lambda\\,\\mathbf{C}_{xx} + \\mathbf{X}_b^\\top\\mathbf{X}_b + q\\,\\mathbf{},\\qquad \\mathbf{C}_{xy} \\leftarrow \\lambda\\,\\mathbf{C}_{xy} + \\mathbf{X}_b^\\top\\mathbf{Y}_b,  mini-batches bb rows, 0<Œª‚â§10<\\lambda\\le 1 forgetting factor q‚â•0q\\ge 0 small process-noise ridge. time extract latent components via SIMPLS (ùêÇxx,ùêÇxy)(\\mathbf{C}_{xx},\\mathbf{C}_{xy}). stable, fast, matches Kalman-style tracking slowly varying covariance structure.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kf-pls.html","id":"api","dir":"Articles","previous_headings":"","what":"API","title":"KF-PLS: Streaming PLS with Kalman-style updates","text":"bigmem, cross-products streamed row chunks; scores ùêì\\mathbf{T} produced via package‚Äôs chunked score kernel.","code":"fit <- pls_fit(X, Y, ncomp = 3,                backend   = \"arma\"  # or \"bigmem\"                ,algorithm = \"kf_pls\",                scores    = \"r\",                tol = 1e-8,                # tuning:                # options(bigPLSR.kf.lambda = 0.995,                #         bigPLSR.kf.q_proc = 1e-6) )"},{"path":"https://fbertran.github.io/bigPLSR/articles/kf-pls.html","id":"notes","dir":"Articles","previous_headings":"","what":"Notes","title":"KF-PLS: Streaming PLS with Kalman-style updates","text":"Œª‚Üí1\\lambda\\1 q‚Üí0q\\0 recovers batch SIMPLS. Smaller Œª\\lambda emphasizes recent batches (concept drift). qq stabilizes ill-conditioned ùêÇxx\\mathbf{C}_{xx} high-dimensional data.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/klogitpls.html","id":"kernel-logistic-pls-klogitpls","dir":"Articles","previous_headings":"","what":"Kernel Logistic PLS (klogitpls)","title":"Kernel Logistic PLS","text":"first extract latent scores Kernel PLS (KPLS): T=KcU, T = K_c U, Kc=HK(X,X)HK_c = H K(X,X) H centered Gram matrix columns UU dual score directions (KPLS deflation). fit logistic link latent space using IRLS: Œ∑=Œ≤0+TŒ≤,p=œÉ(Œ∑), \\eta = \\beta_0 + T \\beta, \\qquad p = \\sigma(\\eta), W=diag(p(1‚àíp)),z=Œ∑+y‚àípp(1‚àíp). W = \\mathrm{diag}(p (1-p)), \\qquad z = \\eta + \\frac{y - p}{p(1-p)}. iteration, solve weighted least squares system [Œ≤0,Œ≤][\\beta_0, \\beta]: (MÃÉ‚ä§MÃÉ)Œ∏=MÃÉ‚ä§zÃÉ,MÃÉ=W1/2[1,T],zÃÉ=W1/2z. (\\tilde{M}^\\top \\tilde{M}) \\theta = \\tilde{M}^\\top \\tilde{z}, \\quad \\tilde{M} = W^{1/2}[1, T], \\ \\tilde{z} = W^{1/2} z. Optionally, alternate: replace yy pp recompute KPLS refresh TT steps. Prediction new data uses centered cross-kernel $K_c(X_\\*, X)$ stored KPLS basis UU: $$ T_\\* = K_c(X_\\*, X) \\, U, \\qquad \\hat{p}_\\* = \\sigma\\!\\big(\\beta_0 + T_\\* \\beta\\big). $$","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"notation","dir":"Articles","previous_headings":"","what":"Notation","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Let X‚àà‚Ñùn√ópX \\\\mathbb{R}^{n\\times p} Y‚àà‚Ñùn√ó\\\\mathbb{R}^{n\\times m}. assume column-centered data unless stated otherwise. PLS extracts latent scores T=[t1,‚Ä¶,ta]T=[t_1,\\dots,t_a] loadings weights covariance XX YY along tat_a maximized, orthogonality constraints across components. kernel methods, let œï(‚ãÖ)\\phi(\\cdot) implicit feature map define Gram matrix KX=Œ¶XŒ¶X‚ä§K_X = \\Phi_X \\Phi_X^\\top (KX)ij=k(xi,xj)(K_X)_{ij} = k(x_i, x_j). centering operator H=‚àí1nùüèùüè‚ä§H = I_n - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top yields centered Gram KÃÉX=HKXH\\tilde K_X = H K_X H.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"pseudo-code-for-bigplsr-algorithms","dir":"Articles","previous_headings":"","what":"Pseudo-code for bigPLSR algorithms","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"package implements several complementary extraction schemes. following pseudo-code summarises core loops.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"simpls-densebigmem","dir":"Articles","previous_headings":"Pseudo-code for bigPLSR algorithms","what":"SIMPLS (dense/bigmem)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Compute centered cross-products Cxx=X‚ä§XC_{xx} = X^\\top X, Cxy=X‚ä§YC_{xy} = X^\\top Y. Initialise orthonormal basis V=[]V = []. Deflate CxyC_{xy} subspace spanned VV. Extract qaq_a dominant eigenvector Cxy‚ä§CxyC_{xy}^\\top C_{xy}. Compute wa=Cxyqaw_a = C_{xy} q_a normalise CxxC_{xx}-metric. Obtain loadings pa=Cxxwap_a = C_{xx} w_a regression weights ca=Cxy‚ä§wac_a = C_{xy}^\\top w_a. Expand V‚Üê[V,pa]V \\leftarrow [V, p_a]. Form W=[wa]W = [w_a], P=[pa]P = [p_a], Q=[ca]Q = [c_a] compute regression coefficients B=W(P‚ä§W)‚àí1Q‚ä§B = W (P^\\top W)^{-1} Q^\\top.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"nipals-densestreamed","dir":"Articles","previous_headings":"Pseudo-code for bigPLSR algorithms","what":"NIPALS (dense/streamed)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Initialise tat_a YY (XX). wa=X‚ä§ta/(ta‚ä§ta)w_a = X^\\top t_a / (t_a^\\top t_a), normalise waw_a. ta=Xwat_a = X w_a. ca=Y‚ä§ta/(ta‚ä§ta)c_a = Y^\\top t_a / (t_a^\\top t_a). ua=Ycau_a = Y c_a (multi-response). Deflate X‚ÜêX‚àítapa‚ä§X \\leftarrow X - t_a p_a^\\top, Y‚ÜêY‚àítaqa‚ä§Y \\leftarrow Y - t_a q_a^\\top repeat next component.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"kernel-pls-rkhs-dense-streamed","dir":"Articles","previous_headings":"Pseudo-code for bigPLSR algorithms","what":"Kernel PLS / RKHS (dense & streamed)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Form (stream) centered Gram matrix KÃÉX\\tilde K_X. iteration extract dual weight Œ±a\\alpha_a maximising covariance YY. Obtain score ta=KÃÉXŒ±at_a = \\tilde K_X \\alpha_a, regress YY tat_a get qaq_a deflate KÃÉX\\tilde K_X metric. Accumulate Œ±a\\alpha_a, qaq_a orthonormal basis subsequent deflation steps.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"double-rkhs-algorithm-rkhs_xy","dir":"Articles","previous_headings":"Pseudo-code for bigPLSR algorithms","what":"Double RKHS ( algorithm = \"rkhs_xy\" )","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Build (approximate) Gram matrices XX YY. Extract dual directions Œ±a\\alpha_a Œ≤a\\beta_a score pair (ta,ua)(t_a, u_a) maximises covariance kernels. Use ridge-regularised projections obtain regression weights. Store kernel centering statistics prediction.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"kalman-filter-pls-algorithm-kf_pls","dir":"Articles","previous_headings":"Pseudo-code for bigPLSR algorithms","what":"Kalman-filter PLS (algorithm = \"kf_pls\")","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Maintain exponentially weighted means Œºx,Œºy\\mu_x, \\mu_y. Update cross-products Cxx,CxyC_{xx}, C_{xy} forgetting factor Œª\\lambda optional process noise. Periodically call SIMPLS smoothed moments recover regression coefficients consistent streamed state. Common kernels: Linear:k(x,z)=x‚ä§zRBF:k(x,z)=exp(‚àíŒ≥‚à•x‚àíz‚à•2)Polynomial:k(x,z)=(Œ≥x‚ä§z+c0)dSigmoid:k(x,z)=tanh(Œ≥x‚ä§z+c0). \\begin{aligned} \\text{Linear:}\\quad& k(x,z) = x^\\top z \\\\ \\text{RBF:}\\quad& k(x,z) = \\exp(-\\gamma \\|x-z\\|^2) \\\\ \\text{Polynomial:}\\quad& k(x,z) = (\\gamma\\,x^\\top z + c_0)^{d} \\\\ \\text{Sigmoid:}\\quad& k(x,z) = \\tanh(\\gamma\\,x^\\top z + c_0). \\end{aligned}","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"centering-the-gram-matrix","dir":"Articles","previous_headings":"Pseudo-code for bigPLSR algorithms","what":"Centering the Gram matrix","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Given K‚àà‚Ñùn√ónK\\\\mathbb{R}^{n\\times n}, centered version : KÃÉ=HKH,H=‚àí1nùüèùüè‚ä§. \\tilde K = H K H, \\quad H = I_n - \\tfrac{1}{n}\\mathbf{1}\\mathbf{1}^\\top.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"klpls-kernel-pls-dayal-macgregor","dir":"Articles","previous_headings":"","what":"KLPLS / Kernel PLS (Dayal & MacGregor)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"operate dual. Consider KXK_X KXY=KXYK_{XY} = K_X Y. step aa, extract dual direction Œ±a\\alpha_a score ta=KÃÉXŒ±at_a = \\tilde K_X \\alpha_a maximizes covariance YY, subject orthogonality RKHS metric: maxŒ±cov(t,Y)s.t.t=KÃÉXŒ±,t‚ä§t=1,t‚ä§tb=0,b<. \\max_{\\alpha} \\ \\mathrm{cov}(t, Y) \\quad \\text{s.t.}\\ \\ t=\\tilde K_X \\alpha,\\ \\ t^\\top t = 1,\\ \\ t^\\top t_b = 0,\\ b<. SIMPLS-style recursion dual: Compute cross-covariance operator C=KÃÉXYC = \\tilde K_X Y. Extract direction ‚Ñùn\\mathbb{R}^n via dominant eigenvector CC‚ä§C C^\\top power iterations. Set ta=KÃÉXŒ±at_a = \\tilde K_X \\alpha_a, normalize tat_a. Regress YY tat_a: qa=(ta‚ä§ta)‚àí1ta‚ä§Yq_a = (t_a^\\top t_a)^{-1} t_a^\\top Y. Deflate Y‚ÜêY‚àítaqa‚ä§Y \\leftarrow Y - t_a q_a^\\top orthogonalize subsequent directions KÃÉX\\tilde K_X-metric. Prediction uses dual coefficients; new x‚ãÜx_\\star, k‚ãÜ=[k(x‚ãÜ,xi)]=1nk_\\star = [k(x_\\star, x_i)]_{=1}^n t‚ãÜ=kÃÉ‚ãÜ‚ä§Œ±t_\\star = \\tilde k_\\star^\\top \\alpha. YY multivariate, apply steps component-wise shared tat_a. bigPLSR - Dense path: algorithm=\"rkhs\" builds KÃÉX\\tilde K_X (approximation) runs dual SIMPLS deflation. - Big-matrix path: block-streamed Gram computations avoid materializing n√ónn\\times n.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"streaming-gram-blocks-column--and-row-chunked","dir":"Articles","previous_headings":"","what":"Streaming Gram blocks (column- and row-chunked)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"avoid forming KÃÉX\\tilde K_X explicitly accumulating blocks. Write KX=‚àëBXBX‚ä§K_X = \\sum_{B} X_B X^\\top blocks XBX_B taken rows (row-chunked/XX·µó) KX=XX‚ä§=‚àëCXC‚ä§K_X = X X^\\top = \\sum_{C} X C^\\top column chunks via XC‚ä§X C^\\top CC column submatrices (useful tall-skinny XX). Row-chunked (XX·µó): 1. blocks B‚äÇ{1,‚Ä¶,n}B \\subset \\{1,\\ldots,n\\}: compute GB=XBX‚ä§G_B = X_B X^\\top. 2. Accumulate K‚ÜêK+HGBHK \\leftarrow K + H G_B H fly needed matrix-vector products (Kv)(K v) without storing full KK. Column-chunked: 1. Partition feature dimension pp blocks JJ. 2. block JJ: GJ=X[:,J]X[:,J]‚ä§G_J = X_{[:,J]} X_{[:,J]}^\\top. 3. Use GJG_J update KvK v accumulators refresh deflation quantities (t,qt, q). Memory - Row-chunked: O(n‚ãÖchunk_rows)O(n \\cdot \\text{chunk\\_rows}). - Column-chunked: O(n‚ãÖchunk_cols)O(n \\cdot \\text{chunk\\_cols}). Pick based layout cache friendliness.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"kernel-approximations-nystr√∂m-and-random-fourier-features","dir":"Articles","previous_headings":"","what":"Kernel approximations: Nystr√∂m and Random Fourier Features","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Nystr√∂m (rank rr) Sample subset SS size rr, form KSSK_{SS} KNSK_{NS}. Define sketch Z=KNSKSS‚àí1/2Z = K_{NS} K_{SS}^{-1/2}, K‚âàZZ‚ä§K \\approx Z Z^\\top. Center ZZ subtracting row/column means. Run linear PLS ZZ. RFF (RBF kernels) Draw {œâ‚Ñì}‚Ñì=1r‚àºùí©(0,2Œ≥I)\\{\\omega_\\ell\\}_{\\ell=1}^r \\sim \\mathcal{N}(0,2\\gamma ) b‚Ñì‚àºùí∞[0,2œÄ]b_\\ell\\sim \\mathcal{U}[0,2\\pi]. Define features œÜ‚Ñì(x)=2rcos(œâ‚Ñì‚ä§x+b‚Ñì)\\varphi_\\ell(x)=\\sqrt{\\tfrac{2}{r}}\\cos(\\omega_\\ell^\\top x + b_\\ell), k(x,z)‚âàœÜ(x)‚ä§œÜ(z)k(x,z)\\approx \\varphi(x)^\\top \\varphi(z). Run linear PLS œÜ(X)\\varphi(X).","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"kernel-logistic-pls-binary-classification","dir":"Articles","previous_headings":"","what":"Kernel Logistic PLS (binary classification)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"first compute KPLS scores T‚àà‚Ñùn√ó\\\\mathbb{R}^{n\\times } XX vs labels y‚àà{0,1}y\\\\{0,1\\}, run logistic regression latent space via IRLS: Minimize ‚Ñì(Œ≤,Œ≤0)=‚àí‚àë{yiŒ∑i‚àílog(1+expŒ∑i)}\\ell(\\beta, \\beta_0) = -\\sum_i \\{ y_i\\eta_i - \\log(1+\\exp\\eta_i)\\} Œ∑=Œ≤0ùüè+TŒ≤\\eta = \\beta_0\\mathbf{1} + T \\beta. IRLS step iteration kk: W=diag(p(k)(1‚àíp(k))),z=Œ∑(k)+W‚àí1(y‚àíp(k)),[Œ≤0Œ≤]=(XŒ∑‚ä§WXŒ∑+ŒªI)‚àí1XŒ∑‚ä§Wz, W = \\mathrm{diag}(p^{(k)}(1-p^{(k)})),\\quad z = \\eta^{(k)} + W^{-1}(y - p^{(k)}),\\quad \\begin{bmatrix}\\beta_0\\\\ \\beta\\end{bmatrix} = (X_\\eta^\\top W X_\\eta + \\lambda )^{-1} X_\\eta^\\top W z, XŒ∑=[ùüè,T]X_\\eta = [\\mathbf{1}, T] p(k)=œÉ(Œ∑(k))p^{(k)} = \\sigma(\\eta^{(k)}). Optionally alternate: recompute KPLS scores current residuals re-run IRLS steps. Class weights wcw_c can injected scaling rows WW. bigPLSRalgorithm=\"klogitpls\" computes TT (dense streamed) fits IRLS latent space.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"sparse-kernel-pls-sketch","dir":"Articles","previous_headings":"","what":"Sparse Kernel PLS (sketch)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Promote sparsity dual primal weights. dual form, constrain Œ±a\\alpha_a ‚Ñì1\\ell_1 (group) penalty: maxŒ±cov(KÃÉŒ±,Y)‚àíŒª‚à•Œ±‚à•1s.t.(KÃÉŒ±)‚ä§(KÃÉŒ±)=1,ta‚ä§tb=0(b<). \\max_{\\alpha}\\ \\mathrm{cov}(\\tilde K \\alpha, Y) - \\lambda\\|\\alpha\\|_1 \\quad\\text{s.t.}\\quad (\\tilde K\\alpha)^\\top (\\tilde K\\alpha) = 1,\\ t_a^\\top t_b=0\\ (b<). practical approach uses proximal gradient coordinate descent smooth surrogate covariance, periodic orthogonalization resulting score vectors KÃÉ\\tilde K metric. Early stop explained covariance. (current release provides scaffolding API.)","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"pls-in-rkhs-for-x-and-y-double-rkhs","dir":"Articles","previous_headings":"","what":"PLS in RKHS for X and Y (double RKHS)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Let KXK_X KYK_Y centered Grams XX YY (small ridge ŒªX,ŒªY\\lambda_X,\\lambda_Y stability). cross-covariance operator =KX(KY+ŒªYI)KXA = K_X (K_Y + \\lambda_Y ) K_X. Dual SIMPLS extracts latent directions via dominant eigenspace AA orthogonalization KXK_X inner product. Prediction returns dual coefficients Œ±\\alpha XX Œ≤\\beta YY. bigPLSRalgorithm=\"rkhs_xy\" wires dense mode; streamed variant can built block Gram accumulations KXK_X KYK_Y.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"kalman-filter-pls-kf-pls-streaming","dir":"Articles","previous_headings":"","what":"Kalman-Filter PLS (KF-PLS; streaming)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"KF-PLS maintains state tracks latent parameters incoming mini-batches. Let state s={w,p,q,b}s = \\{w, p, q, b\\} current component, state transition sk+1=sk+œµks_{k+1} = s_k + \\epsilon_k (random walk) ‚Äúmeasurement‚Äù formed current block cross-covariances (X‚ä§tÃÇ,Y‚ä§tÃÇ\\widehat{X^\\top t}, \\widehat{Y^\\top t}). Kalman update: Predict: sÃÇk|k‚àí1=sÃÇk‚àí1,Pk|k‚àí1=Pk‚àí1+QInnovation: ŒΩk=zk‚àíHksÃÇk|k‚àí1,Sk=HkPk|k‚àí1Hk‚ä§+RGain: Kk=Pk|k‚àí1Hk‚ä§Sk‚àí1Update: sÃÇk=sÃÇk|k‚àí1+KkŒΩk,Pk=(‚àíKkHk)Pk|k‚àí1. \\begin{aligned} &\\text{Predict: } \\hat s_{k|k-1} = \\hat s_{k-1},\\ \\ P_{k|k-1}=P_{k-1}+Q \\\\ &\\text{Innovation: } \\nu_k = z_k - H_k \\hat s_{k|k-1},\\ \\ S_k = H_k P_{k|k-1} H_k^\\top + R \\\\ &\\text{Gain: } K_k = P_{k|k-1} H_k^\\top S_k^{-1} \\\\ &\\text{Update: } \\hat s_k = \\hat s_{k|k-1} + K_k \\nu_k,\\ \\ P_k = (- K_k H_k) P_{k|k-1}. \\end{aligned} convergence (patience stop), form t=Xwt = X w, normalize, proceed next component deflation compatible SIMPLS/NIPALS choice. bigPLSRalgorithm=\"kf_pls\" reuses existing chunked T streaming kernel updates state per block.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"api-quick-start","dir":"Articles","previous_headings":"","what":"API quick start","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"","code":"library(bigPLSR)  # Dense RKHS PLS with Nystr√∂m of rank 500 (rbf kernel) fit_rkhs <- pls_fit(X, Y, ncomp = 5,                     backend   = \"arma\",                     algorithm = \"rkhs\",                     kernel = \"rbf\", gamma = 0.5,                     approx = \"nystrom\", approx_rank = 500,                     scores = \"r\")  # Bigmemory, kernel logistic PLS (streamed scores + IRLS) fit_klog <- pls_fit(bmX, bmy, ncomp = 4,                     backend   = \"bigmem\",                     algorithm = \"klogitpls\",                     kernel = \"rbf\", gamma = 1.0,                     chunk_size = 16384L,                     scores = \"r\")  # Sparse KPLS (dense scaffold) fit_sk <- pls_fit(X, Y, ncomp = 5,                   backend = \"arma\",                   algorithm = \"sparse_kpls\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"prediction-in-rkhs-pls","dir":"Articles","previous_headings":"API quick start","what":"Prediction in RKHS PLS","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Let X‚àà‚Ñùn√ópX\\\\mathbb{R}^{n\\times p} training inputs Y‚àà‚Ñùn√ó\\\\mathbb{R}^{n\\times m} responses. kernel k(‚ãÖ,‚ãÖ)k(\\cdot,\\cdot) training Gram KK, centered Gram Kc=K‚àíùüènk‚Äæ‚ä§‚àík‚Äæùüèn‚ä§+k‚Äæ‚Äæ,k‚Äæ=1n‚àë=1nKi‚ãÖ,k‚Äæ‚Äæ=1n2‚àë,jKij. K_c = K - \\mathbf{1}_n \\bar{k}^\\top - \\bar{k}\\,\\mathbf{1}_n^\\top + \\bar{\\bar{k}}, \\quad \\bar{k} = \\frac{1}{n}\\sum_{=1}^n K_{\\cdot},\\quad \\bar{\\bar{k}} = \\frac{1}{n^2}\\sum_{,j}K_{ij}.  KPLS KcK_c yields dual coefficients ‚àà‚Ñùn√ómA\\\\mathbb{R}^{n\\times m}. new inputs $X_\\*$, cross-kernel $K_\\* \\\\mathbb{R}^{n_\\*\\times n}$ $(K_\\*)_{,j} = k(x^\\*_i, x_j)$. centered cross-Gram $$ K_{\\*,c} = K_\\* - \\mathbf{1}_{n_\\*}\\bar{k}^\\top - \\bar{k}_\\*\\mathbf{1}_n^\\top + \\bar{\\bar{k}}, \\quad \\bar{k}_\\* = \\frac{1}{n}K_\\*\\mathbf{1}_n. $$ Predictions follow $$ \\widehat{Y}_\\* = K_{\\*,c}\\,+ \\mathbf{1}_{n_\\*}\\,\\mu_Y^\\top, $$ ŒºY\\mu_Y vector training response means. bigPLSR, stored : dual_coef (AA), k_colmeans (k‚Äæ\\bar{k}), k_mean (k‚Äæ‚Äæ\\bar{\\bar{k}}), y_means (ŒºY\\mu_Y), X_ref (dense training inputs). RKHS branch predict.big_plsr() uses formula.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"dependency-overview-wrappers-c-entry-points","dir":"Articles","previous_headings":"","what":"Dependency overview (wrappers ‚Üí C++ entry points)","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"","code":"pls_fit(algorithm=\"simpls\", backend=\"arma\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_simpls_from_cross\")  pls_fit(algorithm=\"simpls\", backend=\"bigmem\")   ‚îú‚îÄ .Call(\"_bigPLSR_cpp_bigmem_cross\")   ‚îú‚îÄ .Call(\"_bigPLSR_cpp_simpls_from_cross\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_stream_scores_given_W\")  pls_fit(algorithm=\"nipals\", backend=\"arma\")   ‚îî‚îÄ cpp_dense_plsr_nipals()  pls_fit(algorithm=\"nipals\", backend=\"bigmem\")   ‚îî‚îÄ big_plsr_stream_fit_nipals()  pls_fit(algorithm=\"kernelpls\"/\"widekernelpls\")   ‚îî‚îÄ .kernel_pls_core()  (R)  pls_fit(algorithm=\"rkhs\", backend=\"arma\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_kpls_rkhs_dense\")  pls_fit(algorithm=\"rkhs\", backend=\"bigmem\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_kpls_rkhs_bigmem\")  pls_fit(algorithm=\"klogitpls\", backend=\"arma\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_klogit_pls_dense\")  pls_fit(algorithm=\"klogitpls\", backend=\"bigmem\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_klogit_pls_bigmem\")  pls_fit(algorithm=\"sparse_kpls\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_sparse_kpls_dense\")  pls_fit(algorithm=\"rkhs_xy\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_rkhs_xy_dense\")  pls_fit(algorithm=\"kf_pls\")   ‚îî‚îÄ .Call(\"_bigPLSR_cpp_kf_pls_stream\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/kpls_review.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Kernel and Streaming PLS Methods in bigPLSR","text":"Dayal, B., & MacGregor, J.F. (1997). Improved PLS algorithms. Journal Chemometrics, 11(1), 73‚Äì85, doi:10.1002/(SICI)1099-128X(199701)11:1%3C73::AID-CEM446%3E3.0.CO;2-2. Rosipal, R., & Trejo, L.J. (2001). Kernel PLS regression RKHS. Journal Machine Learning Research, 2, 97‚Äì123, doi:10.1162/153244302760200687. Tenenhaus et al., Kernel Logistic PLS. Sparse Kernel Partial Least Squares Regression. LNCS Proceedings. Rosipal et al., RKHS PLS (JMLR) http://www.jmlr.org/papers/v2/rosipal01a.html. Kernel PLS Regression II (double RKHS). IEEE Transactions Neural Networks Learning Systems, doi:10.1109/TNNLS.2019.2932014. KF-PLS (2024) doi:10.1016/j.chemolab.2024.104024","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/plotting-guide.html","id":"example-data","dir":"Articles","previous_headings":"","what":"Example data","title":"Visualising PLS Fits with bigPLSR","text":"","code":"n <- 80; p <- 6 X <- matrix(rnorm(n * p), n, p) Y <- scale(X[, 1:2] %*% matrix(c(0.7, -0.4, 0.5, 0.8), 2, 2) + rnorm(n * 2, sd = 0.15))  groups <- factor(rep(LETTERS[1:4], length.out = n)) fit <- pls_fit(X, Y, ncomp = 3, scores = \"r\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/plotting-guide.html","id":"score-plots-with-ellipses","dir":"Articles","previous_headings":"","what":"Score plots with ellipses","title":"Visualising PLS Fits with bigPLSR","text":"","code":"plot_pls_individuals(fit, comps = c(1, 2), groups = groups,                      ellipse = TRUE, ellipse_level = 0.90,                       main=\"Component scores with 90% ellipses\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/plotting-guide.html","id":"variable-correlations-and-biplots","dir":"Articles","previous_headings":"","what":"Variable correlations and biplots","title":"Visualising PLS Fits with bigPLSR","text":"","code":"plot_pls_variables(fit, comps = c(1, 2), main=\"Correlation circle\") plot_pls_biplot(fit, comps = c(1, 2), groups = groups,                 ellipse = TRUE, ellipse_level = 0.90,                  main=\"Biplot with grouped individuals\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/plotting-guide.html","id":"bootstrap-summaries","dir":"Articles","previous_headings":"","what":"Bootstrap summaries","title":"Visualising PLS Fits with bigPLSR","text":"","code":"boot <- pls_bootstrap(X, Y, ncomp = 2, R = 30, type = \"xy\",                       parallel = \"none\", seed = 99) summary_boot <- summarise_pls_bootstrap(boot) summary_boot #>    variable response         mean         sd percentile_lower percentile_upper #> 1        X1       Y1  0.791966500 0.08475455       0.65091428       0.90289674 #> 2        X2       Y1 -0.380048683 0.08171928      -0.49781216      -0.25457028 #> 3        X3       Y1  0.012627990 0.09703593      -0.18655063       0.17408431 #> 4        X4       Y1 -0.270970620 0.07432352      -0.38062029      -0.15730963 #> 5        X5       Y1  0.080291163 0.12171747      -0.13147528       0.25938757 #> 6        X6       Y1 -0.046823538 0.11212752      -0.22609050       0.17094542 #> 7        X1       Y2  0.441409522 0.06951718       0.29938758       0.54873705 #> 8        X2       Y2  0.680267880 0.05902086       0.57777174       0.77117828 #> 9        X3       Y2  0.024624397 0.05236652      -0.07377591       0.12615176 #> 10       X4       Y2  0.058193995 0.07629797      -0.07393110       0.17107246 #> 11       X5       Y2  0.004567168 0.09882843      -0.16886315       0.15817355 #> 12       X6       Y2 -0.105158254 0.08187656      -0.24889987       0.02755033 #>      bca_lower   bca_upper #> 1   0.63889624  0.90411438 #> 2  -0.49832691 -0.25254384 #> 3  -0.19046850  0.25161840 #> 4  -0.39465965 -0.20957578 #> 5  -0.14827809  0.27595767 #> 6  -0.24286724  0.17122482 #> 7   0.29520940  0.55900911 #> 8   0.63002240  0.78375488 #> 9  -0.08807535  0.13103953 #> 10 -0.09604065  0.17744844 #> 11 -0.15404991  0.23713080 #> 12 -0.25177682  0.01207836 plot_pls_bootstrap_coefficients(boot, main=\"Bootstrap coefficient intervals\") boot <- pls_bootstrap(X, Y, ncomp = 2, R = 30, type = \"xy\",                        parallel = \"none\", seed = 99, return_scores = TRUE) plot_pls_bootstrap_scores(boot,main=\"Bootstrap score dispersion\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls1-benchmark.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Benchmarking PLS1 Implementations","text":"unified pls_fit() interface now drives dense streaming implementations single-response partial least squares regression. vignette revisits benchmarking workflow modern API introduces two complementary perspectives: Internal comparisons contrast dense (-memory) streaming (big-memory) backends pls_fit(). External references recorded popular packages pls mixOmics. results stored package keep vignette lightweight still documenting performance relative wider ecosystem. chunks tagged eval = LOCAL executed environment variable LOCAL set TRUE, allowing CRAN checks skip time-consuming benchmarks.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/pls1-benchmark.html","id":"simulated-data","dir":"Articles","previous_headings":"","what":"Simulated data","title":"Benchmarking PLS1 Implementations","text":"create synthetic regression problem modest latent structure keep dense big-memory versions predictors response can reused benchmarking chunks. example n=4000 en p=50","code":"n <- 1500 p <- 80 ncomp <- 6  X <- bigmemory::big.matrix(nrow = n, ncol = p, type = \"double\") X[,] <- matrix(rnorm(n * p), nrow = n)  y_vec <- scale(X[,] %*% rnorm(p) + rnorm(n))  y <- bigmemory::big.matrix(nrow = n, ncol = 1, type = \"double\") y[,] <- y_vec  X[1:6, 1:6] #>             [,1]       [,2]        [,3]       [,4]       [,5]       [,6] #> [1,] -0.56047565 -0.8209867 -0.15030748  0.8343715 -0.6992281  0.3500025 #> [2,] -0.23017749 -0.3072572 -0.32775713 -0.6984039  0.9964515  0.8144417 #> [3,]  1.55870831 -0.9020980 -1.44816529  1.3092405 -0.6927454 -0.5166661 #> [4,]  0.07050839  0.6270687 -0.69728458 -0.9801776 -0.1034830 -2.6922644 #> [5,]  0.12928774  1.1203550  2.59849023  0.7479851  0.6038661 -1.0969546 #> [6,]  1.71506499  2.1272136 -0.03741501  1.2577966 -0.6080450 -1.2554751 y[1:6,] #> [1]  0.66723250  0.66189719 -0.77458416  0.07452428  0.28174414  0.15756565"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls1-benchmark.html","id":"internal-benchmarks","dir":"Articles","previous_headings":"","what":"Internal benchmarks","title":"Benchmarking PLS1 Implementations","text":"following chunk compares dense vs.¬†streaming fits SIMPLS NIPALS. dense backend receives base R matrices, streaming backend consumes big.matrix objects directly.    results highlight trade-throughput memory usage: SIMPLS shines dense matrices, whereas streaming backend scales larger--memory inputs thanks block processing.","code":"internal_bench <- bench::mark(   dense_simpls = pls_fit(as.matrix(X[]), y_vec, ncomp = ncomp,                          backend = \"arma\", algorithm = \"simpls\"),   streaming_simpls = pls_fit(X, y, ncomp = ncomp, backend = \"bigmem\",                              algorithm = \"simpls\", chunk_size = 512L),   dense_nipals = pls_fit(as.matrix(X[]), y_vec, ncomp = ncomp,                          backend = \"arma\", algorithm = \"nipals\"),   streaming_nipals = pls_fit(X, y, ncomp = ncomp, backend = \"bigmem\",                              algorithm = \"nipals\", chunk_size = 512L),   dense_kernelpls = pls_fit(as.matrix(X[]), y_vec, ncomp = ncomp,                          backend = \"arma\", algorithm = \"kernelpls\"),   streaming_kernelpls = pls_fit(X, y, ncomp = ncomp, backend = \"bigmem\",                              algorithm = \"kernelpls\", chunk_size = 512L),   dense_widekernelpls = pls_fit(as.matrix(X[]), y_vec, ncomp = ncomp,                          backend = \"arma\", algorithm = \"widekernelpls\"),   streaming_widekernelpls = pls_fit(X, y, ncomp = ncomp, backend = \"bigmem\",                              algorithm = \"widekernelpls\", chunk_size = 512L),   iterations = 20,   check = FALSE ) internal_bench_res <-internal_bench[,2:5] internal_bench_res <- as.matrix(internal_bench_res) rownames(internal_bench_res) <- names(internal_bench$expression) dotchart(internal_bench_res[,2], labels=rownames(internal_bench_res),xlab=\"median_time_s\") dotchart(internal_bench_res[,3], labels=rownames(internal_bench_res),xlab=\"itr_per_sec\") dotchart(internal_bench_res[,4], labels=rownames(internal_bench_res),xlab=\"mem_alloc_bytes\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls1-benchmark.html","id":"external-references","dir":"Articles","previous_headings":"","what":"External references","title":"Benchmarking PLS1 Implementations","text":"avoid heavy dependencies build time ship pre-computed benchmark dataset contrasts bigPLSR implementations pls mixOmics packages. dataset generated helper script stored inst/scripts/external_pls_benchmarks.R. table reports median execution times (seconds), number iterations memory use per second representative single-response scenario. notes column indicates additional packages required reproduce measurements.","code":"data(\"external_pls_benchmarks\", package = \"bigPLSR\") sub_pls1 <- subset(external_pls_benchmarks,task==\"pls1\" & !algorithm==\"widekernelpls\") sub_pls1$n <- factor(sub_pls1$n) sub_pls1$p <- factor(sub_pls1$p) sub_pls1$q <- factor(sub_pls1$q) sub_pls1$ncomp <- factor(sub_pls1$ncomp) replications(~package+algorithm+task+n+p+ncomp,data=sub_pls1) #>   package algorithm      task         n         p     ncomp  #>        18        24        72        36        36        24  sub_pls1_wide <- subset(external_pls_benchmarks,external_pls_benchmarks$task==\"pls1\" & algorithm==\"widekernelpls\") sub_pls1_wide$n <- factor(sub_pls1_wide$n) sub_pls1_wide$p <- factor(sub_pls1_wide$p) sub_pls1_wide$q <- factor(sub_pls1_wide$q) sub_pls1_wide$ncomp <- factor(sub_pls1_wide$ncomp) replications(~package+algorithm+task+n+p+ncomp,data=sub_pls1_wide) #>   package algorithm      task         n         p     ncomp  #>         6        24        24        12        12         8  sub_pls2 <- subset(external_pls_benchmarks,external_pls_benchmarks$task==\"pls2\" & !algorithm==\"widekernelpls\") sub_pls2$n <- factor(sub_pls2$n) sub_pls2$p <- factor(sub_pls2$p) sub_pls2$q <- factor(sub_pls2$q) sub_pls2$ncomp <- factor(sub_pls2$ncomp) replications(~package+algorithm+task+n+p+ncomp,data=sub_pls2) #>   package algorithm      task         n         p     ncomp  #>        36        48       144        72        72        48  sub_pls2_wide <- subset(external_pls_benchmarks,external_pls_benchmarks$task==\"pls2\" & algorithm==\"widekernelpls\") sub_pls2_wide$n <- factor(sub_pls2_wide$n) sub_pls2_wide$p <- factor(sub_pls2_wide$p) sub_pls2_wide$q <- factor(sub_pls2_wide$q) sub_pls2_wide$ncomp <- factor(sub_pls2_wide$ncomp) replications(~package+algorithm+task+n+p+ncomp,data=sub_pls2_wide) #>   package algorithm      task         n         p     ncomp  #>        12        48        48        24        24        16 sub_pls1 #>     task algorithm            package median_time_s  itr_per_sec #> 1   pls1    simpls      bigPLSR_dense  0.0067246150  148.0232093 #> 2   pls1    simpls bigPLSR_big.memory  0.0036029980  277.7090750 #> 3   pls1    simpls                pls  0.0022770375  437.1897736 #> 4   pls1    simpls           mixOmics  0.0044901560  222.5633281 #> 5   pls1 kernelpls      bigPLSR_dense  0.0489712611   18.4018038 #> 6   pls1 kernelpls bigPLSR_big.memory  0.0005245131 1908.9476574 #> 7   pls1 kernelpls                pls  0.0021272850  358.4499622 #> 8   pls1 kernelpls           mixOmics  0.0047856634  168.7496829 #> 13  pls1    nipals      bigPLSR_dense  0.0007314811 1205.3254563 #> 14  pls1    nipals bigPLSR_big.memory  0.0063339670  155.1675479 #> 15  pls1    nipals                pls  0.0022903420  434.8982218 #> 16  pls1    nipals           mixOmics  0.0043836380  227.8532589 #> 17  pls1    simpls      bigPLSR_dense  0.0065849486  151.6776964 #> 18  pls1    simpls bigPLSR_big.memory  0.0035743800  278.8872263 #> 19  pls1    simpls                pls  0.0026474111  377.1385860 #> 20  pls1    simpls           mixOmics  0.0079937701  125.1715709 #> 21  pls1 kernelpls      bigPLSR_dense  0.0453674431   22.0422385 #> 22  pls1 kernelpls bigPLSR_big.memory  0.0011308210  865.1415443 #> 23  pls1 kernelpls                pls  0.0026473495  374.7793386 #> 24  pls1 kernelpls           mixOmics  0.0082481339  120.5571369 #> 29  pls1    nipals      bigPLSR_dense  0.0016668550  592.5614418 #> 30  pls1    nipals bigPLSR_big.memory  0.0066213361  147.4796798 #> 31  pls1    nipals                pls  0.0033974855  289.7519881 #> 32  pls1    nipals           mixOmics  0.0083244351  120.0403884 #> 33  pls1    simpls      bigPLSR_dense  0.0067931670  146.1966203 #> 34  pls1    simpls bigPLSR_big.memory  0.0036683520  266.7412212 #> 35  pls1    simpls                pls  0.0047441100  209.7457999 #> 36  pls1    simpls           mixOmics  0.0212875688   47.2762356 #> 37  pls1 kernelpls      bigPLSR_dense  0.0502987590   19.7701241 #> 38  pls1 kernelpls bigPLSR_big.memory  0.0032584749  304.0034685 #> 39  pls1 kernelpls                pls  0.0047713750  210.3620345 #> 40  pls1 kernelpls           mixOmics  0.0212102840   47.2352685 #> 45  pls1    nipals      bigPLSR_dense  0.0049670886  195.0587514 #> 46  pls1    nipals bigPLSR_big.memory  0.0066567600  147.2806571 #> 47  pls1    nipals                pls  0.0075556440  132.2163658 #> 48  pls1    nipals           mixOmics  0.0209695730   48.0368106 #> 145 pls1    simpls      bigPLSR_dense  6.3196013995    0.1560965 #> 146 pls1    simpls bigPLSR_big.memory  3.4780965225    0.2882154 #> 147 pls1    simpls                pls  0.3617566530    2.8113887 #> 148 pls1    simpls           mixOmics  0.3419937920    2.6744942 #> 149 pls1 kernelpls      bigPLSR_dense  0.0875219005   11.4002975 #> 150 pls1 kernelpls bigPLSR_big.memory  0.0572001660   15.2469776 #> 151 pls1 kernelpls                pls  0.3495531260    3.1086178 #> 152 pls1 kernelpls           mixOmics  0.4555282245    2.2243872 #> 157 pls1    nipals      bigPLSR_dense  0.0793887920   12.4715205 #> 158 pls1    nipals bigPLSR_big.memory  6.2405680775    0.1575006 #> 159 pls1    nipals                pls  0.2215801335    4.4212757 #> 160 pls1    nipals           mixOmics  0.3240520485    3.0859240 #> 161 pls1    simpls      bigPLSR_dense  6.6035757635    0.1511993 #> 162 pls1    simpls bigPLSR_big.memory  3.6277102170    0.2762871 #> 163 pls1    simpls                pls  0.2944476500    2.9759450 #> 164 pls1    simpls           mixOmics  0.7795254970    1.3364046 #> 165 pls1 kernelpls      bigPLSR_dense  0.2228726995    4.4778063 #> 166 pls1 kernelpls bigPLSR_big.memory  0.1308061540    7.5146125 #> 167 pls1 kernelpls                pls  0.2958856020    3.0274729 #> 168 pls1 kernelpls           mixOmics  0.7937342930    1.2842244 #> 173 pls1    nipals      bigPLSR_dense  0.1926474585    5.1410083 #> 174 pls1    nipals bigPLSR_big.memory  6.2397971545    0.1605795 #> 175 pls1    nipals                pls  0.3588060470    2.7022458 #> 176 pls1    nipals           mixOmics  0.6335405281    1.4952876 #> 177 pls1    simpls      bigPLSR_dense  6.2928044145    0.1585666 #> 178 pls1    simpls bigPLSR_big.memory  3.4739361500    0.2879672 #> 179 pls1    simpls                pls  0.5703391100    1.8750414 #> 180 pls1    simpls           mixOmics  1.8980518270    0.5262915 #> 181 pls1 kernelpls      bigPLSR_dense  0.6460428016    1.5505732 #> 182 pls1 kernelpls bigPLSR_big.memory  0.3653160885    2.6907637 #> 183 pls1 kernelpls                pls  0.5757799330    1.7536473 #> 184 pls1 kernelpls           mixOmics  1.7895776350    0.5399431 #> 189 pls1    nipals      bigPLSR_dense  0.5255408905    1.8317478 #> 190 pls1    nipals bigPLSR_big.memory  6.2838405435    0.1591461 #> 191 pls1    nipals                pls  0.7320582391    1.3418031 #> 192 pls1    nipals           mixOmics  1.6411711855    0.6002285 #>     mem_alloc_bytes     n    p q ncomp #> 1           1850464  1000  100 1     1 #> 2            944384  1000  100 1     1 #> 3           8056600  1000  100 1     1 #> 4           8768696  1000  100 1     1 #> 5          42579912  1000  100 1     1 #> 6            821328  1000  100 1     1 #> 7           7570072  1000  100 1     1 #> 8           7550384  1000  100 1     1 #> 13            22752  1000  100 1     1 #> 14           835624  1000  100 1     1 #> 15          8377136  1000  100 1     1 #> 16          7550384  1000  100 1     1 #> 17          1723264  1000  100 1     3 #> 18           901520  1000  100 1     3 #> 19          7764576  1000  100 1     3 #> 20         12407296  1000  100 1     3 #> 21         42480456  1000  100 1     3 #> 22           840528  1000  100 1     3 #> 23          7720248  1000  100 1     3 #> 24         12304368  1000  100 1     3 #> 29            18128  1000  100 1     3 #> 30           865424  1000  100 1     3 #> 31         10122568  1000  100 1     3 #> 32         12304368  1000  100 1     3 #> 33          1734464  1000  100 1    10 #> 34           922096  1000  100 1    10 #> 35          9013040  1000  100 1    10 #> 36         28992768  1000  100 1    10 #> 37         42547656  1000  100 1    10 #> 38           907728  1000  100 1    10 #> 39          8959992  1000  100 1    10 #> 40         28992768  1000  100 1    10 #> 45            29328  1000  100 1    10 #> 46           988624  1000  100 1    10 #> 47         16876368  1000  100 1    10 #> 48         28992768  1000  100 1    10 #> 145       168378464 10000 1000 1     1 #> 146        88146320 10000 1000 1     1 #> 147       722792304 10000 1000 1     1 #> 148       651508960 10000 1000 1     1 #> 149          282544 10000 1000 1     1 #> 150        80194128 10000 1000 1     1 #> 151       722624584 10000 1000 1     1 #> 152       651508960 10000 1000 1     1 #> 157          130128 10000 1000 1     1 #> 158        80282224 10000 1000 1     1 #> 159       802652584 10000 1000 1     1 #> 160       651508960 10000 1000 1     1 #> 161       168410464 10000 1000 1     3 #> 162        88194320 10000 1000 1     3 #> 163       725626656 10000 1000 1     3 #> 164       986912144 10000 1000 1     3 #> 165          474544 10000 1000 1     3 #> 166        80386128 10000 1000 1     3 #> 167       725179128 10000 1000 1     3 #> 168       986912144 10000 1000 1     3 #> 173          162128 10000 1000 1     3 #> 174        80634224 10000 1000 1     3 #> 175       965206648 10000 1000 1     3 #> 176       986912144 10000 1000 1     3 #> 177       168522464 10000 1000 1    10 #> 178        88366096 10000 1000 1    10 #> 179       738038720 10000 1000 1    10 #> 180      2161313744 10000 1000 1    10 #> 181         1146544 10000 1000 1    10 #> 182        81058128 10000 1000 1    10 #> 183       737488872 10000 1000 1    10 #> 184      2161313744 10000 1000 1    10 #> 189          274128 10000 1000 1    10 #> 190        81866224 10000 1000 1    10 #> 191      1536642048 10000 1000 1    10 #> 192      2161313744 10000 1000 1    10 #>                                         notes #> 1        Run via pls_fit() with dense backend #> 2   Run via pls_fit() with big.memory backend #> 3                    Requires the pls package #> 4               Requires the mixOmics package #> 5        Run via pls_fit() with dense backend #> 6   Run via pls_fit() with big.memory backend #> 7                    Requires the pls package #> 8               Requires the mixOmics package #> 13       Run via pls_fit() with dense backend #> 14  Run via pls_fit() with big.memory backend #> 15                   Requires the pls package #> 16              Requires the mixOmics package #> 17       Run via pls_fit() with dense backend #> 18  Run via pls_fit() with big.memory backend #> 19                   Requires the pls package #> 20              Requires the mixOmics package #> 21       Run via pls_fit() with dense backend #> 22  Run via pls_fit() with big.memory backend #> 23                   Requires the pls package #> 24              Requires the mixOmics package #> 29       Run via pls_fit() with dense backend #> 30  Run via pls_fit() with big.memory backend #> 31                   Requires the pls package #> 32              Requires the mixOmics package #> 33       Run via pls_fit() with dense backend #> 34  Run via pls_fit() with big.memory backend #> 35                   Requires the pls package #> 36              Requires the mixOmics package #> 37       Run via pls_fit() with dense backend #> 38  Run via pls_fit() with big.memory backend #> 39                   Requires the pls package #> 40              Requires the mixOmics package #> 45       Run via pls_fit() with dense backend #> 46  Run via pls_fit() with big.memory backend #> 47                   Requires the pls package #> 48              Requires the mixOmics package #> 145      Run via pls_fit() with dense backend #> 146 Run via pls_fit() with big.memory backend #> 147                  Requires the pls package #> 148             Requires the mixOmics package #> 149      Run via pls_fit() with dense backend #> 150 Run via pls_fit() with big.memory backend #> 151                  Requires the pls package #> 152             Requires the mixOmics package #> 157      Run via pls_fit() with dense backend #> 158 Run via pls_fit() with big.memory backend #> 159                  Requires the pls package #> 160             Requires the mixOmics package #> 161      Run via pls_fit() with dense backend #> 162 Run via pls_fit() with big.memory backend #> 163                  Requires the pls package #> 164             Requires the mixOmics package #> 165      Run via pls_fit() with dense backend #> 166 Run via pls_fit() with big.memory backend #> 167                  Requires the pls package #> 168             Requires the mixOmics package #> 173      Run via pls_fit() with dense backend #> 174 Run via pls_fit() with big.memory backend #> 175                  Requires the pls package #> 176             Requires the mixOmics package #> 177      Run via pls_fit() with dense backend #> 178 Run via pls_fit() with big.memory backend #> 179                  Requires the pls package #> 180             Requires the mixOmics package #> 181      Run via pls_fit() with dense backend #> 182 Run via pls_fit() with big.memory backend #> 183                  Requires the pls package #> 184             Requires the mixOmics package #> 189      Run via pls_fit() with dense backend #> 190 Run via pls_fit() with big.memory backend #> 191                  Requires the pls package #> 192             Requires the mixOmics package"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls1-benchmark.html","id":"takeaways","dir":"Articles","previous_headings":"","what":"Takeaways","title":"Benchmarking PLS1 Implementations","text":"Dense vs streaming backends. small/medium data fits RAM, -memory implementations (e.g., pls) typically fastest (median ‚âà0.36 s SIMPLS runs). However, materialize large cross-products/Gram matrices memory grows O(p^2) (O(n^2) kernel views). contrast, bigPLSR‚Äôs streaming big-memory backend keeps memory bounded via chunked BLAS never forms intermediates. PLS2 benchmark, streaming used ~7‚Äì8√ó less RAM pls (‚âà89 MB vs ‚âà732 MB median) remaining competitive runtime (‚âà3.5 s vs 0.36 s). PLS1 shows pattern: streaming often fast enough dramatically reducing memory. n p grow, streaming backend scales dense approaches become memory-limited.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/pls2-benchmark.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Benchmarking PLS2 Implementations","text":"package offers dense (pls2_dense) streaming (pls2_stream) solvers multi-response partial least squares regression (PLS2). vignette demonstrates benchmark variants synthetic dataset featuring three correlated response variables.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/pls2-benchmark.html","id":"recent-additions","dir":"Articles","previous_headings":"Overview","what":"Recent additions","title":"Benchmarking PLS2 Implementations","text":"Beyond dense streaming SIMPLS/NIPALS solvers, bigPLSR now ships Kalman-filter PLS (algorithm = \"kf_pls\"), double-RKHS modelling (algorithm = \"rkhs_xy\") optional coefficient thresholding. resampling helpers (pls_cross_validate(), pls_bootstrap()) can also leverage future ecosystem parallel execution. benchmark variants simply change algorithm parameter chunks , example: remember reset future plan enabling parallelism: Multi-response benchmarks follow principles PLS1 case. focus pls_fit() API contrast dense streaming backends reporting stored results third-party packages.","code":"bench::mark(   dense = pls_fit(X[], Y_mat, ncomp = ncomp, algorithm = \"rkhs_xy\"),   streaming = pls_fit(X, Y, ncomp = ncomp, backend = \"bigmem\",                       algorithm = \"kf_pls\", chunk_size = 1024L) ) future::plan(future::multisession, workers = 2) pls_cross_validate(X[], Y_mat, ncomp = 4, folds = 3,                    parallel = TRUE) future::plan(future::sequential)"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls2-benchmark.html","id":"simulated-data","dir":"Articles","previous_headings":"","what":"Simulated data","title":"Benchmarking PLS2 Implementations","text":"","code":"n <- 1200 p <- 60 q <- 3 ncomp <- 4  X <- bigmemory::big.matrix(nrow = n, ncol = p, type = \"double\") X[,] <- matrix(rnorm(n * p), nrow = n)  loading_matrix <- matrix(rnorm(p * q), nrow = p) latent_scores <- matrix(rnorm(n * q), nrow = n) Y_mat <- scale(latent_scores %*% t(loading_matrix[1:q, , drop = FALSE]) +                  matrix(rnorm(n * q, sd = 0.5), nrow = n))  Y <- bigmemory::big.matrix(nrow = n, ncol = q, type = \"double\") Y[,] <- Y_mat  X[1:6, 1:6] #>            [,1]         [,2]        [,3]       [,4]        [,5]       [,6] #> [1,] -1.3435214 -0.348899457  0.70772263  1.1760806  0.05196595  0.6838164 #> [2,]  0.6217756  1.068279438 -1.17880479 -0.8208425 -1.50933535 -0.8963854 #> [3,]  0.8008747 -0.005793261 -0.04600936 -0.8817557  0.17372573 -1.2315819 #> [4,] -1.3888924  0.560411440 -1.15682882 -1.1538845 -1.26526869 -0.3837956 #> [5,] -0.7143569  2.533318058 -1.47324626  0.2554595 -0.25199313  1.1685923 #> [6,] -0.3240611  0.436737176  0.48330946  0.2118484  0.47779676  1.1944133 Y[1:6, 1:min(6, q)] #>            [,1]         [,2]        [,3] #> [1,]  1.8176535 -0.431537115 -0.24548255 #> [2,]  1.1617501 -1.573377901 -1.21506233 #> [3,] -0.4016607  0.412473967 -0.03420375 #> [4,] -0.9068912  1.328097496  0.67450957 #> [5,]  1.2561929 -0.839498415 -1.71712784 #> [6,]  0.5802884 -0.006450224  1.41649712"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls2-benchmark.html","id":"internal-benchmarks","dir":"Articles","previous_headings":"","what":"Internal benchmarks","title":"Benchmarking PLS2 Implementations","text":"dense path excels memory allows, whereas streaming backend prioritises scalability via block-wise processing.","code":"internal_bench <- bench::mark(   dense_simpls = pls_fit(as.matrix(X[]), Y_mat, ncomp = ncomp,                          backend = \"arma\", algorithm = \"simpls\"),   streaming_simpls = pls_fit(X, Y, ncomp = ncomp, backend = \"bigmem\",                              algorithm = \"simpls\", chunk_size = 512L),   dense_nipals = pls_fit(as.matrix(X[]), Y_mat, ncomp = ncomp,                          backend = \"arma\", algorithm = \"nipals\"),   streaming_nipals = pls_fit(X, Y, ncomp = ncomp, backend = \"bigmem\",                              algorithm = \"nipals\", chunk_size = 512L),   iterations = 15,   check = FALSE ) internal_bench #> # A tibble: 4 √ó 6 #>   expression            min   median `itr/sec` mem_alloc `gc/sec` #>   <bch:expr>       <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> #> 1 dense_simpls          3ms   3.06ms     326.     2.04MB        0 #> 2 streaming_simpls   1.66ms   1.68ms     592.   628.94KB        0 #> 3 dense_nipals       9.86ms   9.96ms     100.   574.56KB        0 #> 4 streaming_nipals  27.85ms  28.03ms      34.0  658.29KB        0"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls2-benchmark.html","id":"external-references","dir":"Articles","previous_headings":"","what":"External references","title":"Benchmarking PLS2 Implementations","text":"stored table mirrors structure PLS1 benchmark produced script inst/scripts/external_pls_benchmarks.R.","code":"data(\"external_pls_benchmarks\", package = \"bigPLSR\") subset(external_pls_benchmarks, task == \"pls2\") #>     task     algorithm            package median_time_s  itr_per_sec #> 49  pls2        simpls      bigPLSR_dense  7.703859e-03 1.294665e+02 #> 50  pls2        simpls bigPLSR_big.memory  3.822082e-03 2.590474e+02 #> 51  pls2        simpls                pls  3.370282e-03 2.963716e+02 #> 52  pls2        simpls           mixOmics  5.738852e-03 1.735170e+02 #> 53  pls2     kernelpls      bigPLSR_dense  5.006055e-02 2.003939e+01 #> 54  pls2     kernelpls bigPLSR_big.memory  5.662920e-04 1.576432e+03 #> 55  pls2     kernelpls                pls  3.379897e-03 2.934735e+02 #> 56  pls2     kernelpls           mixOmics  5.761976e-03 1.730127e+02 #> 57  pls2 widekernelpls      bigPLSR_dense  4.610229e-02 2.162748e+01 #> 58  pls2 widekernelpls bigPLSR_big.memory  2.174968e-03 4.581932e+02 #> 59  pls2 widekernelpls                pls  2.499344e-02 4.028059e+01 #> 60  pls2 widekernelpls           mixOmics  1.913841e-02 5.235320e+01 #> 61  pls2        nipals      bigPLSR_dense  1.299575e-02 7.678785e+01 #> 62  pls2        nipals bigPLSR_big.memory  2.462358e-02 3.967627e+01 #> 63  pls2        nipals                pls  2.664418e-02 3.760858e+01 #> 64  pls2        nipals           mixOmics  5.736310e-03 1.723401e+02 #> 65  pls2        simpls      bigPLSR_dense  7.719398e-03 1.293323e+02 #> 66  pls2        simpls bigPLSR_big.memory  3.971506e-03 2.412338e+02 #> 67  pls2        simpls                pls  4.198913e-03 2.413866e+02 #> 68  pls2        simpls           mixOmics  1.150120e-02 8.662380e+01 #> 69  pls2     kernelpls      bigPLSR_dense  6.406447e-02 1.562173e+01 #> 70  pls2     kernelpls bigPLSR_big.memory  1.202838e-03 7.628686e+02 #> 71  pls2     kernelpls                pls  4.269535e-03 2.351552e+02 #> 72  pls2     kernelpls           mixOmics  1.175333e-02 8.482315e+01 #> 73  pls2 widekernelpls      bigPLSR_dense  8.177622e-02 1.223048e+01 #> 74  pls2 widekernelpls bigPLSR_big.memory  5.333751e-03 1.849824e+02 #> 75  pls2 widekernelpls                pls  3.072517e-02 3.282535e+01 #> 76  pls2 widekernelpls           mixOmics  3.972584e-02 2.526346e+01 #> 77  pls2        nipals      bigPLSR_dense  4.483832e-02 2.215566e+01 #> 78  pls2        nipals bigPLSR_big.memory  1.226770e-01 8.107545e+00 #> 79  pls2        nipals                pls  7.335408e-02 1.363430e+01 #> 80  pls2        nipals           mixOmics  1.186358e-02 8.459785e+01 #> 81  pls2        simpls      bigPLSR_dense  8.050924e-03 1.239772e+02 #> 82  pls2        simpls bigPLSR_big.memory  4.145367e-03 2.385902e+02 #> 83  pls2        simpls                pls  7.349168e-03 1.360419e+02 #> 84  pls2        simpls           mixOmics  3.328077e-02 3.013901e+01 #> 85  pls2     kernelpls      bigPLSR_dense  1.266730e-01 7.813530e+00 #> 86  pls2     kernelpls bigPLSR_big.memory  3.431392e-03 2.876769e+02 #> 87  pls2     kernelpls                pls  7.299107e-03 1.372812e+02 #> 88  pls2     kernelpls           mixOmics  3.280693e-02 3.050771e+01 #> 89  pls2 widekernelpls      bigPLSR_dense  2.555697e-01 3.875118e+00 #> 90  pls2 widekernelpls bigPLSR_big.memory  1.635088e-02 5.865711e+01 #> 91  pls2 widekernelpls                pls  6.454206e-02 1.225868e+01 #> 92  pls2 widekernelpls           mixOmics  1.271702e-01 6.630726e+00 #> 93  pls2        nipals      bigPLSR_dense  1.286592e-01 7.727614e+00 #> 94  pls2        nipals bigPLSR_big.memory  6.568389e-01 1.515447e+00 #> 95  pls2        nipals                pls  1.973329e-01 5.052147e+00 #> 96  pls2        nipals           mixOmics  3.459330e-02 2.903751e+01 #> 97  pls2        simpls      bigPLSR_dense  2.032386e-02 4.879779e+01 #> 98  pls2        simpls bigPLSR_big.memory  8.678818e-03 1.138255e+02 #> 99  pls2        simpls                pls  1.776419e-02 5.643012e+01 #> 100 pls2        simpls           mixOmics  1.896234e-02 5.283046e+01 #> 101 pls2     kernelpls      bigPLSR_dense  5.927104e-02 1.681917e+01 #> 102 pls2     kernelpls bigPLSR_big.memory  9.061000e-04 1.018776e+03 #> 103 pls2     kernelpls                pls  1.768998e-02 5.675683e+01 #> 104 pls2     kernelpls           mixOmics  1.885770e-02 5.317274e+01 #> 105 pls2 widekernelpls      bigPLSR_dense  4.738712e-02 2.099453e+01 #> 106 pls2 widekernelpls bigPLSR_big.memory  3.320098e-03 3.034152e+02 #> 107 pls2 widekernelpls                pls  2.633553e-02 3.784673e+01 #> 108 pls2 widekernelpls           mixOmics  7.408499e-02 1.346245e+01 #> 109 pls2        nipals      bigPLSR_dense  5.611307e-02 1.778514e+01 #> 110 pls2        nipals bigPLSR_big.memory  1.319449e-01 7.574465e+00 #> 111 pls2        nipals                pls  4.722474e-02 2.115455e+01 #> 112 pls2        nipals           mixOmics  1.904216e-02 5.250928e+01 #> 113 pls2        simpls      bigPLSR_dense  2.373109e-02 4.212195e+01 #> 114 pls2        simpls bigPLSR_big.memory  1.205634e-02 8.242741e+01 #> 115 pls2        simpls                pls  2.415490e-02 4.170840e+01 #> 116 pls2        simpls           mixOmics  4.840575e-02 2.068831e+01 #> 117 pls2     kernelpls      bigPLSR_dense  9.049137e-02 1.100459e+01 #> 118 pls2     kernelpls bigPLSR_big.memory  1.842827e-03 5.217241e+02 #> 119 pls2     kernelpls                pls  2.330266e-02 4.292969e+01 #> 120 pls2     kernelpls           mixOmics  4.826778e-02 2.075034e+01 #> 121 pls2 widekernelpls      bigPLSR_dense  1.019824e-01 9.534021e+00 #> 122 pls2 widekernelpls bigPLSR_big.memory  7.138079e-03 1.245259e+02 #> 123 pls2 widekernelpls                pls  4.662245e-02 1.409017e+01 #> 124 pls2 widekernelpls           mixOmics  2.107637e-01 4.590003e+00 #> 125 pls2        nipals      bigPLSR_dense  2.946202e-01 3.390249e+00 #> 126 pls2        nipals bigPLSR_big.memory  6.936715e-01 1.440695e+00 #> 127 pls2        nipals                pls  1.361026e-01 7.345256e+00 #> 128 pls2        nipals           mixOmics  4.842075e-02 2.071722e+01 #> 129 pls2        simpls      bigPLSR_dense  3.680062e-02 2.713059e+01 #> 130 pls2        simpls bigPLSR_big.memory  2.383662e-02 4.192946e+01 #> 131 pls2        simpls                pls  4.651200e-02 2.149983e+01 #> 132 pls2        simpls           mixOmics  1.509870e-01 6.620748e+00 #> 133 pls2     kernelpls      bigPLSR_dense  1.967282e-01 5.075983e+00 #> 134 pls2     kernelpls bigPLSR_big.memory  5.286191e-03 1.857936e+02 #> 135 pls2     kernelpls                pls  4.514108e-02 2.215277e+01 #> 136 pls2     kernelpls           mixOmics  1.513856e-01 6.617886e+00 #> 137 pls2 widekernelpls      bigPLSR_dense  2.904323e-01 3.418831e+00 #> 138 pls2 widekernelpls bigPLSR_big.memory  1.949292e-02 4.789851e+01 #> 139 pls2 widekernelpls                pls  2.320513e-01 4.358741e+00 #> 140 pls2 widekernelpls           mixOmics  6.805235e-01 1.442981e+00 #> 141 pls2        nipals      bigPLSR_dense  9.436001e-01 1.057849e+00 #> 142 pls2        nipals bigPLSR_big.memory  3.825992e+00 2.605992e-01 #> 143 pls2        nipals                pls  4.494321e-01 2.226745e+00 #> 144 pls2        nipals           mixOmics  1.517212e-01 6.576826e+00 #> 193 pls2        simpls      bigPLSR_dense  6.372017e+00 1.571230e-01 #> 194 pls2        simpls bigPLSR_big.memory  3.472076e+00 2.887196e-01 #> 195 pls2        simpls                pls  4.775195e-01 2.191370e+00 #> 196 pls2        simpls           mixOmics  5.650414e-01 1.852813e+00 #> 197 pls2     kernelpls      bigPLSR_dense  9.472596e-01 9.349346e-01 #> 198 pls2     kernelpls bigPLSR_big.memory  5.619858e-02 1.720970e+01 #> 199 pls2     kernelpls                pls  4.310281e-01 2.281177e+00 #> 200 pls2     kernelpls           mixOmics  5.310626e-01 1.868872e+00 #> 201 pls2 widekernelpls      bigPLSR_dense  2.113579e+01 4.747788e-02 #> 202 pls2 widekernelpls bigPLSR_big.memory  3.502788e-01 2.805631e+00 #> 203 pls2 widekernelpls                pls  1.949436e+01 5.129441e-02 #> 204 pls2 widekernelpls           mixOmics  2.167178e+00 4.542438e-01 #> 205 pls2        nipals      bigPLSR_dense  2.266407e+00 4.150288e-01 #> 206 pls2        nipals bigPLSR_big.memory  6.571526e+00 1.518724e-01 #> 207 pls2        nipals                pls  2.581104e+00 3.879612e-01 #> 208 pls2        nipals           mixOmics  4.324950e-01 2.308658e+00 #> 209 pls2        simpls      bigPLSR_dense  6.395176e+00 1.563647e-01 #> 210 pls2        simpls bigPLSR_big.memory  3.498311e+00 2.848683e-01 #> 211 pls2        simpls                pls  3.651399e-01 2.726946e+00 #> 212 pls2        simpls           mixOmics  9.399521e-01 1.064294e+00 #> 213 pls2     kernelpls      bigPLSR_dense  2.429177e+00 3.922059e-01 #> 214 pls2     kernelpls bigPLSR_big.memory  1.254864e-01 7.954267e+00 #> 215 pls2     kernelpls                pls  3.687574e-01 2.716657e+00 #> 216 pls2     kernelpls           mixOmics  9.401213e-01 1.063693e+00 #> 217 pls2 widekernelpls      bigPLSR_dense  2.659180e+01 3.762475e-02 #> 218 pls2 widekernelpls bigPLSR_big.memory  8.806437e-01 1.125303e+00 #> 219 pls2 widekernelpls                pls  2.366234e+01 4.270419e-02 #> 220 pls2 widekernelpls           mixOmics  4.668842e+00 2.159230e-01 #> 221 pls2        nipals      bigPLSR_dense  5.934811e+00 1.678093e-01 #> 222 pls2        nipals bigPLSR_big.memory  2.078671e+01 4.787247e-02 #> 223 pls2        nipals                pls  6.049018e+00 1.646965e-01 #> 224 pls2        nipals           mixOmics  9.546307e-01 1.046914e+00 #> 225 pls2        simpls      bigPLSR_dense  6.514164e+00 1.530249e-01 #> 226 pls2        simpls bigPLSR_big.memory  3.476311e+00 2.880237e-01 #> 227 pls2        simpls                pls  5.870805e-01 1.645529e+00 #> 228 pls2        simpls           mixOmics  2.859591e+00 3.477132e-01 #> 229 pls2     kernelpls      bigPLSR_dense  8.233163e+00 1.211384e-01 #> 230 pls2     kernelpls bigPLSR_big.memory  3.788551e-01 2.629733e+00 #> 231 pls2     kernelpls                pls  7.056001e-01 1.457069e+00 #> 232 pls2     kernelpls           mixOmics  3.074567e+00 3.249551e-01 #> 233 pls2 widekernelpls      bigPLSR_dense  5.247545e+01 1.905195e-02 #> 234 pls2 widekernelpls bigPLSR_big.memory  2.865286e+00 3.475069e-01 #> 235 pls2 widekernelpls                pls  3.751003e+01 2.668356e-02 #> 236 pls2 widekernelpls           mixOmics  1.294977e+01 7.714622e-02 #> 237 pls2        nipals      bigPLSR_dense  1.418784e+01 6.861025e-02 #> 238 pls2        nipals bigPLSR_big.memory  9.487661e+01 1.052749e-02 #> 239 pls2        nipals                pls  2.114819e+01 4.722697e-02 #> 240 pls2        nipals           mixOmics  3.044969e+00 3.303240e-01 #> 241 pls2        simpls      bigPLSR_dense  7.703874e+00 1.296015e-01 #> 242 pls2        simpls bigPLSR_big.memory  3.780275e+00 2.646158e-01 #> 243 pls2        simpls                pls  1.637158e+00 6.121031e-01 #> 244 pls2        simpls           mixOmics  1.735009e+00 5.870069e-01 #> 245 pls2     kernelpls      bigPLSR_dense  9.937571e-01 1.003997e+00 #> 246 pls2     kernelpls bigPLSR_big.memory  6.376103e-02 1.385058e+01 #> 247 pls2     kernelpls                pls  1.636113e+00 6.101758e-01 #> 248 pls2     kernelpls           mixOmics  1.736925e+00 5.860221e-01 #> 249 pls2 widekernelpls      bigPLSR_dense  2.099252e+01 4.752287e-02 #> 250 pls2 widekernelpls bigPLSR_big.memory  3.755343e-01 2.625361e+00 #> 251 pls2 widekernelpls                pls  1.770315e+01 5.671672e-02 #> 252 pls2 widekernelpls           mixOmics  7.913064e+00 1.261158e-01 #> 253 pls2        nipals      bigPLSR_dense  3.185903e+00 3.131190e-01 #> 254 pls2        nipals bigPLSR_big.memory  9.254061e+00 1.063789e-01 #> 255 pls2        nipals                pls  2.881640e+00 3.456227e-01 #> 256 pls2        nipals           mixOmics  1.614169e+00 6.213729e-01 #> 257 pls2        simpls      bigPLSR_dense  7.645301e+00 1.306593e-01 #> 258 pls2        simpls bigPLSR_big.memory  3.753011e+00 2.664389e-01 #> 259 pls2        simpls                pls  1.689489e+00 5.968439e-01 #> 260 pls2        simpls           mixOmics  4.516086e+00 2.204282e-01 #> 261 pls2     kernelpls      bigPLSR_dense  2.993731e+00 3.331780e-01 #> 262 pls2     kernelpls bigPLSR_big.memory  1.414862e-01 6.733593e+00 #> 263 pls2     kernelpls                pls  1.639517e+00 6.035807e-01 #> 264 pls2     kernelpls           mixOmics  4.369330e+00 2.278782e-01 #> 265 pls2 widekernelpls      bigPLSR_dense  2.826351e+01 3.504447e-02 #> 266 pls2 widekernelpls bigPLSR_big.memory  9.084143e-01 1.087509e+00 #> 267 pls2 widekernelpls                pls  2.101756e+01 4.558670e-02 #> 268 pls2 widekernelpls           mixOmics  2.059091e+01 4.857424e-02 #> 269 pls2        nipals      bigPLSR_dense  1.264592e+01 7.697070e-02 #> 270 pls2        nipals bigPLSR_big.memory  5.290264e+01 1.897654e-02 #> 271 pls2        nipals                pls  8.000667e+00 1.251130e-01 #> 272 pls2        nipals           mixOmics  4.458265e+00 2.254346e-01 #> 273 pls2        simpls      bigPLSR_dense  7.743644e+00 1.286048e-01 #> 274 pls2        simpls bigPLSR_big.memory  3.897823e+00 2.559043e-01 #> 275 pls2        simpls                pls  1.912496e+00 5.196822e-01 #> 276 pls2        simpls           mixOmics  1.446949e+01 6.916947e-02 #> 277 pls2     kernelpls      bigPLSR_dense  9.659917e+00 1.029904e-01 #> 278 pls2     kernelpls bigPLSR_big.memory  3.957490e-01 2.459442e+00 #> 279 pls2     kernelpls                pls  2.013304e+00 4.870815e-01 #> 280 pls2     kernelpls           mixOmics  1.441765e+01 6.871777e-02 #> 281 pls2 widekernelpls      bigPLSR_dense  5.711213e+01 1.753805e-02 #> 282 pls2 widekernelpls bigPLSR_big.memory  2.793954e+00 3.546727e-01 #> 283 pls2 widekernelpls                pls  3.886405e+01 2.591319e-02 #> 284 pls2 widekernelpls           mixOmics  6.853456e+01 1.461738e-02 #> 285 pls2        nipals      bigPLSR_dense  9.142437e+01 1.084685e-02 #> 286 pls2        nipals bigPLSR_big.memory  4.005222e+02 2.512184e-03 #> 287 pls2        nipals                pls  2.653060e+01 3.788249e-02 #> 288 pls2        nipals           mixOmics  1.431443e+01 6.961404e-02 #>     mem_alloc_bytes     n     p   q ncomp #> 49          1862368  1000   100  10     1 #> 50           983120  1000   100  10     1 #> 51          8479304  1000   100  10     1 #> 52          8318896  1000   100  10     1 #> 53         42452360  1000   100  10     1 #> 54           900528  1000   100  10     1 #> 55          8436448  1000   100  10     1 #> 56          8190272  1000   100  10     1 #> 57         12968760   100  5000  10     1 #> 58          4530928   100  5000  10     1 #> 59         35304944   100  5000  10     1 #> 60         34578992   100  5000  10     1 #> 61            14080  1000   100  10     1 #> 62           915560  1000   100  10     1 #> 63         13485848  1000   100  10     1 #> 64          8190272  1000   100  10     1 #> 65          1865856  1000   100  10     3 #> 66           988208  1000   100  10     3 #> 67          9444440  1000   100  10     3 #> 68         13388032  1000   100  10     3 #> 69         42471848  1000   100  10     3 #> 70           920016  1000   100  10     3 #> 71          9402656  1000   100  10     3 #> 72         13388032  1000   100  10     3 #> 73         13130648   100  5000  10     3 #> 74          4692816   100  5000  10     3 #> 75         38990776   100  5000  10     3 #> 76         54606752   100  5000  10     3 #> 77            17568  1000   100  10     3 #> 78           931360  1000   100  10     3 #> 79         24181160  1000   100  10     3 #> 80         13388032  1000   100  10     3 #> 81          1877616  1000   100  10    10 #> 82          1009344  1000   100  10    10 #> 83         12918736  1000   100  10    10 #> 84         31625968  1000   100  10    10 #> 85         42539608  1000   100  10    10 #> 86           987776  1000   100  10    10 #> 87         12874168  1000   100  10    10 #> 88         31625968  1000   100  10    10 #> 89         13696808   100  5000  10    10 #> 90          5258976   100  5000  10    10 #> 91         53420288   100  5000  10    10 #> 92        127150688   100  5000  10    10 #> 93            29328  1000   100  10    10 #> 94          1004720  1000   100  10    10 #> 95         54567512  1000   100  10    10 #> 96         31625968  1000   100  10    10 #> 97          3451456  1000   100 100     1 #> 98          1852208  1000   100 100     1 #> 99         18594944  1000   100 100     1 #> 100        14977592  1000   100 100     1 #> 101        42528600  1000   100 100     1 #> 102         1695072  1000   100 100     1 #> 103        18575032  1000   100 100     1 #> 104        14800240  1000   100 100     1 #> 105        16573000   100  5000 100     1 #> 106         8205472   100  5000 100     1 #> 107        43430704   100  5000 100     1 #> 108        38867640   100  5000 100     1 #> 109           90320  1000   100 100     1 #> 110         1778512  1000   100 100     1 #> 111        23818376  1000   100 100     1 #> 112        14800240  1000   100 100     1 #> 113         3456256  1000   100 100     3 #> 114         1858608  1000   100 100     3 #> 115        27460048  1000   100 100     3 #> 116        24555680  1000   100 100     3 #> 117        42549400  1000   100 100     3 #> 118         1715872  1000   100 100     3 #> 119        27405544  1000   100 100     3 #> 120        24555680  1000   100 100     3 #> 121        16736200   100  5000 100     3 #> 122         8368672   100  5000 100     3 #> 123        62684192   100  5000 100     3 #> 124        66586440   100  5000 100     3 #> 125           95120  1000   100 100     3 #> 126         1800912  1000   100 100     3 #> 127        44869208  1000   100 100     3 #> 128        24555680  1000   100 100     3 #> 129         3473056  1000   100 100    10 #> 130         1884784  1000   100 100    10 #> 131        56015344  1000   100 100    10 #> 132        58798008  1000   100 100    10 #> 133        42622200  1000   100 100    10 #> 134         1788672  1000   100 100    10 #> 135        55928376  1000   100 100    10 #> 136        58798008  1000   100 100    10 #> 137        17307400   100  5000 100    10 #> 138         8939872   100  5000 100    10 #> 139       131189584   100  5000 100    10 #> 140       166101528   100  5000 100    10 #> 141          111920  1000   100 100    10 #> 142         1879312  1000   100 100    10 #> 143       116080424  1000   100 100    10 #> 144        58798008  1000   100 100    10 #> 193       169802368 10000  1000  10     1 #> 194        89010320 10000  1000  10     1 #> 195       732448600 10000  1000  10     1 #> 196       657916048 10000  1000  10     1 #> 197          194448 10000  1000  10     1 #> 198        80986128 10000  1000  10     1 #> 199       732288928 10000  1000  10     1 #> 200       657916048 10000  1000  10     1 #> 201      1245640176  1000 50000  10     1 #> 202       405290128  1000 50000  10     1 #> 203      3303891552  1000 50000  10     1 #> 204      3225391600  1000 50000  10     1 #> 205          122080 10000  1000  10     1 #> 206        81082272 10000  1000  10     1 #> 207       854535432 10000  1000  10     1 #> 208       657916048 10000  1000  10     1 #> 209       169834656 10000  1000  10     3 #> 210        89058608 10000  1000  10     3 #> 211       742210520 10000  1000  10     3 #> 212       997737408 10000  1000  10     3 #> 213          386736 10000  1000  10     3 #> 214        81178416 10000  1000  10     3 #> 215       741787136 10000  1000  10     3 #> 216       997737408 10000  1000  10     3 #> 217      1247256464  1000 50000  10     3 #> 218       406906416  1000 50000  10     3 #> 219      3441602896  1000 50000  10     3 #> 220      4865512960  1000 50000  10     3 #> 221          154368 10000  1000  10     3 #> 222        81290560 10000  1000  10     3 #> 223      1085826632 10000  1000  10     3 #> 224       997737408 10000  1000  10     3 #> 225       169947216 10000  1000  10    10 #> 226        89230944 10000  1000  10    10 #> 227       776353216 10000  1000  10    10 #> 228      2187598944 10000  1000  10    10 #> 229         1059296 10000  1000  10    10 #> 230        81850976 10000  1000  10    10 #> 231       775883848 10000  1000  10    10 #> 232      2187598944 10000  1000  10    10 #> 233      1252913024  1000 50000  10    10 #> 234       412562976  1000 50000  10    10 #> 235      3940663200  1000 50000  10    10 #> 236     10630434496  1000 50000  10    10 #> 237          266928 10000  1000  10    10 #> 238        82019120 10000  1000  10    10 #> 239      1940248616 10000  1000  10    10 #> 240      2187598944 10000  1000  10    10 #> 241       185647456 10000  1000 100     1 #> 242        97655408 10000  1000 100     1 #> 243       829340832 10000  1000 100     1 #> 244       723476696 10000  1000 100     1 #> 245          918688 10000  1000 100     1 #> 246        88908672 10000  1000 100     1 #> 247       829181160 10000  1000 100     1 #> 248       723476696 10000  1000 100     1 #> 249      1281644416  1000 50000 100     1 #> 250       442012672  1000 50000 100     1 #> 251      3387267744  1000 50000 100     1 #> 252      3267912248  1000 50000 100     1 #> 253          846320 10000  1000 100     1 #> 254        89726512 10000  1000 100     1 #> 255       956883960 10000  1000 100     1 #> 256       723476696 10000  1000 100     1 #> 257       185681056 10000  1000 100     3 #> 258        97705008 10000  1000 100     3 #> 259       909018352 10000  1000 100     3 #> 260      1107997096 10000  1000 100     3 #> 261         1112288 10000  1000 100     3 #> 262        89102272 10000  1000 100     3 #> 263       908594968 10000  1000 100     3 #> 264      1107997096 10000  1000 100     3 #> 265      1283262016  1000 50000 100     3 #> 266       443630272  1000 50000 100     3 #> 267      3680010256  1000 50000 100     3 #> 268      4984412648  1000 50000 100     3 #> 269          879920 10000  1000 100     3 #> 270        89936112 10000  1000 100     3 #> 271      1309070888 10000  1000 100     3 #> 272      1107997096 10000  1000 100     3 #> 273       185798656 10000  1000 100    10 #> 274        97882384 10000  1000 100    10 #> 275      1162711904 10000  1000 100    10 #> 276      2454357784 10000  1000 100    10 #> 277         1789888 10000  1000 100    10 #> 278        89779872 10000  1000 100    10 #> 279      1162242536 10000  1000 100    10 #> 280      2454357784 10000  1000 100    10 #> 281      1288923616  1000 50000 100    10 #> 282       449291872  1000 50000 100    10 #> 283      4714714672  1000 50000 100    10 #> 284     11016713336  1000 50000 100    10 #> 285          997520 10000  1000 100    10 #> 286        90669712 10000  1000 100    10 #> 287      2516549912 10000  1000 100    10 #> 288      2454357784 10000  1000 100    10 #>                                         notes #> 49       Run via pls_fit() with dense backend #> 50  Run via pls_fit() with big.memory backend #> 51                   Requires the pls package #> 52              Requires the mixOmics package #> 53       Run via pls_fit() with dense backend #> 54  Run via pls_fit() with big.memory backend #> 55                   Requires the pls package #> 56              Requires the mixOmics package #> 57       Run via pls_fit() with dense backend #> 58  Run via pls_fit() with big.memory backend #> 59                   Requires the pls package #> 60              Requires the mixOmics package #> 61       Run via pls_fit() with dense backend #> 62  Run via pls_fit() with big.memory backend #> 63                   Requires the pls package #> 64              Requires the mixOmics package #> 65       Run via pls_fit() with dense backend #> 66  Run via pls_fit() with big.memory backend #> 67                   Requires the pls package #> 68              Requires the mixOmics package #> 69       Run via pls_fit() with dense backend #> 70  Run via pls_fit() with big.memory backend #> 71                   Requires the pls package #> 72              Requires the mixOmics package #> 73       Run via pls_fit() with dense backend #> 74  Run via pls_fit() with big.memory backend #> 75                   Requires the pls package #> 76              Requires the mixOmics package #> 77       Run via pls_fit() with dense backend #> 78  Run via pls_fit() with big.memory backend #> 79                   Requires the pls package #> 80              Requires the mixOmics package #> 81       Run via pls_fit() with dense backend #> 82  Run via pls_fit() with big.memory backend #> 83                   Requires the pls package #> 84              Requires the mixOmics package #> 85       Run via pls_fit() with dense backend #> 86  Run via pls_fit() with big.memory backend #> 87                   Requires the pls package #> 88              Requires the mixOmics package #> 89       Run via pls_fit() with dense backend #> 90  Run via pls_fit() with big.memory backend #> 91                   Requires the pls package #> 92              Requires the mixOmics package #> 93       Run via pls_fit() with dense backend #> 94  Run via pls_fit() with big.memory backend #> 95                   Requires the pls package #> 96              Requires the mixOmics package #> 97       Run via pls_fit() with dense backend #> 98  Run via pls_fit() with big.memory backend #> 99                   Requires the pls package #> 100             Requires the mixOmics package #> 101      Run via pls_fit() with dense backend #> 102 Run via pls_fit() with big.memory backend #> 103                  Requires the pls package #> 104             Requires the mixOmics package #> 105      Run via pls_fit() with dense backend #> 106 Run via pls_fit() with big.memory backend #> 107                  Requires the pls package #> 108             Requires the mixOmics package #> 109      Run via pls_fit() with dense backend #> 110 Run via pls_fit() with big.memory backend #> 111                  Requires the pls package #> 112             Requires the mixOmics package #> 113      Run via pls_fit() with dense backend #> 114 Run via pls_fit() with big.memory backend #> 115                  Requires the pls package #> 116             Requires the mixOmics package #> 117      Run via pls_fit() with dense backend #> 118 Run via pls_fit() with big.memory backend #> 119                  Requires the pls package #> 120             Requires the mixOmics package #> 121      Run via pls_fit() with dense backend #> 122 Run via pls_fit() with big.memory backend #> 123                  Requires the pls package #> 124             Requires the mixOmics package #> 125      Run via pls_fit() with dense backend #> 126 Run via pls_fit() with big.memory backend #> 127                  Requires the pls package #> 128             Requires the mixOmics package #> 129      Run via pls_fit() with dense backend #> 130 Run via pls_fit() with big.memory backend #> 131                  Requires the pls package #> 132             Requires the mixOmics package #> 133      Run via pls_fit() with dense backend #> 134 Run via pls_fit() with big.memory backend #> 135                  Requires the pls package #> 136             Requires the mixOmics package #> 137      Run via pls_fit() with dense backend #> 138 Run via pls_fit() with big.memory backend #> 139                  Requires the pls package #> 140             Requires the mixOmics package #> 141      Run via pls_fit() with dense backend #> 142 Run via pls_fit() with big.memory backend #> 143                  Requires the pls package #> 144             Requires the mixOmics package #> 193      Run via pls_fit() with dense backend #> 194 Run via pls_fit() with big.memory backend #> 195                  Requires the pls package #> 196             Requires the mixOmics package #> 197      Run via pls_fit() with dense backend #> 198 Run via pls_fit() with big.memory backend #> 199                  Requires the pls package #> 200             Requires the mixOmics package #> 201      Run via pls_fit() with dense backend #> 202 Run via pls_fit() with big.memory backend #> 203                  Requires the pls package #> 204             Requires the mixOmics package #> 205      Run via pls_fit() with dense backend #> 206 Run via pls_fit() with big.memory backend #> 207                  Requires the pls package #> 208             Requires the mixOmics package #> 209      Run via pls_fit() with dense backend #> 210 Run via pls_fit() with big.memory backend #> 211                  Requires the pls package #> 212             Requires the mixOmics package #> 213      Run via pls_fit() with dense backend #> 214 Run via pls_fit() with big.memory backend #> 215                  Requires the pls package #> 216             Requires the mixOmics package #> 217      Run via pls_fit() with dense backend #> 218 Run via pls_fit() with big.memory backend #> 219                  Requires the pls package #> 220             Requires the mixOmics package #> 221      Run via pls_fit() with dense backend #> 222 Run via pls_fit() with big.memory backend #> 223                  Requires the pls package #> 224             Requires the mixOmics package #> 225      Run via pls_fit() with dense backend #> 226 Run via pls_fit() with big.memory backend #> 227                  Requires the pls package #> 228             Requires the mixOmics package #> 229      Run via pls_fit() with dense backend #> 230 Run via pls_fit() with big.memory backend #> 231                  Requires the pls package #> 232             Requires the mixOmics package #> 233      Run via pls_fit() with dense backend #> 234 Run via pls_fit() with big.memory backend #> 235                  Requires the pls package #> 236             Requires the mixOmics package #> 237      Run via pls_fit() with dense backend #> 238 Run via pls_fit() with big.memory backend #> 239                  Requires the pls package #> 240             Requires the mixOmics package #> 241      Run via pls_fit() with dense backend #> 242 Run via pls_fit() with big.memory backend #> 243                  Requires the pls package #> 244             Requires the mixOmics package #> 245      Run via pls_fit() with dense backend #> 246 Run via pls_fit() with big.memory backend #> 247                  Requires the pls package #> 248             Requires the mixOmics package #> 249      Run via pls_fit() with dense backend #> 250 Run via pls_fit() with big.memory backend #> 251                  Requires the pls package #> 252             Requires the mixOmics package #> 253      Run via pls_fit() with dense backend #> 254 Run via pls_fit() with big.memory backend #> 255                  Requires the pls package #> 256             Requires the mixOmics package #> 257      Run via pls_fit() with dense backend #> 258 Run via pls_fit() with big.memory backend #> 259                  Requires the pls package #> 260             Requires the mixOmics package #> 261      Run via pls_fit() with dense backend #> 262 Run via pls_fit() with big.memory backend #> 263                  Requires the pls package #> 264             Requires the mixOmics package #> 265      Run via pls_fit() with dense backend #> 266 Run via pls_fit() with big.memory backend #> 267                  Requires the pls package #> 268             Requires the mixOmics package #> 269      Run via pls_fit() with dense backend #> 270 Run via pls_fit() with big.memory backend #> 271                  Requires the pls package #> 272             Requires the mixOmics package #> 273      Run via pls_fit() with dense backend #> 274 Run via pls_fit() with big.memory backend #> 275                  Requires the pls package #> 276             Requires the mixOmics package #> 277      Run via pls_fit() with dense backend #> 278 Run via pls_fit() with big.memory backend #> 279                  Requires the pls package #> 280             Requires the mixOmics package #> 281      Run via pls_fit() with dense backend #> 282 Run via pls_fit() with big.memory backend #> 283                  Requires the pls package #> 284             Requires the mixOmics package #> 285      Run via pls_fit() with dense backend #> 286 Run via pls_fit() with big.memory backend #> 287                  Requires the pls package #> 288             Requires the mixOmics package"},{"path":"https://fbertran.github.io/bigPLSR/articles/pls2-benchmark.html","id":"key-messages","dir":"Articles","previous_headings":"","what":"Key messages","title":"Benchmarking PLS2 Implementations","text":"Dense SIMPLS remains fastest option well-sized dense matrices. Streaming NIPALS offers robustness responses numerous predictor matrix file-backed. External comparisons help position bigPLSR relative established alternatives without adding heavyweight dependencies vignette.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/rkhs-overview.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"RKHS-based Algorithms in bigPLSR","text":"bigPLSR implements two kernel-based partial least squares solvers: algorithm = \"rkhs\" (Rosipal & Trejo style) projects predictor matrix XX RKHS; algorithm = \"rkhs_xy\" projects XX response matrix YY (possibly different) RKHSs couples latent scores regularised cross-covariance operator. solvers available dense matrices bigmemory::big.matrix objects. big-memory paths stream kernel blocks persist centering statistics predictions remain cheap.","code":""},{"path":"https://fbertran.github.io/bigPLSR/articles/rkhs-overview.html","id":"dense-example","dir":"Articles","previous_headings":"","what":"Dense example","title":"RKHS-based Algorithms in bigPLSR","text":"fits run well five seconds moderately sized example. RKHS-XY variant stores kernel centering statistics sides predict() can re-use without recomputing entire Gram matrix.","code":"library(bigPLSR) set.seed(42) n <- 120; p <- 8; m <- 2 X <- matrix(rnorm(n * p), n, p) Y <- cbind(   sin(X[, 1]) + 0.3 * X[, 2]^2 + rnorm(n, sd = 0.1),   cos(X[, 3]) - 0.2 * X[, 4] + rnorm(n, sd = 0.1) )  fit_rkhs <- pls_fit(X, Y, ncomp = 3, algorithm = \"rkhs\",                     kernel = \"rbf\", gamma = 1 / p, scores = \"r\")  options(bigPLSR.rkhs_xy.lambda_x = 1e-6) options(bigPLSR.rkhs_xy.lambda_y = 1e-6)  fit_rkhs_xy <- pls_fit(X, Y, ncomp = 3, algorithm = \"rkhs_xy\",                        kernel = \"rbf\", gamma = 1 / p,                        scores = \"none\")  head(predict(fit_rkhs, X)) #>              [,1]       [,2] #> [1,]  1.450177968 0.89078409 #> [2,] -0.007192657 0.54629507 #> [3,]  0.339494159 0.56225998 #> [4,]  0.437162419 0.48400010 #> [5,]  0.427444560 0.42801983 #> [6,] -0.058173062 0.06114628 head(predict(fit_rkhs_xy, X)) #>           [,1]       [,2] #> [1,] 1.7433526  0.9580851 #> [2,] 0.1298308  0.7311463 #> [3,] 0.2710529  0.3426840 #> [4,] 0.8168251  0.2991740 #> [5,] 0.4341051  0.2350586 #> [6,] 0.1174506 -0.4975300"},{"path":"https://fbertran.github.io/bigPLSR/articles/rkhs-overview.html","id":"streaming-example","dir":"Articles","previous_headings":"","what":"Streaming example","title":"RKHS-based Algorithms in bigPLSR","text":"streaming call attaches training descriptors ($X_ref) kernel centering summaries ($kstats) automatically. predict() invoked new data Xtrain = fit_stream$X_ref, package streams cross-kernel blocks avoids materialising full nnew√óntrainn_\\text{new} \\times n_\\text{train} Gram matrix.","code":"library(bigmemory) Xbm <- as.big.matrix(X) Ybm <- as.big.matrix(Y)  fit_stream <- pls_fit(Xbm, Ybm, ncomp = 3, backend = \"bigmem\",                       algorithm = \"rkhs\", kernel = \"rbf\",                       gamma = 1 / p, chunk_size = 1024L,                       scores = \"none\")"},{"path":"https://fbertran.github.io/bigPLSR/articles/rkhs-overview.html","id":"logistic-response","dir":"Articles","previous_headings":"","what":"Logistic response","title":"RKHS-based Algorithms in bigPLSR","text":"Kernel logistic PLS (algorithm = \"klogitpls\") builds RKHS infrastructure. extracting latent scores centered Gram matrix algorithm runs logistic IRLS procedure score space support class weighting optional alternating score updates. Small datasets (hundreds observations) remain well within five-second budget.","code":"y <- as.integer(X[, 1]^2 + X[, 2]^2 + rnorm(n, sd = 0.2) > 1) fit_logit <- pls_fit(X, y, ncomp = 2, algorithm = \"klogitpls\",                      kernel = \"rbf\", gamma = 1 / p) mean(predict(fit_logit, X))"},{"path":"https://fbertran.github.io/bigPLSR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Frederic Bertrand. Maintainer, author. Myriam Maumy. Author.","code":""},{"path":"https://fbertran.github.io/bigPLSR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Frederic Bertrand Myriam Maumy (2025). Partial Least Squares Regression Models Big Matrices, R package version 0.7.1. Maumy, M. Bertrand, F. (2023). PLS models extension big data. Conference presentation Joint Statistical Meetings (JSM 2023), Toronto, Ontario, Canada, Aug 5‚Äì10, 2023. Maumy, M. Bertrand, F. (2023). bigPLS: Fitting cross-validating PLS-based Cox models censored big data. Poster BioC2023: Bioconductor Annual Conference, Dana-Farber Cancer Institute, Boston, MA, USA, Aug 2‚Äì4, 2023. doi:10.7490/f1000research.1119546.1.","code":"@Manual{,   title = {Partial Least Squares Regression Models with Big Matrices},   author = {Frederic Bertrand and Myriam Maumy},   publisher = {manual},   year = {2025},   note = {R package version 0.7.1},   url = {https://fbertran.github.io/bigPLSR/}, } @Misc{,   title = {PLS models and their extension for big data},   author = {Myriam Maumy and Fr√©d√©ric Bertrand},   year = {2023},   howpublished = {Conference presentation at the Joint Statistical Meetings (JSM 2023)},   address = {Toronto, Ontario, Canada},   note = {Aug 5‚Äì10, 2023}, } @Misc{,   title = {bigPLS: Fitting and cross-validating PLS-based Cox models to censored big data},   author = {Myriam Maumy and Fr√©d√©ric Bertrand},   year = {2023},   howpublished = {Conference presentation at BioC2023: The Bioconductor Annual Conference},   address = {Dana-Farber Cancer Institute, Boston, MA, USA},   note = {Aug 2‚Äì4, 2023},   doi = {10.7490/f1000research.1119546.1},   url = {https://doi.org/10.7490/f1000research.1119546.1}, }"},{"path":[]},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"fr√©d√©ric-bertrand-and-myriam-maumy","dir":"","previous_headings":"","what":"Fr√©d√©ric Bertrand and Myriam Maumy","title":"Partial Least Squares Regression Models with Big Matrices","text":"bigPLSR provides fast, scalable Partial Least Squares (PLS) two execution backends: Dense (backend = \"arma\"): -memory Armadillo/BLAS speed. Big-matrix (backend = \"bigmem\"): chunked streaming bigmemory::big.matrix large data. PLS1 (single response) PLS2 (multi-response) supported. PLS2 uses SIMPLS cross-products backends numerical parity. Recent updates bring additional solvers tooling: Kernel PLS wide kernel PLS available alongside SIMPLS/NIPALS. Plot helpers now include correlation circles, loading arrows VIP bar charts. New wrappers simplify prediction, information-criteria based component selection, cross-validation bootstrapping workflows. Kalman-filter PLS double RKHS solvers extend modelling toolkit streaming dual-kernel settings. Cross-validation bootstrap helpers optionally run parallel via future ecosystem. Fresh vignettes cover RKHS usage, Kalman-filter state management plotting utilities showcased . package set CRAN-friendly: optional CBLAS fast path default. Support parallel computation GPU developed. website examples created F. Bertrand M. Maumy.","code":""},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Partial Least Squares Regression Models with Big Matrices","text":"can install released version bigPLSR CRAN : can install development version bigPLSR github :","code":"install.packages(\"bigPLSR\") devtools::install_github(\"fbertran/bigPLSR\")"},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick start","title":"Partial Least Squares Regression Models with Big Matrices","text":"","code":"library(bigPLSR)  set.seed(1) n <- 200; p <- 50 X <- matrix(rnorm(n*p), n, p) y <- X[,1]*2 - X[,2] + rnorm(n)  # Dense PLS1 (fast) fit <- pls_fit(X, y, ncomp = 3, backend = \"arma\", scores = \"r\") str(list(   coef=dim(fit$coefficients),   scores=dim(fit$scores),   ncomp=fit$ncomp ))"},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"big-matrix-pls1-with-file-backed-scores","dir":"","previous_headings":"Quick start","what":"Big-matrix PLS1 with file-backed scores","title":"Partial Least Squares Regression Models with Big Matrices","text":"","code":"options_val_before <- options(\"bigmemory.allow.dimnames\") options(bigmemory.allow.dimnames=TRUE)  bmX <- bigmemory::as.big.matrix(X) bmy <- bigmemory::as.big.matrix(matrix(y, n, 1))  tmp=tempdir() if(file.exists(paste(tmp,\"scores.desc\",sep=\"/\"))){unlink(paste(tmp,\"scores.desc\",sep=\"/\"))} if(file.exists(paste(tmp,\"scores.bin\",sep=\"/\"))){unlink(paste(tmp,\"scores.bin\",sep=\"/\"))} sink <- bigmemory::filebacked.big.matrix(   nrow=n, ncol=3, type=\"double\",   backingfile=\"scores.bin\",   backingpath=tmp,   descriptorfile=\"scores.desc\" )  fit_b <- pls_fit(   bmX, bmy, ncomp=3, backend=\"bigmem\", scores=\"big\",   scores_target=\"existing\", scores_bm=sink,   scores_colnames = c(\"t1\",\"t2\",\"t3\"),   return_scores_descriptor = TRUE )  fit_b$scores_descriptor  # big.matrix.descriptor options(bigmemory.allow.dimnames=options_val_before)"},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"pls2-multi-response","dir":"","previous_headings":"Quick start","what":"PLS2 (multi-response)","title":"Partial Least Squares Regression Models with Big Matrices","text":"","code":"set.seed(2) m <- 3 B <- matrix(rnorm(p*m), p, m) Y <- scale(X, scale = FALSE) %*% B + matrix(rnorm(n*m, sd = 0.1), n, m)  # Dense PLS2 ‚Äì SIMPLS on cross-products (parity with bigmem) fit2 <- pls_fit(X, Y, ncomp = 2, backend = \"arma\", mode = \"pls2\", scores = \"none\") str(list(coef=dim(fit2$coefficients), ncomp=fit2$ncomp))"},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"api","dir":"","previous_headings":"","what":"API","title":"Partial Least Squares Regression Models with Big Matrices","text":"Auto selection backend = \"auto\" ‚Üí \"bigmem\" X big.matrix (descriptor), else \"arma\". mode = \"auto\" ‚Üí \"pls1\" y one column, else \"pls2\". Return values PLS1: coefficients (p), intercept (scalar), x_weights, x_loadings, y_loadings, scores (optional), x_means, y_mean, ncomp. PLS2: coefficients (p√óm), intercept (length m), x_weights (p√óncomp), x_loadings (p√óncomp), y_loadings (m√óncomp), scores (optional), x_means, y_means, ncomp.","code":"pls_fit(   X, y, ncomp,   tol = 1e-8,   backend = c(\"auto\", \"arma\", \"bigmem\"),   scores  = c(\"none\", \"r\", \"big\"),   chunk_size = 10000L,   scores_name = \"scores\",   mode = c(\"auto\",\"pls1\",\"pls2\"),   scores_target = c(\"auto\",\"new\",\"existing\"),   scores_bm = NULL,   scores_backingfile = NULL,   scores_backingpath = NULL,   scores_descriptorfile = NULL,   scores_colnames = NULL,   return_scores_descriptor = FALSE )"},{"path":[]},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"dense-path-backend--arma","dir":"","previous_headings":"Backends & algorithms","what":"Dense path (backend = \"arma\")","title":"Partial Least Squares Regression Models with Big Matrices","text":"PLS1: fast dense solver (BLAS). Center X, Y, build XtX = Xc·µÄXc, XtY = Xc·µÄYc. Cholesky-whitened symmetric eigen solve; enforce symmetry add tiny ridge stabilize. Optional scores: T = Xc %*% W.","code":""},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"big-matrix-path-backend--bigmem","dir":"","previous_headings":"Backends & algorithms","what":"Big-matrix path (backend = \"bigmem\")","title":"Partial Least Squares Regression Models with Big Matrices","text":"Chunked /O big.matrix preallocated buffers. PLS1: streaming cross-products deflation; optional scores streamed chunk-wise sink. PLS2: chunked cross-products (XtX += B·µÄB, XtY += B·µÄY) + SIMPLS solver parity; optional score streaming: T = (X ‚àí Œº) %*% W. paths enforce symmetry (0.5*(M+M·µÄ)) eigen use small ridge XtX stability.","code":""},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"scores-sinks-and-descriptors","dir":"","previous_headings":"","what":"Scores, sinks, and descriptors","title":"Partial Least Squares Regression Models with Big Matrices","text":"scores = \"none\" ‚Äì don‚Äôt compute scores. scores = \"r\" ‚Äì return -memory matrix. Provide sink via scores_target = \"existing\" + scores_bm (big.matrix descriptor), Let function create file-backed storage via scores_backingfile (+ optional scores_backingpath, scores_descriptorfile). scores_colnames ‚Äì set column names scores. return_scores_descriptor = TRUE ‚Äì adds fit$scores_descriptor scores big.matrix.","code":""},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"determinism-tests--reproducibility","dir":"","previous_headings":"","what":"Determinism (tests & reproducibility)","title":"Partial Least Squares Regression Models with Big Matrices","text":"tight parity tests, force 1 BLAS thread fix RNG:","code":"set.seed(1) if (requireNamespace(\"RhpcBLASctl\", quietly = TRUE)) {   RhpcBLASctl::blas_set_num_threads(1L) } else {   # Use env vars before BLAS loads in the session   Sys.setenv(     OMP_NUM_THREADS=\"1\",     OPENBLAS_NUM_THREADS=\"1\",     MKL_NUM_THREADS=\"1\",     VECLIB_MAXIMUM_THREADS=\"1\",     BLIS_NUM_THREADS=\"1\"   ) }"},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"performance-tuning","dir":"","previous_headings":"","what":"Performance tuning","title":"Partial Least Squares Regression Models with Big Matrices","text":"chunk_size: default 10000L. Apple Silicon, internal default larger (e.g., 16384) chunk_size == 0. Tune per dataset best GEMM throughput. Scores streaming: scores=\"big\", streaming avoids holding T fully RAM. Multi-thread BLAS: production, allow multi-thread BLAS; tests, use 1 thread.","code":""},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"optional-cblas-fast-path-in-place-gemm","dir":"","previous_headings":"","what":"Optional CBLAS fast path (in-place GEMM)","title":"Partial Least Squares Regression Models with Big Matrices","text":"Default: (CRAN-safe). optional -place accumulation (true beta = 1 CBLAS dgemm) available guarded compile-time checks. available enabled, package falls back automatically portable Armadillo path. Enable locally (Unix/macOS): src/Makevars, link BLAS/LAPACK R uses: macOS: code attempts <vecLib/cblas.h>; Linux/others: <cblas.h>. headers aren‚Äôt present, build silently falls back portable GEMM path. hardcode -lopenblas -framework Accelerate; use R‚Äôs variables. Windows: leave macro unless ‚Äôve explicitly provided CBLAS headers/libs.","code":"R CMD INSTALL .   --configure-vars=\"PKG_CPPFLAGS='-DBIGPLSR_USE_CBLAS'\" PKG_LIBS += $(LAPACK_LIBS) $(BLAS_LIBS) $(FLIBS)"},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"development","dir":"","previous_headings":"","what":"Development","title":"Partial Least Squares Regression Models with Big Matrices","text":"Unit tests compare dense vs big-matrix backends PLS1/PLS2 tight tolerances. Vignettes examples keep datasets small; file-backed output uses tempdir().","code":""},{"path":"https://fbertran.github.io/bigPLSR/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Partial Least Squares Regression Models with Big Matrices","text":"use bigPLSR academic work, please cite package SIMPLS method de Jong (1993).","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigPLSR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"bigPLSR-package ‚Äî bigPLSR-package","title":"bigPLSR-package ‚Äî bigPLSR-package","text":"Provides Partial least squares Regression big data. allows missing data explanatory variables. Repeated k-fold cross-validation models using various criteria. Bootstrap confidence intervals constructions also available.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigPLSR-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"bigPLSR-package ‚Äî bigPLSR-package","text":"Maumy, M., Bertrand, F. (2023). PLS models extension big data. Joint Statistical Meetings (JSM 2023), Toronto, , Canada. Maumy, M., Bertrand, F. (2023). bigPLS: Fitting cross-validating PLS-based Cox models censored big data. BioC2023 ‚Äî Bioconductor Annual Conference, Dana-Farber Cancer Institute, Boston, MA, USA. Poster. https://doi.org/10.7490/f1000research.1119546.1","code":""},{"path":[]},{"path":"https://fbertran.github.io/bigPLSR/reference/bigPLSR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"bigPLSR-package ‚Äî bigPLSR-package","text":"Maintainer: Frederic Bertrand frederic.bertrand@lecnam.net (ORCID) Authors: Myriam Maumy myriam.maumy@ehesp.fr (ORCID)","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigPLSR-package.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"bigPLSR-package ‚Äî bigPLSR-package","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\", algorithm = \"simpls\") head(pls_predict_response(fit, X, ncomp = 2)) #> [1] -0.2557041 -0.3103345  1.8935717  0.1961492  0.2217772  2.3503614"},{"path":"https://fbertran.github.io/bigPLSR/reference/bigPLSR_stream_kstats.html","id":null,"dir":"Reference","previous_headings":"","what":"Streamed centering statistics for RKHS kernels ‚Äî bigPLSR_stream_kstats","title":"Streamed centering statistics for RKHS kernels ‚Äî bigPLSR_stream_kstats","text":"Compute column means grand mean kernel matrix \\(K(X, X)\\) without materialising memory. input design matrix must stored bigmemory::big.matrix (descriptor), kernel evaluated iterating row/column chunks.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigPLSR_stream_kstats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Streamed centering statistics for RKHS kernels ‚Äî bigPLSR_stream_kstats","text":"","code":"bigPLSR_stream_kstats(   Xbm,   kernel,   gamma,   degree,   coef0,   chunk_rows = getOption(\"bigPLSR.predict.chunk_rows\", 8192L),   chunk_cols = getOption(\"bigPLSR.predict.chunk_cols\", 8192L) )"},{"path":"https://fbertran.github.io/bigPLSR/reference/bigPLSR_stream_kstats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Streamed centering statistics for RKHS kernels ‚Äî bigPLSR_stream_kstats","text":"Xbm bigmemory::big.matrix (descriptor) containing training design matrix. kernel Kernel name passed stats::kernel() compatible helpers (\"linear\", \"rbf\", \"poly\", \"sigmoid\"). gamma, degree, coef0 Kernel hyper-parameters. chunk_rows, chunk_cols Numbers rows/columns process per chunk.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigPLSR_stream_kstats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Streamed centering statistics for RKHS kernels ‚Äî bigPLSR_stream_kstats","text":"list entries r (column means) g (grand mean) kernel matrix.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigmatrix-operations.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix and arithmetic operations for big.matrix objects ‚Äî bigmatrix-operations","title":"Matrix and arithmetic operations for big.matrix objects ‚Äî bigmatrix-operations","text":"methods extend base matrix multiplication operator (%*%) group generic Arithmetic big.matrix objects can interoperate base R matrices numeric scalars using high-performance routines provided bigalgebra.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigmatrix-operations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matrix and arithmetic operations for big.matrix objects ‚Äî bigmatrix-operations","text":"","code":"# S4 method for class 'big.matrix,big.matrix' x %*% y  # S4 method for class 'matrix,big.matrix' x %*% y  # S4 method for class 'big.matrix,matrix' x %*% y  # S4 method for class 'big.matrix,big.matrix' Arith(e1, e2)  # S4 method for class 'big.matrix,matrix' Arith(e1, e2)  # S4 method for class 'matrix,big.matrix' Arith(e1, e2)  # S4 method for class 'numeric,big.matrix' Arith(e1, e2)  # S4 method for class 'big.matrix,numeric' Arith(e1, e2)"},{"path":"https://fbertran.github.io/bigPLSR/reference/bigmatrix-operations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matrix and arithmetic operations for big.matrix objects ‚Äî bigmatrix-operations","text":"x, y Matrix operands supplied either big.matrix instances base R matrices, depending method signature. e1, e2 Numeric operands, may big.matrix objects, base R matrices, numeric scalars depending method signature.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigmatrix-operations.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Matrix and arithmetic operations for big.matrix objects ‚Äî bigmatrix-operations","text":"Matrix multiplications dispatch bigalgebra::dgemm(), mixed arithmetic matrices relies bigalgebra::daxpy(), scalar/matrix combinations use bigalgebra::dadd() appropriate.","code":""},{"path":[]},{"path":"https://fbertran.github.io/bigPLSR/reference/bigmatrix-operations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Matrix and arithmetic operations for big.matrix objects ‚Äî bigmatrix-operations","text":"","code":"if (requireNamespace(\"bigmemory\", quietly = TRUE) &&     requireNamespace(\"bigalgebra\", quietly = TRUE)) {   x <- bigmemory::big.matrix(2, 2, init = 1)   y <- bigmemory::big.matrix(2, 2, init = 2)   x %*% y   x + y   x * 3 } #> An object of class \"big.matrix\" #> Slot \"address\": #> <pointer: 0x104485300> #>"},{"path":"https://fbertran.github.io/bigPLSR/reference/bigscale.html","id":null,"dir":"Reference","previous_headings":"","what":"Title ‚Äî bigscale","title":"Title ‚Äî bigscale","text":"Title","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigscale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Title ‚Äî bigscale","text":"","code":"bigscale(   formula = Surv(time = time, status = status) ~ .,   data,   norm.method = \"standardize\",   strata.size = 20,   batch.size = 1,   features.mean = NULL,   features.sd = NULL,   parallel.flag = FALSE,   num.cores = NULL,   bigmemory.flag = FALSE,   num.rows.chunk = 1e+06,   col.names = NULL,   type = \"short\" )"},{"path":"https://fbertran.github.io/bigPLSR/reference/bigscale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Title ‚Äî bigscale","text":"formula  data  norm.method  strata.size  batch.size  features.mean  features.sd  parallel.flag  num.cores  bigmemory.flag  num.rows.chunk  col.names  type","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigscale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Title ‚Äî bigscale","text":"object scaler class time.indices: indices time variable cens.indices: indices censored variables features.indices: indices features time.sd: standard deviation time variable time.mean: mean time variable features.sd: standard deviation features features.mean: mean features nr: number rows nc: number columns col.names: columns names","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/bigscale.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Title ‚Äî bigscale","text":"","code":"1+1 #> [1] 2"},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_irls_binomial.html","id":null,"dir":"Reference","previous_headings":"","what":"Fast IRLS for binomial logit with class weights ‚Äî cpp_irls_binomial","title":"Fast IRLS for binomial logit with class weights ‚Äî cpp_irls_binomial","text":"Fast IRLS binomial logit class weights","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_irls_binomial.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fast IRLS for binomial logit with class weights ‚Äî cpp_irls_binomial","text":"","code":"cpp_irls_binomial(T, ybin, w_class = NULL, maxit = 50L, tol = 1e-08)"},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_irls_binomial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fast IRLS for binomial logit with class weights ‚Äî cpp_irls_binomial","text":"T n x numeric matrix latent scores (intercept column) ybin integer vector {0,1} labels (length n) w_class optional length-2 numeric vector: weights classes c( w0, w1 ) maxit max IRLS iterations tol relative tolerance parameter change","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_irls_binomial.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fast IRLS for binomial logit with class weights ‚Äî cpp_irls_binomial","text":"list(beta = -vector, b = scalar intercept, fitted = n-vector, iter = integer, converged = logical)","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_kernel_pls.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal kernel and wide-kernel PLS solver ‚Äî cpp_kernel_pls","title":"Internal kernel and wide-kernel PLS solver ‚Äî cpp_kernel_pls","text":"Internal kernel wide-kernel PLS solver","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_kernel_pls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal kernel and wide-kernel PLS solver ‚Äî cpp_kernel_pls","text":"","code":"cpp_kernel_pls(X, Y, ncomp, tol, wide)"},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_kernel_pls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal kernel and wide-kernel PLS solver ‚Äî cpp_kernel_pls","text":"X Centered design matrix. Y Centered response matrix. ncomp Maximum number components. tol Numerical tolerance. wide Whether use wide-kernel update.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/cpp_kernel_pls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal kernel and wide-kernel PLS solver ‚Äî cpp_kernel_pls","text":"list containing kernel PLS factors.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/dot-resolve_training_ref.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal: resolve training reference for RKHS predictions ‚Äî .resolve_training_ref","title":"Internal: resolve training reference for RKHS predictions ‚Äî .resolve_training_ref","text":"Accepts: dense matrix (returned -) big.matrix (returned -) big.matrix.descriptor (attached returned)","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/dot-resolve_training_ref.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal: resolve training reference for RKHS predictions ‚Äî .resolve_training_ref","text":"","code":".resolve_training_ref(obj, dots)"},{"path":"https://fbertran.github.io/bigPLSR/reference/dot-resolve_training_ref.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal: resolve training reference for RKHS predictions ‚Äî .resolve_training_ref","text":"Sources (priority): object$X, object$Xtrain, ...$Xtrain, ...$X_ref, object$X_ref","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/external_pls_benchmarks.html","id":null,"dir":"Reference","previous_headings":"","what":"Benchmark results against external PLS implementations ‚Äî external_pls_benchmarks","title":"Benchmark results against external PLS implementations ‚Äî external_pls_benchmarks","text":"Pre-computed runtime comparisons bigPLSR (dense big.memory backends) reference implementations pls mixOmics packages.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/external_pls_benchmarks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Benchmark results against external PLS implementations ‚Äî external_pls_benchmarks","text":"","code":"data(external_pls_benchmarks)"},{"path":"https://fbertran.github.io/bigPLSR/reference/external_pls_benchmarks.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Benchmark results against external PLS implementations ‚Äî external_pls_benchmarks","text":"data frame 384 rows 11 columns: task Character vector identifying task (\"pls1\" \"pls2\"). algorithm PLS algorithm used benchmark (e.g., \"simpls\"). package Package providing implementation. median_time_s Median execution time seconds. itr_per_sec Iterations per second recorded bench::mark(). mem_alloc_bytes Memory usage bytes recorded bench::mark(). n Number observations simulated dataset. p Number predictors (X) simulated dataset. q Number responses (Y) simulated dataset. ncomp Number extracted components. notes Helpful context dependencies configuration.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/external_pls_benchmarks.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Benchmark results against external PLS implementations ‚Äî external_pls_benchmarks","text":"Generated via inst/scripts/external_pls_benchmarks.R.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/external_pls_benchmarks.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Benchmark results against external PLS implementations ‚Äî external_pls_benchmarks","text":"Fix task = \"pls1\" select algorithms \"kernelpls\", \"nipals\" \"simpls\" get full factorial design. Fix task = \"pls1\" fix algorithm = \"widekernelpls\" get full factorial design. Fix task = \"pls2\" select algorithms \"kernelpls\", \"nipals\" \"simpls\" get full factorial design. Fix task = \"pls2\" fix algorithm = \"widekernelpls\" get full factorial design.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/external_pls_benchmarks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Benchmark results against external PLS implementations ‚Äî external_pls_benchmarks","text":"","code":"# \\donttest{ data(\"external_pls_benchmarks\", package = \"bigPLSR\")  sub_pls1 <- subset(external_pls_benchmarks, task == \"pls1\" &                                              algorithm != \"widekernelpls\") sub_pls1$n     <- factor(sub_pls1$n) sub_pls1$p     <- factor(sub_pls1$p) sub_pls1$q     <- factor(sub_pls1$q) sub_pls1$ncomp <- factor(sub_pls1$ncomp) if (exists(\"replications\")) replications(~ package + algorithm + task + n +                                            p + ncomp, data = sub_pls1) #> $package #> [1] 24 #>  #> $algorithm #> [1] 32 #>  #> $task #> [1] 96 #>  #> $n #> [1] 48 #>  #> $p #> [1] 48 #>  #> $ncomp #> ncomp #>   1   3  10 100  #>  24  24  36  12  #>   sub_pls1_wide <- subset(external_pls_benchmarks, task == \"pls1\" &                                                   algorithm == \"widekernelpls\") sub_pls1_wide$n     <- factor(sub_pls1_wide$n) sub_pls1_wide$p     <- factor(sub_pls1_wide$p) sub_pls1_wide$q     <- factor(sub_pls1_wide$q) sub_pls1_wide$ncomp <- factor(sub_pls1_wide$ncomp) if (exists(\"replications\")) replications(~ package + algorithm + task + n +                                             p + ncomp, data = sub_pls1_wide) #> $package #> [1] 8 #>  #> $algorithm #> [1] 32 #>  #> $task #> [1] 32 #>  #> $n #> [1] 16 #>  #> $p #> [1] 16 #>  #> $ncomp #> ncomp #>   1   3  10 100  #>   8   8  12   4  #>   sub_pls2 <- subset(external_pls_benchmarks, task == \"pls2\" &                                              algorithm != \"widekernelpls\") sub_pls2$n     <- factor(sub_pls2$n) sub_pls2$p     <- factor(sub_pls2$p) sub_pls2$q     <- factor(sub_pls2$q) sub_pls2$ncomp <- factor(sub_pls2$ncomp) if (exists(\"replications\")) replications(~ package + algorithm + task + n +                                             p + ncomp, data = sub_pls2) #> $package #> [1] 48 #>  #> $algorithm #> [1] 64 #>  #> $task #> [1] 192 #>  #> $n #> [1] 96 #>  #> $p #> [1] 96 #>  #> $ncomp #> ncomp #>  1  3 10  #> 48 48 96  #>   sub_pls2_wide <- subset(external_pls_benchmarks, task == \"pls2\" &                                                   algorithm == \"widekernelpls\") sub_pls2_wide$n     <- factor(sub_pls2_wide$n) sub_pls2_wide$p     <- factor(sub_pls2_wide$p) sub_pls2_wide$q     <- factor(sub_pls2_wide$q) sub_pls2_wide$ncomp <- factor(sub_pls2_wide$ncomp) if (exists(\"replications\")) replications(~ package + algorithm + task + n +                                             p + ncomp, data = sub_pls2_wide) #> $package #> [1] 16 #>  #> $algorithm #> [1] 64 #>  #> $task #> [1] 64 #>  #> $n #> [1] 32 #>  #> $p #> [1] 32 #>  #> $ncomp #> ncomp #>  1  3 10  #> 16 16 32  #>  # }"},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Finalize a KF-PLS state into a fitted model ‚Äî kf_pls_state_fit","title":"Finalize a KF-PLS state into a fitted model ‚Äî kf_pls_state_fit","text":"Converts accumulated KF-PLS state SIMPLS-equivalent fitted model (using current sufficient statistics). result compatible predict.big_plsr().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Finalize a KF-PLS state into a fitted model ‚Äî kf_pls_state_fit","text":"","code":"kf_pls_state_fit(state, tol = 1e-08)"},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Finalize a KF-PLS state into a fitted model ‚Äî kf_pls_state_fit","text":"state External pointer created kf_pls_state_new(). tol Numeric tolerance inner SIMPLS step.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Finalize a KF-PLS state into a fitted model ‚Äî kf_pls_state_fit","text":"list PLS factors coefficients, classed big_plsr.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Finalize a KF-PLS state into a fitted model ‚Äî kf_pls_state_fit","text":"","code":"n <- 200; p <- 30; m <- 2; A <- 3 X <- matrix(rnorm(n*p), n, p) Y <- X[,1:2] %*% matrix(c(0.7, -0.3, 0.2, 0.9), 2, m) + matrix(rnorm(n*m, sd=0.2), n, m)  state <- kf_pls_state_new(p, m, A, lambda = 0.99, q_proc = 1e-6)  # stream in mini-batches bs <- 64 for (i in seq(1, n, by = bs)) {   idx <- i:min(i+bs-1, n)   kf_pls_state_update(state, X[idx, , drop=FALSE], Y[idx, , drop=FALSE]) }  fit <- kf_pls_state_fit(state)  # returns a big_plsr-compatible list # predict via your existing predict.big_plsr (linear case) Yhat <- cbind(1, scale(X, center = fit$x_means, scale = FALSE)) %*%   rbind(fit$intercept, fit$coefficients)"},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_new.html","id":null,"dir":"Reference","previous_headings":"","what":"KF-PLS streaming state (constructor) ‚Äî kf_pls_state_new","title":"KF-PLS streaming state (constructor) ‚Äî kf_pls_state_new","text":"Create persistent Kalman‚Äìfilter PLS (KF-PLS) state accumulates cross-products streaming mini-batches later produces big_plsr-compatible fit via kf_pls_state_fit().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_new.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"KF-PLS streaming state (constructor) ‚Äî kf_pls_state_new","text":"","code":"kf_pls_state_new(p, m, ncomp, lambda = 0.99, q_proc = 0, r_meas = 0)"},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_new.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"KF-PLS streaming state (constructor) ‚Äî kf_pls_state_new","text":"p Integer, number predictors (columns X). m Integer, number responses (columns Y). ncomp Integer, number latent components extract fit time. lambda Numeric (0,1], forgetting factor (closer 1 = slower decay). q_proc Non-negative numeric, process-noise magnitude (adds ridge \\(C_{xx}\\) update; useful stabilizing ill-conditioned problems). r_meas Reserved measurement-noise parameter (used minimal API yet; kept forward compatibility).","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_new.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"KF-PLS streaming state (constructor) ‚Äî kf_pls_state_new","text":"external pointer internal KF-PLS state (opaque object) pass kf_pls_state_update() kf_pls_state_fit() produce model coefficients.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_new.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"KF-PLS streaming state (constructor) ‚Äî kf_pls_state_new","text":"state maintains exponentially weighted cross-moments \\(C_{xx}\\) \\(C_{xy}\\) forgetting factor lambda. lambda >= 0.999999 q_proc == 0, backend switches exact accumulation mode matches concatenating chunks (decay).","code":""},{"path":[]},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_new.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"KF-PLS streaming state (constructor) ‚Äî kf_pls_state_new","text":"","code":"if (FALSE) { # \\dontrun{ set.seed(1) n <- 1000; p <- 50; m <- 2 X1 <- matrix(rnorm(n/2 * p), n/2, p) X2 <- matrix(rnorm(n/2 * p), n/2, p) B  <- matrix(rnorm(p*m), p, m) Y1 <- scale(X1, TRUE, FALSE) %*% B + 0.05*matrix(rnorm(n/2*m), n/2, m) Y2 <- scale(X2, TRUE, FALSE) %*% B + 0.05*matrix(rnorm(n/2*m), n/2, m)  st <- kf_pls_state_new(p, m, ncomp = 4, lambda = 0.99, q_proc = 1e-6) kf_pls_state_update(st, X1, Y1) kf_pls_state_update(st, X2, Y2) fit <- kf_pls_state_fit(st)          # returns a big_plsr-compatible list preds <- predict(.finalize_pls_fit(fit, \"kf_pls\"), rbind(X1, X2)) } # }"},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_update.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a KF-PLS streaming state with a mini-batch ‚Äî kf_pls_state_update","title":"Update a KF-PLS streaming state with a mini-batch ‚Äî kf_pls_state_update","text":"Feed one chunk (X_chunk, Y_chunk) existing KF-PLS state created kf_pls_state_new(). function updates exponentially weighted means cross-products (exact sufficient statistics exact mode).","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_update.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a KF-PLS streaming state with a mini-batch ‚Äî kf_pls_state_update","text":"","code":"kf_pls_state_update(state, X_chunk, Y_chunk)"},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_update.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a KF-PLS streaming state with a mini-batch ‚Äî kf_pls_state_update","text":"state External pointer produced kf_pls_state_new(). X_chunk Numeric matrix number columns p used create state. Y_chunk Numeric matrix m columns (numeric vector m == 1). Must number rows X_chunk.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_update.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update a KF-PLS streaming state with a mini-batch ‚Äî kf_pls_state_update","text":"Invisibly returns state, updated place.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/kf_pls_state_update.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update a KF-PLS streaming state with a mini-batch ‚Äî kf_pls_state_update","text":"Call repeatedly incoming batch. want model coefficients (weights/loadings/intercepts), call kf_pls_state_fit(), solves SIMPLS accumulated cross-moments without re-materializing past data.","code":""},{"path":[]},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_biplot.html","id":null,"dir":"Reference","previous_headings":"","what":"PLS biplot ‚Äî plot_pls_biplot","title":"PLS biplot ‚Äî plot_pls_biplot","text":"PLS biplot","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_biplot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PLS biplot ‚Äî plot_pls_biplot","text":"","code":"plot_pls_biplot(   object,   comps = c(1L, 2L),   scale_variables = 1,   circle = TRUE,   circle_col = \"grey85\",   arrow_col = \"firebrick\",   groups = NULL,   ellipse = TRUE,   ellipse_level = 0.95,   ellipse_n = 200L,   group_col = NULL,   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_biplot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"PLS biplot ‚Äî plot_pls_biplot","text":"object fitted PLS model scores loadings. comps Components display. scale_variables Scaling factor applied variable loadings. circle Logical; draw correlation circle behind loadings. circle_col Colour circle guide. arrow_col Colour loading arrows. groups Optional factor character vector defining groups individuals. supplied, group-specific colours used , ellipse = TRUE, confidence ellipses drawn group. ellipse Logical; draw group confidence ellipses groups provided. ellipse_level Confidence level group ellipses (0 1). ellipse_n Number points used draw ellipse. group_col Optional vector colours groups. Recycled needed. ... Additional arguments passed graphics::plot().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_biplot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"PLS biplot ‚Äî plot_pls_biplot","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") plot_pls_biplot(fit)"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_bootstrap_coefficients.html","id":null,"dir":"Reference","previous_headings":"","what":"Boxplots of bootstrap coefficient distributions ‚Äî plot_pls_bootstrap_coefficients","title":"Boxplots of bootstrap coefficient distributions ‚Äî plot_pls_bootstrap_coefficients","text":"Boxplots bootstrap coefficient distributions","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_bootstrap_coefficients.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Boxplots of bootstrap coefficient distributions ‚Äî plot_pls_bootstrap_coefficients","text":"","code":"plot_pls_bootstrap_coefficients(   boot_result,   responses = NULL,   variables = NULL,   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_bootstrap_coefficients.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Boxplots of bootstrap coefficient distributions ‚Äî plot_pls_bootstrap_coefficients","text":"boot_result Result returned pls_bootstrap(). responses Optional character vector selecting response columns. variables Optional character vector selecting predictor variables. ... Additional arguments passed graphics::boxplot().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_bootstrap_scores.html","id":null,"dir":"Reference","previous_headings":"","what":"Boxplots of bootstrap score distributions ‚Äî plot_pls_bootstrap_scores","title":"Boxplots of bootstrap score distributions ‚Äî plot_pls_bootstrap_scores","text":"Visualise variability latent scores obtained pls_bootstrap() return_scores = TRUE.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_bootstrap_scores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Boxplots of bootstrap score distributions ‚Äî plot_pls_bootstrap_scores","text":"","code":"plot_pls_bootstrap_scores(   boot_result,   components = NULL,   observations = NULL,   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_bootstrap_scores.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Boxplots of bootstrap score distributions ‚Äî plot_pls_bootstrap_scores","text":"boot_result Result returned pls_bootstrap(). components Optional vector component indices names include. observations Optional vector observation indices names include. ... Additional arguments passed graphics::boxplot().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_individuals.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot individual scores ‚Äî plot_pls_individuals","title":"Plot individual scores ‚Äî plot_pls_individuals","text":"Plot individual scores","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_individuals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot individual scores ‚Äî plot_pls_individuals","text":"","code":"plot_pls_individuals(   object,   comps = c(1L, 2L),   labels = NULL,   groups = NULL,   ellipse = TRUE,   ellipse_level = 0.95,   ellipse_n = 200L,   group_col = NULL,   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_individuals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot individual scores ‚Äî plot_pls_individuals","text":"object fitted PLS model scores. comps Components plot (length two). labels Optional character vector point labels. groups Optional factor character vector defining groups individuals. supplied, group-specific colours used , ellipse = TRUE, confidence ellipses drawn group. ellipse Logical; draw group confidence ellipses groups provided. ellipse_level Confidence level ellipses (0 1). ellipse_n Number points used draw ellipse. group_col Optional vector colours groups. Recycled needed. ... Additional plotting parameters passed graphics::plot().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_individuals.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot individual scores ‚Äî plot_pls_individuals","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") plot_pls_individuals(fit)"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_variables.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot variable loadings ‚Äî plot_pls_variables","title":"Plot variable loadings ‚Äî plot_pls_variables","text":"Plot variable loadings","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_variables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot variable loadings ‚Äî plot_pls_variables","text":"","code":"plot_pls_variables(   object,   comps = c(1L, 2L),   circle = TRUE,   circle_col = \"grey80\",   arrow_col = \"steelblue\",   arrow_scale = 1,   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_variables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot variable loadings ‚Äî plot_pls_variables","text":"object fitted PLS model. comps Components display (length two). circle Logical; draw correlation circle. circle_col Colour correlation circle. arrow_col Colour variable arrows. arrow_scale Scaling applied variable vectors. ... Additional plotting parameters passed graphics::plot().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_variables.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot variable loadings ‚Äî plot_pls_variables","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") plot_pls_variables(fit)"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_vip.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Variable Importance in Projection (VIP) ‚Äî plot_pls_vip","title":"Plot Variable Importance in Projection (VIP) ‚Äî plot_pls_vip","text":"Plot Variable Importance Projection (VIP)","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_vip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Variable Importance in Projection (VIP) ‚Äî plot_pls_vip","text":"","code":"plot_pls_vip(   object,   comps = NULL,   threshold = 1,   palette = c(\"#4575b4\", \"#d73027\"),   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_vip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Variable Importance in Projection (VIP) ‚Äî plot_pls_vip","text":"object fitted PLS model. comps Components aggregate. Defaults available. threshold Optional threshold highlight influential variables. palette Colour palette used bars. ... Additional parameters passed graphics::barplot().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/plot_pls_vip.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Variable Importance in Projection (VIP) ‚Äî plot_pls_vip","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") plot_pls_vip(fit)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense.html","id":null,"dir":"Reference","previous_headings":"","what":"Single-response partial least squares regression (PLS1) ‚Äî pls1_dense","title":"Single-response partial least squares regression (PLS1) ‚Äî pls1_dense","text":"helpers expose optimised dense streaming solvers tailored partial least squares regression problems response consists single column. wrap high performance C++ routines shipped package provide user friendly entry point benchmarking available implementations.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Single-response partial least squares regression (PLS1) ‚Äî pls1_dense","text":"","code":".harmonize_pls_result(res)  pls1_stream(   X,   y,   ncomp = 2L,   chunk_size = 1024L,   center = TRUE,   scale = FALSE,   center_y = TRUE,   scale_y = FALSE,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Single-response partial least squares regression (PLS1) ‚Äî pls1_dense","text":"X bigmemory::big.matrix storing design matrix. y Numeric vector responses length nrow(X). ncomp Number latent components compute. chunk_size Number rows process per chunk. Must strictly positive. Smaller chunks reduce peak memory usage larger chunks may improve speed. center columns X centered? Defaults TRUE. scale columns X scaled unit variance? Defaults FALSE. center_y response centered? Defaults TRUE. scale_y response scaled unit variance? Defaults FALSE. algorithm Algorithm use fit. Either \"simpls\" \"nipals\". choosing \"simpls\", preprocessing options must remain default values. return_big Logical; TRUE, coefficients, scores loadings returned bigmemory::big.matrix objects. Defaults FALSE.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Single-response partial least squares regression (PLS1) ‚Äî pls1_dense","text":"list containing regression coefficients, intercept, latent scores, loadings weights.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Single-response partial least squares regression (PLS1) ‚Äî pls1_dense","text":"","code":"# \\donttest{ library(bigmemory) X <- as.big.matrix(matrix(rnorm(2000), nrow = 100)) y <- matrix(rnorm(100), ncol = 1) fit <- pls1_dense(X, y, ncomp = 3) str(fit) #> List of 15 #>  $ coefficients: num [1:20, 1] 0.1145 0.1132 -0.0814 -0.1989 0.1312 ... #>  $ intercept   : num -0.167 #>  $ x_weights   : num [1:20, 1:3] 0.0944 0.093 -0.2491 -0.1897 0.1397 ... #>  $ x_loadings  : num [1:20, 1:3] 0.03071 0.09135 -0.34936 -0.00304 0.07533 ... #>  $ y_loadings  : num [1:3, 1] 0.439 0.224 0.1 #>  $ x_means     : num [1:20] -0.0032 -0.07841 -0.16905 -0.08206 -0.00219 ... #>  $ y_mean      : num -0.091 #>  $ ncomp       : int 3 #>  $ weights     : num [1:20, 1:3] 0.0944 0.093 -0.2491 -0.1897 0.1397 ... #>  $ loadings    : num [1:20, 1:3] 0.03071 0.09135 -0.34936 -0.00304 0.07533 ... #>  $ x_center    : num [1:20] -0.0032 -0.07841 -0.16905 -0.08206 -0.00219 ... #>  $ y_center    : num -0.091 #>  $ x_scale     : NULL #>  $ y_scale     : NULL #>  $ scores      : NULL # }  # \\donttest{ library(bigmemory) X <- as.big.matrix(matrix(rnorm(2000), nrow = 100)) y <- matrix(rnorm(100), ncol = 1) fit <- pls1_stream(X, y, ncomp = 3) str(fit) #> List of 15 #>  $ coefficients: num [1:20, 1] -0.0634 -0.0947 0.0649 0.1416 -0.0312 ... #>  $ intercept   : num -0.0348 #>  $ x_weights   : num [1:20, 1:3] -0.0193 -0.2892 0.0764 0.0932 -0.0772 ... #>  $ x_loadings  : num [1:20, 1:3] 0.069 -0.3726 0.0123 -0.0112 -0.0616 ... #>  $ y_loadings  : num [1:3, 1] 0.405 0.194 0.074 #>  $ x_means     : num [1:20] -0.0804 -0.0878 -0.1322 0.104 0.1434 ... #>  $ y_mean      : num -0.0538 #>  $ ncomp       : int 3 #>  $ weights     : num [1:20, 1:3] -0.0193 -0.2892 0.0764 0.0932 -0.0772 ... #>  $ loadings    : num [1:20, 1:3] 0.069 -0.3726 0.0123 -0.0112 -0.0616 ... #>  $ x_center    : num [1:20] -0.0804 -0.0878 -0.1322 0.104 0.1434 ... #>  $ y_center    : num -0.0538 #>  $ x_scale     : NULL #>  $ y_scale     : NULL #>  $ scores      : NULL # }"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_a.html","id":null,"dir":"Reference","previous_headings":"","what":"Single-response partial least squares regression (PLS1) another implementation ‚Äî pls1_dense_a","title":"Single-response partial least squares regression (PLS1) another implementation ‚Äî pls1_dense_a","text":"helpers wrap high-performance C++ routines built top bigmemory bigalgebra infrastructure. pls1_dense_ya function performs standard PLS regression using NIPALS-style algorithm without copying data memory. pls1_stream_ya variant iterates data blocks makes possible handle --core datasets efficiently.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_a.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Single-response partial least squares regression (PLS1) another implementation ‚Äî pls1_dense_a","text":"","code":"pls1_dense_a(   X,   y,   ncomp = 2L,   center = TRUE,   scale = FALSE,   tol = 1e-08,   max_iter = 100L,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )  pls1_stream_a(   X,   y,   ncomp = 2L,   chunk_size = 1024L,   center = TRUE,   scale = FALSE,   tol = 1e-08,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_a.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Single-response partial least squares regression (PLS1) another implementation ‚Äî pls1_dense_a","text":"X big.matrix object containing predictors. y Either big.matrix single column numeric vector response values. ncomp Number latent components extract. center Logical; predictors response centered. scale Logical; predictors response scaled unit variance fitting model. tol Numerical tolerance used detect convergence breakdown. max_iter Maximum number iterations internal solver (kept compatibility; solver adapts automatically convergence issues detected). algorithm Algorithm used compute PLS fit. Either \"simpls\" \"nipals\". SIMPLS backend supports default centering scaling configuration. return_big Logical; TRUE, coefficients, scores loadings returned bigmemory::big.matrix objects. Defaults FALSE. chunk_size Number rows processed time streaming backend.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_a.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Single-response partial least squares regression (PLS1) another implementation ‚Äî pls1_dense_a","text":"list regression coefficients, intercept, latent scores, weights additional metadata.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_a.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Single-response partial least squares regression (PLS1) another implementation ‚Äî pls1_dense_a","text":"","code":"# \\donttest{ library(bigmemory) X <- as.big.matrix(matrix(rnorm(2000), nrow = 100)) y <- as.big.matrix(matrix(rnorm(100), ncol = 1)) fit <- pls1_dense_a(X, y, ncomp = 3) str(fit) #> List of 16 #>  $ coefficients: num [1:20, 1] -0.0168 -0.1308 0.1621 0.1499 0.0418 ... #>  $ intercept   : num 0.222 #>  $ x_weights   : num [1:20, 1:3] -0.0027 -0.27846 0.35683 0.41143 0.00574 ... #>  $ x_loadings  : num [1:20, 1:3] 0.0337 -0.2535 0.3214 0.4333 -0.0755 ... #>  $ y_loadings  : num [1:3, 1] 0.3218 0.1069 0.0493 #>  $ x_means     : num [1:20] 0.0265 -0.0546 -0.0189 0.0167 -0.1192 ... #>  $ y_mean      : num 0.192 #>  $ ncomp       : int 3 #>  $ weights     : num [1:20, 1:3] -0.0027 -0.27846 0.35683 0.41143 0.00574 ... #>  $ loadings    : num [1:20, 1:3] 0.0337 -0.2535 0.3214 0.4333 -0.0755 ... #>  $ x_center    : num [1:20] 0.0265 -0.0546 -0.0189 0.0167 -0.1192 ... #>  $ y_center    : num 0.192 #>  $ x_scale     : NULL #>  $ y_scale     : NULL #>  $ scores      : NULL #>  $ call        : language pls1_dense_a(X = X, y = y, ncomp = 3) #>  - attr(*, \"class\")= chr [1:2] \"big_plsr\" \"list\" # }  # \\donttest{ library(bigmemory) X <- as.big.matrix(matrix(rnorm(2000), nrow = 100)) y <- as.big.matrix(matrix(rnorm(100), ncol = 1)) fit <- pls1_stream_a(X, y, ncomp = 3) str(fit) #> List of 17 #>  $ coefficients: num [1:20, 1] -0.2457 0.0752 0.1336 -0.2089 0.1301 ... #>  $ intercept   : num 0.0927 #>  $ x_weights   : num [1:20, 1:3] -0.2551 0.0453 0.2238 -0.2733 0.097 ... #>  $ x_loadings  : num [1:20, 1:3] -0.1273 -0.0333 0.2768 -0.195 -0.0221 ... #>  $ y_loadings  : num [1:3, 1] 0.579 0.197 0.1 #>  $ x_means     : num [1:20] -0.0705 -0.0676 0.0993 0.218 -0.1831 ... #>  $ y_mean      : num -0.0159 #>  $ ncomp       : int 3 #>  $ weights     : num [1:20, 1:3] -0.2551 0.0453 0.2238 -0.2733 0.097 ... #>  $ loadings    : num [1:20, 1:3] -0.1273 -0.0333 0.2768 -0.195 -0.0221 ... #>  $ x_center    : num [1:20] -0.0705 -0.0676 0.0993 0.218 -0.1831 ... #>  $ y_center    : num -0.0159 #>  $ x_scale     : NULL #>  $ y_scale     : NULL #>  $ scores      : NULL #>  $ chunk_size  : int 1024 #>  $ call        : language pls1_stream_a(X = X, y = y, ncomp = 3) #>  - attr(*, \"class\")= chr [1:2] \"big_plsr\" \"list\" # }"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_ya.html","id":null,"dir":"Reference","previous_headings":"","what":"Single-response partial least squares regression (PLS1) yet another implementation ‚Äî pls1_dense_ya","title":"Single-response partial least squares regression (PLS1) yet another implementation ‚Äî pls1_dense_ya","text":"Single-response partial least squares regression (PLS1) yet another implementation","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_ya.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Single-response partial least squares regression (PLS1) yet another implementation ‚Äî pls1_dense_ya","text":"","code":"pls1_dense_ya(   x,   y,   ncomp,   tol = 1e-08,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )  pls1_stream_ya(   x,   y,   ncomp,   chunk_size = 4096,   tol = 1e-08,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_ya.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Single-response partial least squares regression (PLS1) yet another implementation ‚Äî pls1_dense_ya","text":"x, y Predictor response objects stored double precision bigmemory::big.matrix instances. response must contain single column. dense helper also accepts numeric vectors y converts transparently. dense routine copies predictors R matrix, streaming version accesses blocks. ncomp Number latent components extract. tol Convergence tolerance used estimating component. relevant dense variant. algorithm Algorithm used compute PLS fit. Either \"simpls\" \"nipals\". SIMPLS backend generally faster data fits memory. return_big Logical; TRUE, coefficients, scores loadings returned bigmemory::big.matrix objects. Defaults FALSE. chunk_size Number rows processed per block streaming variant.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_ya.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Single-response partial least squares regression (PLS1) yet another implementation ‚Äî pls1_dense_ya","text":"list containing regression coefficients, intercept, loadings preprocessing statistics. structure matches output underlying C++ routines.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls1_dense_ya.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Single-response partial least squares regression (PLS1) yet another implementation ‚Äî pls1_dense_ya","text":"","code":"# \\donttest{ library(bigmemory) X <- as.big.matrix(matrix(rnorm(2000), nrow = 100)) y <- as.big.matrix(matrix(rnorm(100), ncol = 1)) fit <- pls1_dense_ya(X, y, ncomp = 3) str(fit) #> List of 16 #>  $ coefficients: num [1:20, 1] 0.032576 0.063625 -0.000905 -0.055482 0.097963 ... #>  $ intercept   : num -0.135 #>  $ x_weights   : num [1:20, 1:3] -0.03604 0.16865 -0.00802 -0.01691 0.16181 ... #>  $ x_loadings  : num [1:20, 1:3] -0.108728 0.196599 0.000502 0.065538 0.092192 ... #>  $ y_loadings  : num [1:3, 1] 0.3892 0.119 0.0509 #>  $ x_means     : num [1:20] 4.29e-02 -6.20e-02 5.56e-05 -4.11e-04 1.79e-01 ... #>  $ y_mean      : num -0.1 #>  $ ncomp       : int 3 #>  $ weights     : num [1:20, 1:3] -0.03604 0.16865 -0.00802 -0.01691 0.16181 ... #>  $ loadings    : num [1:20, 1:3] -0.108728 0.196599 0.000502 0.065538 0.092192 ... #>  $ x_center    : num [1:20] 4.29e-02 -6.20e-02 5.56e-05 -4.11e-04 1.79e-01 ... #>  $ y_center    : num -0.1 #>  $ x_scale     : NULL #>  $ y_scale     : NULL #>  $ scores      : NULL #>  $ call        : language pls1_dense_ya(x = X, y = y, ncomp = 3) #>  - attr(*, \"class\")= chr [1:2] \"big_plsr\" \"list\" # }  # \\donttest{ library(bigmemory) X <- as.big.matrix(matrix(rnorm(2000), nrow = 100)) y <- as.big.matrix(matrix(rnorm(100), ncol = 1)) fit <- pls1_stream_ya(X, y, ncomp = 3) str(fit) #> List of 16 #>  $ coefficients: num [1:20, 1] 0.1189 0.0449 -0.0474 -0.0133 0.0545 ... #>  $ intercept   : num -0.215 #>  $ x_weights   : num [1:20, 1:3] 0.2465 0.1987 -0.2812 0.1544 0.0542 ... #>  $ x_loadings  : num [1:20, 1:3] 0.1625 0.2773 -0.3115 0.2517 0.0276 ... #>  $ y_loadings  : num [1:3, 1] 0.3138 0.1438 0.0843 #>  $ x_means     : num [1:20] 0.0192 0.1354 -0.0974 -0.19 -0.0393 ... #>  $ y_mean      : num -0.222 #>  $ ncomp       : int 3 #>  $ weights     : num [1:20, 1:3] 0.2465 0.1987 -0.2812 0.1544 0.0542 ... #>  $ loadings    : num [1:20, 1:3] 0.1625 0.2773 -0.3115 0.2517 0.0276 ... #>  $ x_center    : num [1:20] 0.0192 0.1354 -0.0974 -0.19 -0.0393 ... #>  $ y_center    : num -0.222 #>  $ x_scale     : NULL #>  $ y_scale     : NULL #>  $ scores      : NULL #>  $ call        : language pls1_stream_ya(x = X, y = y, ncomp = 3) #>  - attr(*, \"class\")= chr [1:2] \"big_plsr\" \"list\" # }"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls2_dense.html","id":null,"dir":"Reference","previous_headings":"","what":"Partial least squares regression for multi-response problems (PLS2) ‚Äî pls2_dense","title":"Partial least squares regression for multi-response problems (PLS2) ‚Äî pls2_dense","text":"Partial least squares regression multi-response problems (PLS2)","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls2_dense.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Partial least squares regression for multi-response problems (PLS2) ‚Äî pls2_dense","text":"","code":"pls2_dense(   X,   Y,   ncomp,   center = TRUE,   scale = FALSE,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )  pls2_stream(   X,   Y,   ncomp,   center = TRUE,   scale = FALSE,   chunk_size = 1024L,   algorithm = c(\"simpls\", \"nipals\"),   return_big = FALSE )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls2_dense.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Partial least squares regression for multi-response problems (PLS2) ‚Äî pls2_dense","text":"X bigmemory::big.matrix containing predictor variables. Y bigmemory::big.matrix storing multi-dimensional response. ncomp Number latent components compute. center inputs centered prior fitting? scale inputs scaled unit variance prior fitting? algorithm PLS backend use. Either \"simpls\" (default) \"nipals\". return_big Logical; TRUE, coefficients, scores loadings returned bigmemory::big.matrix objects. Defaults FALSE. chunk_size Number rows processed per block streaming variant.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls2_dense.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Partial least squares regression for multi-response problems (PLS2) ‚Äî pls2_dense","text":"list regression coefficients, intercept, weights, loadings preprocessing metadata.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_bootstrap.html","id":null,"dir":"Reference","previous_headings":"","what":"Bootstrap a PLS model ‚Äî pls_bootstrap","title":"Bootstrap a PLS model ‚Äî pls_bootstrap","text":"Draw bootstrap replicates fitted PLS model, refitting resample.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_bootstrap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bootstrap a PLS model ‚Äî pls_bootstrap","text":"","code":"pls_bootstrap(   X,   Y,   ncomp,   R = 100L,   algorithm = c(\"simpls\", \"nipals\", \"kernelpls\", \"widekernelpls\"),   backend = \"arma\",   conf = 0.95,   seed = NULL,   type = c(\"xy\", \"xt\"),   parallel = c(\"none\", \"future\"),   future_seed = TRUE,   return_scores = FALSE,   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_bootstrap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bootstrap a PLS model ‚Äî pls_bootstrap","text":"X Predictor matrix. Y Response matrix vector. ncomp Number components. R Number bootstrap replications. algorithm Backend algorithm (\"simpls\", \"nipals\", \"kernelpls\" \"widekernelpls\"). backend Backend argument passed fitting routine. conf Confidence level. seed Optional seed. type Character; bootstrap scheme, e.g. \"pairs\", \"residual\", \"parametric\". parallel Logical character; TRUE one c(\"sequential\", \"multisession\", \"multicore\"), uses future framework. future_seed Logical integer; forwarded future.seed reproducible parallel streams. return_scores Logical; TRUE, return component scores replicate (may large). ... Additional arguments forwarded pls_fit().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_bootstrap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bootstrap a PLS model ‚Äî pls_bootstrap","text":"list bootstrap estimates summaries.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_bootstrap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bootstrap a PLS model ‚Äî pls_bootstrap","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) pls_bootstrap(X, y, ncomp = 2, R = 20) #> $mean #>            [,1] #> [1,]  0.9913106 #> [2,] -0.4513641 #> [3,] -0.0364948 #>  #> $lower #>            [,1] #> [1,]  0.9008479 #> [2,] -0.5554312 #> [3,] -0.1578291 #>  #> $upper #>             [,1] #> [1,]  1.04643103 #> [2,] -0.29747912 #> [3,]  0.03218767 #>  #> $samples #> $samples[[1]] #>             [,1] #> [1,]  1.01370235 #> [2,] -0.47362869 #> [3,] -0.01871238 #>  #> $samples[[2]] #>             [,1] #> [1,]  1.00805220 #> [2,] -0.29373547 #> [3,] -0.03438166 #>  #> $samples[[3]] #>             [,1] #> [1,]  1.02482494 #> [2,] -0.43981055 #> [3,] -0.06598034 #>  #> $samples[[4]] #>            [,1] #> [1,]  0.9217837 #> [2,] -0.4195667 #> [3,] -0.1675766 #>  #> $samples[[5]] #>             [,1] #> [1,]  0.90088847 #> [2,] -0.56898923 #> [3,] -0.06228952 #>  #> $samples[[6]] #>             [,1] #> [1,]  1.03738707 #> [2,] -0.48335047 #> [3,]  0.02431306 #>  #> $samples[[7]] #>             [,1] #> [1,]  1.03665669 #> [2,] -0.41794678 #> [3,] -0.01365786 #>  #> $samples[[8]] #>             [,1] #> [1,]  0.90081111 #> [2,] -0.53015677 #> [3,] -0.06947427 #>  #> $samples[[9]] #>              [,1] #> [1,]  1.046587228 #> [2,] -0.420771928 #> [3,]  0.006859411 #>  #> $samples[[10]] #>            [,1] #> [1,]  1.0205916 #> [2,] -0.3137173 #> [3,] -0.0397945 #>  #> $samples[[11]] #>             [,1] #> [1,]  1.04625839 #> [2,] -0.46446341 #> [3,]  0.00655102 #>  #> $samples[[12]] #>            [,1] #> [1,]  0.9487480 #> [2,] -0.5126467 #> [3,] -0.1244670 #>  #> $samples[[13]] #>             [,1] #> [1,]  0.96695471 #> [2,] -0.52643057 #> [3,] -0.01118588 #>  #> $samples[[14]] #>             [,1] #> [1,]  1.00591617 #> [2,] -0.51301573 #> [3,]  0.03150559 #>  #> $samples[[15]] #>             [,1] #> [1,]  1.00430562 #> [2,] -0.49498888 #> [3,] -0.03036961 #>  #> $samples[[16]] #>              [,1] #> [1,]  1.006820717 #> [2,] -0.490124440 #> [3,]  0.006203915 #>  #> $samples[[17]] #>            [,1] #> [1,]  1.0193070 #> [2,] -0.3016168 #> [3,] -0.1470557 #>  #> $samples[[18]] #>            [,1] #> [1,]  0.9601683 #> [2,] -0.5404460 #> [3,] -0.0334001 #>  #> $samples[[19]] #>             [,1] #> [1,]  0.99306219 #> [2,] -0.51709589 #> [3,] -0.01978846 #>  #> $samples[[20]] #>             [,1] #> [1,]  0.96338505 #> [2,] -0.30477937 #> [3,]  0.03280478 #>  #>  #> $type #> [1] \"xy\" #>  #> $base_fit #> $coefficients #>             [,1] #> [1,]  1.01956855 #> [2,] -0.45295054 #> [3,] -0.01325522 #>  #> $intercept #> [1] -0.01093997 #>  #> $x_weights #>             [,1]        [,2] #> [1,]  0.20997486  0.04194897 #> [2,] -0.08441327 -0.08227311 #> [3,] -0.03572358  0.23617705 #>  #> $x_loadings #>           [,1]       [,2] #> [1,]  3.972259  0.3468121 #> [2,] -1.452865 -0.7870847 #> [3,] -1.211650  3.8983283 #>  #> $y_loadings #>          [,1]     [,2] #> [1,] 4.724127 0.658436 #>  #> $x_means #> [1]  0.14162380 -0.05125716  0.10648523 #>  #> $y_means #> [1] 0.1552607 #>  #> $ncomp #> [1] 2 #>  #> $mode #> [1] \"pls1\" #>  #> $algorithm #> [1] \"simpls\" #>  #> $x_center #> [1]  0.14162380 -0.05125716  0.10648523 #>  #> $y_center #> [1] 0.1552607 #>  #> $X #>              [,1]        [,2]        [,3] #>  [1,] -0.56047565 -1.06782371 -0.69470698 #>  [2,] -0.23017749 -0.21797491 -0.20791728 #>  [3,]  1.55870831 -1.02600445 -1.26539635 #>  [4,]  0.07050839 -0.72889123  2.16895597 #>  [5,]  0.12928774 -0.62503927  1.20796200 #>  [6,]  1.71506499 -1.68669331 -1.12310858 #>  [7,]  0.46091621  0.83778704 -0.40288484 #>  [8,] -1.26506123  0.15337312 -0.46665535 #>  [9,] -0.68685285 -1.13813694  0.77996512 #> [10,] -0.44566197  1.25381492 -0.08336907 #> [11,]  1.22408180  0.42646422  0.25331851 #> [12,]  0.35981383 -0.29507148 -0.02854676 #> [13,]  0.40077145  0.89512566 -0.04287046 #> [14,]  0.11068272  0.87813349  1.36860228 #> [15,] -0.55584113  0.82158108 -0.22577099 #> [16,]  1.78691314  0.68864025  1.51647060 #> [17,]  0.49785048  0.55391765 -1.54875280 #> [18,] -1.96661716 -0.06191171  0.58461375 #> [19,]  0.70135590 -0.30596266  0.12385424 #> [20,] -0.47279141 -0.38047100  0.21594157 #>  #> attr(,\"class\") #> [1] \"big_plsr\" \"list\"     #>  #> $bca_lower #>            [,1] #> [1,]  0.9569592 #> [2,] -0.5388344 #> [3,] -0.1286506 #>  #> $bca_upper #>             [,1] #> [1,]  1.04658723 #> [2,] -0.29373547 #> [3,]  0.03280478 #>  #> $jackknife #>             [,1]        [,2]       [,3]         [,4]         [,5]         [,6] #> [1,]  1.02017326  1.01810673  1.0157243  1.022873634  1.022063121  1.006261044 #> [2,] -0.45607710 -0.45505536 -0.4721037 -0.447478457 -0.450290543 -0.459143899 #> [3,] -0.01054496 -0.01446393 -0.0163173 -0.007862433 -0.008440752 -0.001161789 #>             [,7]        [,8]       [,9]        [,10]       [,11]       [,12] #> [1,]  1.02218562  1.01821092  1.0298271  1.033548831  1.02008830  1.02213589 #> [2,] -0.44599976 -0.46178303 -0.4318445 -0.451637856 -0.46045832 -0.45635818 #> [3,] -0.01287389 -0.01462013 -0.0257894  0.001207447 -0.01504661 -0.01400757 #>            [,13]        [,14]       [,15]       [,16]       [,17]       [,18] #> [1,]  1.02077655  1.018276698  1.01973985  0.97161991  1.02227656  0.99412356 #> [2,] -0.45207275 -0.453293180 -0.43544349 -0.51802509 -0.44269325 -0.48281199 #> [3,] -0.01111254 -0.005173051 -0.01309007 -0.03708979 -0.02380684 -0.01061448 #>            [,19]       [,20] #> [1,]  1.01867452  1.01955556 #> [2,] -0.45201139 -0.45425090 #> [3,] -0.01344972 -0.01466117 #>"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cross_validate.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validate PLS models ‚Äî pls_cross_validate","title":"Cross-validate PLS models ‚Äî pls_cross_validate","text":"Cross-validate PLS models","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cross_validate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validate PLS models ‚Äî pls_cross_validate","text":"","code":"pls_cross_validate(   X,   Y,   ncomp,   folds = 5L,   type = c(\"kfold\", \"loo\"),   algorithm = c(\"simpls\", \"nipals\", \"kernelpls\", \"widekernelpls\"),   backend = \"arma\",   metrics = c(\"rmse\", \"mae\", \"r2\"),   seed = NULL,   parallel = c(\"none\", \"future\"),   future_seed = TRUE,   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cross_validate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validate PLS models ‚Äî pls_cross_validate","text":"X Predictor matrix accepted pls_fit() Y Response matrix vector accepted pls_fit() ncomp Integer; components grid evaluate. folds Number folds (ignored type = \"loo\"). type Either \"kfold\" (default) \"loo\". algorithm Backend algorithm: \"simpls\", \"nipals\", \"kernelpls\" \"widekernelpls\". backend Backend passed pls_fit(). metrics Metrics compute (subset \"rmse\", \"mae\", \"r2\"). seed Optional seed reproducibility. parallel Logical character; semantics pls_bootstrap(). future_seed Logical integer; reproducible seeds parallel evaluation. ... Passed pls_fit().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cross_validate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validate PLS models ‚Äî pls_cross_validate","text":"list containing per-fold metrics summary across folds.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cross_validate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-validate PLS models ‚Äî pls_cross_validate","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) pls_cross_validate(X, y, ncomp = 2, folds = 3) #> $details #>    fold ncomp metric      value #> 1     1     1   rmse 0.16328299 #> 2     1     1    mae 0.12805868 #> 3     1     1     r2 0.97080073 #> 4     1     2   rmse 0.07303034 #> 5     1     2    mae 0.05597072 #> 6     1     2     r2 0.99415887 #> 7     2     1   rmse 0.24657144 #> 8     2     1    mae 0.20919627 #> 9     2     1     r2 0.95049915 #> 10    2     2   rmse 0.11001802 #> 11    2     2    mae 0.08976574 #> 12    2     2     r2 0.99014504 #> 13    3     1   rmse 0.36542334 #> 14    3     1    mae 0.34056432 #> 15    3     1     r2 0.78937830 #> 16    3     2   rmse 0.38837465 #> 17    3     2    mae 0.37082266 #> 18    3     2     r2 0.76209023 #>  #> $summary #>   ncomp metric     value #> 1     1    mae 0.2259398 #> 2     2    mae 0.1721864 #> 3     1     r2 0.9035594 #> 4     2     r2 0.9154647 #> 5     1   rmse 0.2584259 #> 6     2   rmse 0.1904743 #>"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cv_select.html","id":null,"dir":"Reference","previous_headings":"","what":"Select components from cross-validation results ‚Äî pls_cv_select","title":"Select components from cross-validation results ‚Äî pls_cv_select","text":"Select components cross-validation results","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cv_select.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select components from cross-validation results ‚Äî pls_cv_select","text":"","code":"pls_cv_select(cv_result, metric = c(\"rmse\", \"mae\", \"r2\"), minimise = NULL)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cv_select.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select components from cross-validation results ‚Äî pls_cv_select","text":"cv_result Result returned pls_cross_validate(). metric Metric optimise. minimise Logical; whether metric minimised.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cv_select.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select components from cross-validation results ‚Äî pls_cv_select","text":"Selected number components.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_cv_select.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select components from cross-validation results ‚Äî pls_cv_select","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) cv <- pls_cross_validate(X, y, ncomp = 2, folds = 3) pls_cv_select(cv, metric = \"rmse\") #> [1] 2"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Unified PLS fit with auto backend and selectable algorithm ‚Äî pls_fit","title":"Unified PLS fit with auto backend and selectable algorithm ‚Äî pls_fit","text":"Dispatches dense (Arm/BLAS) backend -memory matrices streaming big.matrix backend X (Y) big.matrix. Algorithm can chosen : \"simpls\" (default), \"nipals\", \"kernelpls\", \"widekernelpls\", \"rkhs\" (Rosipal & Trejo), \"klogitpls\", \"sparse_kpls\", \"rkhs_xy\" (double RKHS), \"kf_pls\" (Kalman-filter PLS, streaming). \"kernelpls\" paths now include streaming XX' variant big.matrix inputs, optional row-chunking loop controlled chunk_cols.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unified PLS fit with auto backend and selectable algorithm ‚Äî pls_fit","text":"","code":"pls_fit(   X,   y,   ncomp,   tol = 1e-08,   backend = c(\"auto\", \"arma\", \"bigmem\"),   mode = c(\"auto\", \"pls1\", \"pls2\"),   algorithm = c(\"auto\", \"simpls\", \"nipals\", \"kernelpls\", \"widekernelpls\", \"rkhs\",     \"klogitpls\", \"sparse_kpls\", \"rkhs_xy\", \"kf_pls\"),   scores = c(\"none\", \"r\", \"big\"),   chunk_size = 10000L,   chunk_cols = NULL,   scores_name = \"scores\",   scores_target = c(\"auto\", \"new\", \"existing\"),   scores_bm = NULL,   scores_backingfile = NULL,   scores_backingpath = NULL,   scores_descriptorfile = NULL,   scores_colnames = NULL,   return_scores_descriptor = FALSE,   coef_threshold = NULL,   kernel = c(\"linear\", \"rbf\", \"poly\", \"sigmoid\"),   gamma = 1,   degree = 3L,   coef0 = 0,   approx = c(\"none\", \"nystrom\", \"rff\"),   approx_rank = NULL,   class_weights = NULL )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unified PLS fit with auto backend and selectable algorithm ‚Äî pls_fit","text":"X numeric matrix bigmemory::big.matrix y numeric vector/matrix big.matrix ncomp number latent components tol numeric tolerance used core solver backend one \"auto\", \"arma\", \"bigmem\" mode one \"auto\", \"pls1\", \"pls2\" algorithm one \"auto\", \"simpls\", \"nipals\", \"kernelpls\", \"widekernelpls\", \"rkhs\", \"klogitpls\", \"sparse_kpls\", \"rkhs_xy\", \"kf_pls\" scores one \"none\", \"r\", \"big\" chunk_size chunk size bigmem backend chunk_cols columns chunk size bigmem backend scores_name name dense scores (output big.matrix) scores_target one \"auto\", \"new\", \"existing\" scores_bm optional existing big.matrix descriptor scores scores_backingfile Character; file name file-backed scores (scores=\"big\"). scores_backingpath Character; directory file-backed scores. Defaults getwd() tempdir() streamed predict, unless overridden. scores_descriptorfile Character; descriptor file name file-backed scores. scores_colnames optional character vector score column names return_scores_descriptor logical; TRUE scores big.matrix, add $scores_descriptor coef_threshold Optional non-negative value used hard-threshold fitted coefficients model estimation. supplied, absolute coefficients strictly threshold set zero via pls_threshold(). kernel kernel name RKHS/KPLS (\"linear\", \"rbf\", \"poly\", \"sigmoid\") gamma RBF/sigmoid/poly scale parameter degree polynomial degree coef0 polynomial/sigmoid bias approx kernel approximation: \"none\", \"nystrom\", \"rff\" approx_rank rank (columns / features) approximation class_weights optional numeric weights classes klogitpls","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unified PLS fit with auto backend and selectable algorithm ‚Äî pls_fit","text":"list coefficients, intercept, weights, loadings, means, optionally $scores.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unified PLS fit with auto backend and selectable algorithm ‚Äî pls_fit","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\", algorithm = \"simpls\") head(pls_predict_response(fit, X, ncomp = 2)) #> [1] -0.2557041 -0.3103345  1.8935717  0.1961492  0.2217772  2.3503614"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_information_criteria.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute information criteria for component selection ‚Äî pls_information_criteria","title":"Compute information criteria for component selection ‚Äî pls_information_criteria","text":"Compute information criteria component selection","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_information_criteria.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute information criteria for component selection ‚Äî pls_information_criteria","text":"","code":"pls_information_criteria(object, X, Y, max_comp = NULL)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_information_criteria.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute information criteria for component selection ‚Äî pls_information_criteria","text":"object fitted PLS model. X Training design matrix. Y Training response matrix vector. max_comp Maximum number components consider.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_information_criteria.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute information criteria for component selection ‚Äî pls_information_criteria","text":"data frame RSS, RMSE, AIC BIC per component.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_information_criteria.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute information criteria for component selection ‚Äî pls_information_criteria","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") pls_information_criteria(fit, X, y) #>   ncomp       rss      rmse       aic       bic #> 1     1 1.1527765 0.2400809 -53.07118 -51.07971 #> 2     2 0.7192385 0.1896363 -60.50589 -57.51869"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_response.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict responses from a PLS fit ‚Äî pls_predict_response","title":"Predict responses from a PLS fit ‚Äî pls_predict_response","text":"Predict responses PLS fit","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict responses from a PLS fit ‚Äî pls_predict_response","text":"","code":"pls_predict_response(object, newdata, ncomp = NULL)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict responses from a PLS fit ‚Äî pls_predict_response","text":"object fitted PLS model. newdata Predictor matrix scoring. ncomp Number components use.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict responses from a PLS fit ‚Äî pls_predict_response","text":"numeric matrix vector predictions.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict responses from a PLS fit ‚Äî pls_predict_response","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") pls_predict_response(fit, X, ncomp = 2) #>  [1] -1.0141790 -0.5820463  1.1675424  0.1269469  0.4234421  0.5985047 #>  [7]  0.3557169 -0.3717287 -1.1921817 -0.2849629"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_scores.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict latent scores from a PLS fit ‚Äî pls_predict_scores","title":"Predict latent scores from a PLS fit ‚Äî pls_predict_scores","text":"Predict latent scores PLS fit","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_scores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict latent scores from a PLS fit ‚Äî pls_predict_scores","text":"","code":"pls_predict_scores(object, newdata, ncomp = NULL)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_scores.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict latent scores from a PLS fit ‚Äî pls_predict_scores","text":"object fitted PLS model. newdata Predictor matrix scoring. ncomp Number components use.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_scores.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict latent scores from a PLS fit ‚Äî pls_predict_scores","text":"Matrix component scores.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_predict_scores.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict latent scores from a PLS fit ‚Äî pls_predict_scores","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") pls_predict_scores(fit, X, ncomp = 2) #>                t1          t2 #>  [1,] -0.13544977 -0.52708945 #>  [2,] -0.17098259 -0.14902942 #>  [3,]  0.50972517  0.24632134 #>  [4,]  0.08135113  0.04355323 #>  [5,]  0.08487733  0.26452562 #>  [6,]  0.55636410 -0.25130464 #>  [7,]  0.08732598  0.20957085 #>  [8,] -0.44910850  0.39409855 #>  [9,] -0.26198780 -0.48844206 #> [10,] -0.30211506  0.25779599"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_select_components.html","id":null,"dir":"Reference","previous_headings":"","what":"Component selection via information criteria ‚Äî pls_select_components","title":"Component selection via information criteria ‚Äî pls_select_components","text":"Component selection via information criteria","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_select_components.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Component selection via information criteria ‚Äî pls_select_components","text":"","code":"pls_select_components(   object,   X,   Y,   criteria = c(\"aic\", \"bic\"),   max_comp = NULL )"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_select_components.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Component selection via information criteria ‚Äî pls_select_components","text":"object fitted PLS model. X Training design matrix. Y Training response matrix vector. criteria Character vector specifying criteria compute. max_comp Maximum number components consider.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_select_components.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Component selection via information criteria ‚Äî pls_select_components","text":"list per-component table selected components.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_select_components.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Component selection via information criteria ‚Äî pls_select_components","text":"","code":"set.seed(123) X <- matrix(rnorm(60), nrow = 20) y <- X[, 1] - 0.5 * X[, 2] + rnorm(20, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") pls_select_components(fit, X, y) #> $table #>   ncomp       rss      rmse       aic       bic #> 1     1 1.1527765 0.2400809 -53.07118 -51.07971 #> 2     2 0.7192385 0.1896363 -60.50589 -57.51869 #>  #> $best #> $best$aic #> [1] 2 #>  #> $best$bic #> [1] 2 #>  #>"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_threshold.html","id":null,"dir":"Reference","previous_headings":"","what":"Naive sparsity control by coefficient thresholding ‚Äî pls_threshold","title":"Naive sparsity control by coefficient thresholding ‚Äî pls_threshold","text":"Naive sparsity control coefficient thresholding","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_threshold.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Naive sparsity control by coefficient thresholding ‚Äî pls_threshold","text":"","code":"pls_threshold(object, threshold)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_threshold.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Naive sparsity control by coefficient thresholding ‚Äî pls_threshold","text":"object fitted PLS model. threshold Values absolute magnitude set zero.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_threshold.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Naive sparsity control by coefficient thresholding ‚Äî pls_threshold","text":"modified copy object thresholded coefficients.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_threshold.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Naive sparsity control by coefficient thresholding ‚Äî pls_threshold","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2) pls_threshold(fit, threshold = 0.05) #> $coefficients #>            [,1] #> [1,]  0.7945827 #> [2,] -0.3874556 #> [3,]  0.1205194 #> [4,]  0.3708827 #>  #> $intercept #> [1] -0.07729456 #>  #> $x_weights #>              [,1]       [,2] #> [1,]  0.278021666  0.2224039 #> [2,]  0.002259737 -0.2982192 #> [3,] -0.039744939  0.1465173 #> [4,]  0.126996697  0.1076294 #>  #> $x_loadings #>           [,1]       [,2] #> [1,]  2.803684  0.1385679 #> [2,]  1.818750 -2.3265396 #> [3,] -1.468176  1.6845482 #> [4,]  1.244542  0.2652412 #>  #> $y_loadings #>          [,1]     [,2] #> [1,] 1.807709 1.312928 #>  #> $x_means #> [1]  0.07462564  0.20862196 -0.42455887  0.32204455 #>  #> $y_means #> [1] -0.03055689 #>  #> $ncomp #> [1] 2 #>  #> $mode #> [1] \"pls1\" #>  #> $algorithm #> [1] \"simpls\" #>  #> $x_center #> [1]  0.07462564  0.20862196 -0.42455887  0.32204455 #>  #> $y_center #> [1] -0.03055689 #>  #> $X #>              [,1]       [,2]       [,3]        [,4] #>  [1,] -0.56047565  1.2240818 -1.0678237  0.42646422 #>  [2,] -0.23017749  0.3598138 -0.2179749 -0.29507148 #>  [3,]  1.55870831  0.4007715 -1.0260044  0.89512566 #>  [4,]  0.07050839  0.1106827 -0.7288912  0.87813349 #>  [5,]  0.12928774 -0.5558411 -0.6250393  0.82158108 #>  [6,]  1.71506499  1.7869131 -1.6866933  0.68864025 #>  [7,]  0.46091621  0.4978505  0.8377870  0.55391765 #>  [8,] -1.26506123 -1.9666172  0.1533731 -0.06191171 #>  [9,] -0.68685285  0.7013559 -1.1381369 -0.30596266 #> [10,] -0.44566197 -0.4727914  1.2538149 -0.38047100 #>  #> $coef_threshold #> [1] 0.05 #>  #> attr(,\"class\") #> [1] \"big_plsr\" \"list\""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_vip.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable importance in projection (VIP) scores ‚Äî pls_vip","title":"Variable importance in projection (VIP) scores ‚Äî pls_vip","text":"Variable importance projection (VIP) scores","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_vip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable importance in projection (VIP) scores ‚Äî pls_vip","text":"","code":"pls_vip(object, comps = NULL)"},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_vip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable importance in projection (VIP) scores ‚Äî pls_vip","text":"object fitted PLS model. comps Components used compute VIP scores. Defaults available components.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_vip.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Variable importance in projection (VIP) scores ‚Äî pls_vip","text":"named numeric vector VIP scores.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/pls_vip.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable importance in projection (VIP) scores ‚Äî pls_vip","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") pls_vip(fit) #> [1] 0.3679251 0.2478539 0.1299815 0.1706395"},{"path":"https://fbertran.github.io/bigPLSR/reference/predict.big_plsr.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict method for big_plsr objects ‚Äî predict.big_plsr","title":"Predict method for big_plsr objects ‚Äî predict.big_plsr","text":"Predict method big_plsr objects","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/predict.big_plsr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict method for big_plsr objects ‚Äî predict.big_plsr","text":"","code":"# S3 method for class 'big_plsr' predict(   object,   newdata,   ncomp = NULL,   type = c(\"response\", \"scores\", \"prob\", \"class\"),   ... )"},{"path":"https://fbertran.github.io/bigPLSR/reference/predict.big_plsr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict method for big_plsr objects ‚Äî predict.big_plsr","text":"object fitted PLS model produced pls_fit(). newdata Matrix bigmemory::big.matrix predictor values. ncomp Number components use prediction. type Either \"response\" (default) \"scores\". ... Unused, compatibility generic.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/predict.big_plsr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict method for big_plsr objects ‚Äî predict.big_plsr","text":"Predicted responses component scores.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/predict.big_plsr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict method for big_plsr objects ‚Äî predict.big_plsr","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") predict(fit, X, ncomp = 2) #>  [1] -1.0141790 -0.5820463  1.1675424  0.1269469  0.4234421  0.5985047 #>  [7]  0.3557169 -0.3717287 -1.1921817 -0.2849629"},{"path":"https://fbertran.github.io/bigPLSR/reference/print.summary.big_plsr.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a summary.big_plsr object ‚Äî print.summary.big_plsr","title":"Print a summary.big_plsr object ‚Äî print.summary.big_plsr","text":"Print summary.big_plsr object","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/print.summary.big_plsr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a summary.big_plsr object ‚Äî print.summary.big_plsr","text":"","code":"# S3 method for class 'summary.big_plsr' print(x, ...)"},{"path":"https://fbertran.github.io/bigPLSR/reference/print.summary.big_plsr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a summary.big_plsr object ‚Äî print.summary.big_plsr","text":"x summary.big_plsr object. ... Passed lower-level print methods.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/print.summary.big_plsr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print a summary.big_plsr object ‚Äî print.summary.big_plsr","text":"x, invisibly.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/print.summary.big_plsr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print a summary.big_plsr object ‚Äî print.summary.big_plsr","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") print(summary(fit)) #> Partial least squares regression summary #> Algorithm: simpls  #> Mode: pls1  #> Components: 2  #> Score variance: 0.1111, 0.1111  #> Explained variance (%): 50.0, 50.0  #> VIP (first 10): #> [1] 0.3679251 0.2478539 0.1299815 0.1706395"},{"path":"https://fbertran.github.io/bigPLSR/reference/sim_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated dataset ‚Äî sim_data","title":"Simulated dataset ‚Äî sim_data","text":"dataset provides explantory variables simulations censoring status.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/sim_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated dataset ‚Äî sim_data","text":"data frame 1000 observations following 11 variables. status binary vector X1 numeric vector X2 numeric vector X3 numeric vector X4 numeric vector X5 numeric vector X6 numeric vector X7 numeric vector X8 numeric vector X9 numeric vector X10 numeric vector","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/sim_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simulated dataset ‚Äî sim_data","text":"TODO.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/sim_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulated dataset ‚Äî sim_data","text":"","code":"# \\donttest{ data(sim_data) X_sim_data_train <- sim_data[1:800,2:11] C_sim_data_train <- sim_data$status[1:800] X_sim_data_test <- sim_data[801:1000,2:11] C_sim_data_test <- sim_data$status[801:1000] rm(X_sim_data_train,C_sim_data_train,X_sim_data_test,C_sim_data_test) # }"},{"path":"https://fbertran.github.io/bigPLSR/reference/summarise_pls_bootstrap.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarise bootstrap estimates ‚Äî summarise_pls_bootstrap","title":"Summarise bootstrap estimates ‚Äî summarise_pls_bootstrap","text":"Summarise bootstrap estimates","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/summarise_pls_bootstrap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarise bootstrap estimates ‚Äî summarise_pls_bootstrap","text":"","code":"summarise_pls_bootstrap(boot_result)"},{"path":"https://fbertran.github.io/bigPLSR/reference/summarise_pls_bootstrap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarise bootstrap estimates ‚Äî summarise_pls_bootstrap","text":"boot_result Result returned pls_bootstrap().","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/summarise_pls_bootstrap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarise bootstrap estimates ‚Äî summarise_pls_bootstrap","text":"data frame containing mean, standard deviation, percentile BCa confidence intervals coefficient.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/summary.big_plsr.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize a big_plsr model ‚Äî summary.big_plsr","title":"Summarize a big_plsr model ‚Äî summary.big_plsr","text":"Summarize big_plsr model","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/summary.big_plsr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize a big_plsr model ‚Äî summary.big_plsr","text":"","code":"# S3 method for class 'big_plsr' summary(object, ..., X = NULL, Y = NULL)"},{"path":"https://fbertran.github.io/bigPLSR/reference/summary.big_plsr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize a big_plsr model ‚Äî summary.big_plsr","text":"object fitted PLS model. ... Unused. X Optional design matrix recompute reconstruction metrics. Y Optional response matrix/vector.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/summary.big_plsr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize a big_plsr model ‚Äî summary.big_plsr","text":"object class summary.big_plsr.","code":""},{"path":"https://fbertran.github.io/bigPLSR/reference/summary.big_plsr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize a big_plsr model ‚Äî summary.big_plsr","text":"","code":"set.seed(123) X <- matrix(rnorm(40), nrow = 10) y <- X[, 1] - 0.5 * X[, 2] + rnorm(10, sd = 0.1) fit <- pls_fit(X, y, ncomp = 2, scores = \"r\") summary(fit) #> Partial least squares regression summary #> Algorithm: simpls  #> Mode: pls1  #> Components: 2  #> Score variance: 0.1111, 0.1111  #> Explained variance (%): 50.0, 50.0  #> VIP (first 10): #> [1] 0.3679251 0.2478539 0.1299815 0.1706395"},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-071","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.7.1","title":"bigPLSR 0.7.1","text":"New tuning option: options(bigPLSR.stream.block_align = 8192L). streamed backends (bigmem SIMPLS, streamed scores, RKHS/klogitpls Gram passes, bigmem predict) round chunk_size multiple alignment, clamp available number rows. Typical sweet spots 4096‚Äì16384 modern CPUs. always need scores disk, prefer scores = \"big\" avoid large R dense allocations; streams directly big.matrix. Added benchmarks results analysis two vignettes.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-070","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.7.0","title":"bigPLSR 0.7.0","text":"Added plot_pls_bootstrap_scores() group-aware ellipses plot_pls_biplot() visualise latent structures. Exposed bigPLSR_stream_kstats() streamed RKHS centering statistics corrected bigmemory RKHS interface accept dense response blocks.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-069","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.9","title":"bigPLSR 0.6.9","text":"Stabilised kernel logistic PLS class weighting, reinstated IRLS fallbacks improved dense/big-memory parity. Reworked Kalman-filter state helper reuse SIMPLS backend, ensuring identical coefficients/intercepts batch fits. Added dedicated RKHS/RKHS-XY plotting vignettes, refreshed PLS1/PLS2 benchmarking guides notes new algorithms parallel helpers.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-068","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.8","title":"bigPLSR 0.6.8","text":"Added optional future-powered parallel execution pls_cross_validate() pls_bootstrap(). Extended pls_bootstrap() (X, Y) (X, T) strategies, percentile BCa confidence intervals, numerical summaries, coefficient boxplots. Added group-aware score plotting confidence ellipses plot_pls_individuals(). Added vignettes covering cross-validation/information-criteria workflows bootstrap diagnostics.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-067","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.7","title":"bigPLSR 0.6.7","text":"kernelpls backend=‚Äòbigmem‚Äô now uses streaming XX·µó/column paths; previous dense fallback removed. Control options(bigPLSR.kpls_gram = ‚Äòrows‚Äô|‚Äòcols‚Äô|‚Äòauto‚Äô) bigPLSR.chunk_rows, bigPLSR.chunk_cols.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-066","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.6","title":"bigPLSR 0.6.6","text":"Vignettes: Kernel Streaming PLS Methods, Automatic Algorithm Selection. Stub C++ entry points RKHS / kernel logistic / sparse KPLS / KF-PLS.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-065","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.5","title":"bigPLSR 0.6.5","text":"XtX SIMPLS (standard cross-product SIMPLS), XXt (‚Äúwidekernelpls‚Äù) n << p, NIPALS memory tight rank low. Tuned options(bigPLSR.mem_budget_gb = 8). Users can override algorithm=. Kernel-style PLS routes: algorithm = \"kernelpls\" algorithm = \"widekernelpls\" implementing Dayal & MacGregor‚Äìstyle (1997) kernel PLS X-space wide-X (XX·µó) space. Implemented high-performance kernel wide-kernel PLS algorithms pls_fit() dense bigmemory backends using RcppArmadillo. Introduced optional coefficient thresholding. Added fast-running examples exported functions improve documentation usability CRAN.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-064","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.4","title":"bigPLSR 0.6.4","text":"Added kernel PLS wide-kernel PLS algorithms pls_fit() dense bigmemory backends. Refreshed plotting helpers correlation circles, arrow-based loadings dedicated VIP bar plot. Introduced convenience prediction wrappers, information-criteria helpers, expanded cross-validation/bootstrapping utilities support new algorithms. Improved summaries explained-variance reporting updated package documentation.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-062","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.2","title":"bigPLSR 0.6.2","text":"Added cross validation bootstrap plsR.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-061","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.1","title":"bigPLSR 0.6.1","text":"Added plots summaries pls_fit().","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-060","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.6.0","title":"bigPLSR 0.6.0","text":"Added unified path pls_fit() plsR regression features : dense bigmemory, simpls nipals.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-050","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.5.0","title":"bigPLSR 0.5.0","text":"Added several plsR implementations. Benchmarks.","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-040","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.4.0","title":"bigPLSR 0.4.0","text":"Maintainer email update Added unit tests","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-030","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.3.0","title":"bigPLSR 0.3.0","text":"Code update","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-020","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.2.0","title":"bigPLSR 0.2.0","text":"Improving code help pages","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-010","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.1.0","title":"bigPLSR 0.1.0","text":"Implementing gpls, sgpls based models","code":""},{"path":"https://fbertran.github.io/bigPLSR/news/index.html","id":"bigplsr-001","dir":"Changelog","previous_headings":"","what":"bigPLSR 0.0.1","title":"bigPLSR 0.0.1","text":"Package creation","code":""}]
