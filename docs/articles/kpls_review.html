<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Kernel and Streaming PLS Methods in bigPLSR ‚Ä¢ bigPLSR</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Kernel and Streaming PLS Methods in bigPLSR">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">bigPLSR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.7.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/bigPLSR-auto-selection.html">Automatic Algorithm Selection in bigPLSR</a></li>
    <li><a class="dropdown-item" href="../articles/bigPLSR-kpls-streaming.html">Streaming Kernel PLS in bigPLSR: XX^T and Column-Chunked Variants</a></li>
    <li><a class="dropdown-item" href="../articles/bootstrap-strategies.html">Bootstrap strategies for bigPLSR</a></li>
    <li><a class="dropdown-item" href="../articles/cross-validation-ic.html">Cross-validation and Information Criteria in bigPLSR</a></li>
    <li><a class="dropdown-item" href="../articles/double-rkhs-pls.html">Double RKHS PLS (rkhs_xy): Theory and Usage</a></li>
    <li><a class="dropdown-item" href="../articles/external-pls-benchmarks-long.html">External PLS benchmarks for bigPLSR: detailed analysis</a></li>
    <li><a class="dropdown-item" href="../articles/external-pls-benchmarks-short.html">Benchmarking bigPLSR against external PLS implementations</a></li>
    <li><a class="dropdown-item" href="../articles/kf-pls.html">KF-PLS: Streaming PLS with Kalman-style updates</a></li>
    <li><a class="dropdown-item" href="../articles/klogitpls.html">Kernel Logistic PLS</a></li>
    <li><a class="dropdown-item" href="../articles/kpls_review.html">Kernel and Streaming PLS Methods in bigPLSR</a></li>
    <li><a class="dropdown-item" href="../articles/plotting-guide.html">Visualising PLS Fits with bigPLSR</a></li>
    <li><a class="dropdown-item" href="../articles/pls1-benchmark.html">Benchmarking PLS1 Implementations</a></li>
    <li><a class="dropdown-item" href="../articles/pls2-benchmark.html">Benchmarking PLS2 Implementations</a></li>
    <li><a class="dropdown-item" href="../articles/rkhs-overview.html">RKHS-based Algorithms in bigPLSR</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/fbertran/bigPLSR/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Kernel and Streaming PLS Methods in bigPLSR</h1>
                        <h4 data-toc-skip class="author">Fr√©d√©ric
Bertrand</h4>
            <address class="author_afil">
      Cedric, Cnam,
Paris<br><a class="author_email" href="mailto:#"></a><a href="mailto:frederic.bertrand@lecnam.net" class="email">frederic.bertrand@lecnam.net</a>
      </address>
                  
            <h4 data-toc-skip class="date">2025-11-18</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/fbertran/bigPLSR/blob/HEAD/vignettes/kpls_review.Rmd" class="external-link"><code>vignettes/kpls_review.Rmd</code></a></small>
      <div class="d-none name"><code>kpls_review.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="notation">Notation<a class="anchor" aria-label="anchor" href="#notation"></a>
</h2>
<p>Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>n</mi><mo>√ó</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X \in \mathbb{R}^{n\times p}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>n</mi><mo>√ó</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">Y \in \mathbb{R}^{n\times m}</annotation></semantics></math>.
We assume column-centered data unless stated otherwise. PLS extracts
latent scores
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>t</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">T=[t_1,\dots,t_a]</annotation></semantics></math>
with loadings and weights so that covariance between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
along
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mi>a</mi></msub><annotation encoding="application/x-tex">t_a</annotation></semantics></math>
is maximized, with orthogonality constraints across components.</p>
<p>For kernel methods, let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œï</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚ãÖ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\phi(\cdot)</annotation></semantics></math>
be an implicit feature map and define the Gram matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>X</mi></msub><mo>=</mo><msub><mi>Œ¶</mi><mi>X</mi></msub><msubsup><mi>Œ¶</mi><mi>X</mi><mi>‚ä§</mi></msubsup></mrow><annotation encoding="application/x-tex">K_X = \Phi_X \Phi_X^\top</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>K</mi><mi>X</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(K_X)_{ij} = k(x_i, x_j)</annotation></semantics></math>.
The centering operator
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><msub><mi>I</mi><mi>n</mi></msub><mo>‚àí</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mn>ùüè</mn><msup><mn>ùüè</mn><mi>‚ä§</mi></msup></mrow><annotation encoding="application/x-tex">H = I_n - \frac{1}{n}\mathbf{1}\mathbf{1}^\top</annotation></semantics></math>
yields a centered Gram
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>X</mi></msub><mo>=</mo><mi>H</mi><msub><mi>K</mi><mi>X</mi></msub><mi>H</mi></mrow><annotation encoding="application/x-tex">\tilde K_X = H K_X H</annotation></semantics></math>.</p>
</div>
<div class="section level2">
<h2 id="pseudo-code-for-bigplsr-algorithms">Pseudo-code for bigPLSR algorithms<a class="anchor" aria-label="anchor" href="#pseudo-code-for-bigplsr-algorithms"></a>
</h2>
<p>The package implements several complementary extraction schemes. The
following pseudo-code summarises the core loops.</p>
<div class="section level3">
<h3 id="simpls-densebigmem">SIMPLS (dense/bigmem)<a class="anchor" aria-label="anchor" href="#simpls-densebigmem"></a>
</h3>
<ol style="list-style-type: decimal">
<li>Compute centered cross-products
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub><mo>=</mo><msup><mi>X</mi><mi>‚ä§</mi></msup><mi>X</mi></mrow><annotation encoding="application/x-tex">C_{xx} = X^\top X</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>x</mi><mi>y</mi></mrow></msub><mo>=</mo><msup><mi>X</mi><mi>‚ä§</mi></msup><mi>Y</mi></mrow><annotation encoding="application/x-tex">C_{xy} = X^\top Y</annotation></semantics></math>.</li>
<li>Initialise orthonormal basis
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">V = []</annotation></semantics></math>.</li>
<li>For each component
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mn>1</mn><mi>.</mi><mi>.</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">a = 1..A</annotation></semantics></math>:
<ul>
<li>Deflate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mrow><mi>x</mi><mi>y</mi></mrow></msub><annotation encoding="application/x-tex">C_{xy}</annotation></semantics></math>
in the subspace spanned by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>.</li>
<li>Extract
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding="application/x-tex">q_a</annotation></semantics></math>
as the dominant eigenvector of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>C</mi><mrow><mi>x</mi><mi>y</mi></mrow><mi>‚ä§</mi></msubsup><msub><mi>C</mi><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{xy}^\top C_{xy}</annotation></semantics></math>.</li>
<li>Compute
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>a</mi></msub><mo>=</mo><msub><mi>C</mi><mrow><mi>x</mi><mi>y</mi></mrow></msub><msub><mi>q</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">w_a = C_{xy} q_a</annotation></semantics></math>
and normalise under the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">C_{xx}</annotation></semantics></math>-metric.</li>
<li>Obtain loadings
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>a</mi></msub><mo>=</mo><msub><mi>C</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub><msub><mi>w</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">p_a = C_{xx} w_a</annotation></semantics></math>
and regression weights
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>a</mi></msub><mo>=</mo><msubsup><mi>C</mi><mrow><mi>x</mi><mi>y</mi></mrow><mi>‚ä§</mi></msubsup><msub><mi>w</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">c_a = C_{xy}^\top w_a</annotation></semantics></math>.</li>
<li>Expand
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>‚Üê</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>V</mi><mo>,</mo><msub><mi>p</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">V \leftarrow [V, p_a]</annotation></semantics></math>.</li>
</ul>
</li>
<li>Form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>w</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">W = [w_a]</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>p</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">P = [p_a]</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>c</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Q = [c_a]</annotation></semantics></math>
and compute regression coefficients
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mi>W</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>P</mi><mi>‚ä§</mi></msup><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>‚àí</mi><mn>1</mn></mrow></msup><msup><mi>Q</mi><mi>‚ä§</mi></msup></mrow><annotation encoding="application/x-tex">B = W (P^\top W)^{-1} Q^\top</annotation></semantics></math>.</li>
</ol>
</div>
<div class="section level3">
<h3 id="nipals-densestreamed">NIPALS (dense/streamed)<a class="anchor" aria-label="anchor" href="#nipals-densestreamed"></a>
</h3>
<ol style="list-style-type: decimal">
<li>Initialise
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mi>a</mi></msub><annotation encoding="application/x-tex">t_a</annotation></semantics></math>
from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
(or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>).</li>
<li>Iterate until convergence:
<ul>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>a</mi></msub><mo>=</mo><msup><mi>X</mi><mi>‚ä§</mi></msup><msub><mi>t</mi><mi>a</mi></msub><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>t</mi><mi>a</mi><mi>‚ä§</mi></msubsup><msub><mi>t</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">w_a = X^\top t_a / (t_a^\top t_a)</annotation></semantics></math>,
normalise
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>a</mi></msub><annotation encoding="application/x-tex">w_a</annotation></semantics></math>.</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>a</mi></msub><mo>=</mo><mi>X</mi><msub><mi>w</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">t_a = X w_a</annotation></semantics></math>.</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>a</mi></msub><mo>=</mo><msup><mi>Y</mi><mi>‚ä§</mi></msup><msub><mi>t</mi><mi>a</mi></msub><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>t</mi><mi>a</mi><mi>‚ä§</mi></msubsup><msub><mi>t</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">c_a = Y^\top t_a / (t_a^\top t_a)</annotation></semantics></math>.</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mi>a</mi></msub><mo>=</mo><mi>Y</mi><msub><mi>c</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">u_a = Y c_a</annotation></semantics></math>
(for multi-response).</li>
</ul>
</li>
<li>Deflate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>‚Üê</mo><mi>X</mi><mo>‚àí</mo><msub><mi>t</mi><mi>a</mi></msub><msubsup><mi>p</mi><mi>a</mi><mi>‚ä§</mi></msubsup></mrow><annotation encoding="application/x-tex">X \leftarrow X - t_a p_a^\top</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>‚Üê</mo><mi>Y</mi><mo>‚àí</mo><msub><mi>t</mi><mi>a</mi></msub><msubsup><mi>q</mi><mi>a</mi><mi>‚ä§</mi></msubsup></mrow><annotation encoding="application/x-tex">Y \leftarrow Y - t_a q_a^\top</annotation></semantics></math>
and repeat for the next component.</li>
</ol>
</div>
<div class="section level3">
<h3 id="kernel-pls-rkhs-dense-streamed">Kernel PLS / RKHS (dense &amp; streamed)<a class="anchor" aria-label="anchor" href="#kernel-pls-rkhs-dense-streamed"></a>
</h3>
<ol style="list-style-type: decimal">
<li>Form (or stream) the centered Gram matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>X</mi></msub><annotation encoding="application/x-tex">\tilde K_X</annotation></semantics></math>.</li>
<li>At each iteration extract a dual weight
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ±</mi><mi>a</mi></msub><annotation encoding="application/x-tex">\alpha_a</annotation></semantics></math>
maximising covariance with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.</li>
<li>Obtain the score
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>a</mi></msub><mo>=</mo><msub><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>X</mi></msub><msub><mi>Œ±</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">t_a = \tilde K_X \alpha_a</annotation></semantics></math>,
regress
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mi>a</mi></msub><annotation encoding="application/x-tex">t_a</annotation></semantics></math>
to get
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding="application/x-tex">q_a</annotation></semantics></math>
and deflate in the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>X</mi></msub><annotation encoding="application/x-tex">\tilde K_X</annotation></semantics></math>
metric.</li>
<li>Accumulate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ±</mi><mi>a</mi></msub><annotation encoding="application/x-tex">\alpha_a</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding="application/x-tex">q_a</annotation></semantics></math>
and the orthonormal basis for subsequent deflation steps.</li>
</ol>
</div>
<div class="section level3">
<h3 id="double-rkhs-algorithm-rkhs_xy">Double RKHS ( <code>algorithm = "rkhs_xy"</code> )<a class="anchor" aria-label="anchor" href="#double-rkhs-algorithm-rkhs_xy"></a>
</h3>
<ol style="list-style-type: decimal">
<li>Build (or approximate) Gram matrices for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.</li>
<li>Extract dual directions
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ±</mi><mi>a</mi></msub><annotation encoding="application/x-tex">\alpha_a</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mi>a</mi></msub><annotation encoding="application/x-tex">\beta_a</annotation></semantics></math>
so that the score pair
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>t</mi><mi>a</mi></msub><mo>,</mo><msub><mi>u</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(t_a, u_a)</annotation></semantics></math>
maximises covariance under both kernels.</li>
<li>Use ridge-regularised projections to obtain regression weights.</li>
<li>Store kernel centering statistics for prediction.</li>
</ol>
</div>
<div class="section level3">
<h3 id="kalman-filter-pls-algorithm-kf_pls">Kalman-filter PLS (<code>algorithm = "kf_pls"</code>)<a class="anchor" aria-label="anchor" href="#kalman-filter-pls-algorithm-kf_pls"></a>
</h3>
<ol style="list-style-type: decimal">
<li>Maintain exponentially weighted means
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œº</mi><mi>x</mi></msub><mo>,</mo><msub><mi>Œº</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">\mu_x, \mu_y</annotation></semantics></math>.</li>
<li>Update cross-products
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub><mo>,</mo><msub><mi>C</mi><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{xx}, C_{xy}</annotation></semantics></math>
with forgetting factor
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
and optional process noise.</li>
<li>Periodically call SIMPLS on the smoothed moments to recover
regression coefficients consistent with the streamed state.</li>
</ol>
<p>Common kernels:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">Linear:</mtext><mspace width="1.0em"></mspace></mtd><mtd columnalign="left" style="text-align: left"><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>‚ä§</mi></msup><mi>z</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">RBF:</mtext><mspace width="1.0em"></mspace></mtd><mtd columnalign="left" style="text-align: left"><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚àí</mi><mi>Œ≥</mi><mo stretchy="false" form="postfix">‚à•</mo><mi>x</mi><mo>‚àí</mo><mi>z</mi><msup><mo stretchy="false" form="postfix">‚à•</mo><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">Polynomial:</mtext><mspace width="1.0em"></mspace></mtd><mtd columnalign="left" style="text-align: left"><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≥</mi><mspace width="0.167em"></mspace><msup><mi>x</mi><mi>‚ä§</mi></msup><mi>z</mi><mo>+</mo><msub><mi>c</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">Sigmoid:</mtext><mspace width="1.0em"></mspace></mtd><mtd columnalign="left" style="text-align: left"><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>tanh</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≥</mi><mspace width="0.167em"></mspace><msup><mi>x</mi><mi>‚ä§</mi></msup><mi>z</mi><mo>+</mo><msub><mi>c</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\text{Linear:}\quad&amp; k(x,z) = x^\top z \\
\text{RBF:}\quad&amp; k(x,z) = \exp(-\gamma \|x-z\|^2) \\
\text{Polynomial:}\quad&amp; k(x,z) = (\gamma\,x^\top z + c_0)^{d} \\
\text{Sigmoid:}\quad&amp; k(x,z) = \tanh(\gamma\,x^\top z + c_0).
\end{aligned}
</annotation></semantics></math></p>
</div>
<div class="section level3">
<h3 id="centering-the-gram-matrix">Centering the Gram matrix<a class="anchor" aria-label="anchor" href="#centering-the-gram-matrix"></a>
</h3>
<p>Given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>n</mi><mo>√ó</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">K\in\mathbb{R}^{n\times n}</annotation></semantics></math>,
the centered version is:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mo>=</mo><mi>H</mi><mi>K</mi><mi>H</mi><mo>,</mo><mspace width="1.0em"></mspace><mi>H</mi><mo>=</mo><msub><mi>I</mi><mi>n</mi></msub><mo>‚àí</mo><mfrac displaystyle="false"><mn>1</mn><mi>n</mi></mfrac><mn>ùüè</mn><msup><mn>ùüè</mn><mi>‚ä§</mi></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
\tilde K = H K H, \quad H = I_n - \tfrac{1}{n}\mathbf{1}\mathbf{1}^\top.
</annotation></semantics></math></p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="klpls-kernel-pls-dayal-macgregor">KLPLS / Kernel PLS (Dayal &amp; MacGregor)<a class="anchor" aria-label="anchor" href="#klpls-kernel-pls-dayal-macgregor"></a>
</h2>
<p>We operate in the dual. Consider
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mi>X</mi></msub><annotation encoding="application/x-tex">K_X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo>=</mo><msub><mi>K</mi><mi>X</mi></msub><mi>Y</mi></mrow><annotation encoding="application/x-tex">K_{XY} = K_X Y</annotation></semantics></math>.
At step
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>,
we extract a dual direction
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ±</mi><mi>a</mi></msub><annotation encoding="application/x-tex">\alpha_a</annotation></semantics></math>
so that the score
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>a</mi></msub><mo>=</mo><msub><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>X</mi></msub><msub><mi>Œ±</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">t_a = \tilde K_X \alpha_a</annotation></semantics></math>
maximizes covariance with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>,
subject to orthogonality in the RKHS metric:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mo>max</mo><mi>Œ±</mi></munder><mspace width="0.222em"></mspace><mrow><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">v</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>,</mo><mi>Y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="1.0em"></mspace><mtext mathvariant="normal">s.t.</mtext><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mi>t</mi><mo>=</mo><msub><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>X</mi></msub><mi>Œ±</mi><mo>,</mo><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><msup><mi>t</mi><mi>‚ä§</mi></msup><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><msup><mi>t</mi><mi>‚ä§</mi></msup><msub><mi>t</mi><mi>b</mi></msub><mo>=</mo><mn>0</mn><mo>,</mo><mspace width="0.222em"></mspace><mi>b</mi><mo>&lt;</mo><mi>a</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
\max_{\alpha} \ \mathrm{cov}(t, Y) \quad
\text{s.t.}\ \ t=\tilde K_X \alpha,\ \ t^\top t = 1,\ \ t^\top t_b = 0,\ b&lt;a.
</annotation></semantics></math></p>
<p>A SIMPLS-style recursion in the dual:</p>
<ol style="list-style-type: decimal">
<li>Compute the cross-covariance operator
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><msub><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>X</mi></msub><mi>Y</mi></mrow><annotation encoding="application/x-tex">C = \tilde K_X Y</annotation></semantics></math>.</li>
<li>Extract a direction in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>‚Ñù</mi><mi>n</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^n</annotation></semantics></math>
via dominant eigenvector of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><msup><mi>C</mi><mi>‚ä§</mi></msup></mrow><annotation encoding="application/x-tex">C C^\top</annotation></semantics></math>
or by power iterations.</li>
<li>Set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>a</mi></msub><mo>=</mo><msub><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>X</mi></msub><msub><mi>Œ±</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">t_a = \tilde K_X \alpha_a</annotation></semantics></math>,
normalize
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mi>a</mi></msub><annotation encoding="application/x-tex">t_a</annotation></semantics></math>.</li>
<li>Regress
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mi>a</mi></msub><annotation encoding="application/x-tex">t_a</annotation></semantics></math>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mi>a</mi></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>t</mi><mi>a</mi><mi>‚ä§</mi></msubsup><msub><mi>t</mi><mi>a</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>‚àí</mi><mn>1</mn></mrow></msup><msubsup><mi>t</mi><mi>a</mi><mi>‚ä§</mi></msubsup><mi>Y</mi></mrow><annotation encoding="application/x-tex">q_a = (t_a^\top t_a)^{-1} t_a^\top Y</annotation></semantics></math>.</li>
<li>Deflate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>‚Üê</mo><mi>Y</mi><mo>‚àí</mo><msub><mi>t</mi><mi>a</mi></msub><msubsup><mi>q</mi><mi>a</mi><mi>‚ä§</mi></msubsup></mrow><annotation encoding="application/x-tex">Y \leftarrow Y - t_a q_a^\top</annotation></semantics></math>
and orthogonalize subsequent directions in the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>X</mi></msub><annotation encoding="application/x-tex">\tilde K_X</annotation></semantics></math>-metric.</li>
</ol>
<p>Prediction uses the dual coefficients; for a new
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mo>‚ãÜ</mo></msub><annotation encoding="application/x-tex">x_\star</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mo>‚ãÜ</mo></msub><mo>=</mo><msubsup><mrow><mo stretchy="true" form="prefix">[</mo><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mo>‚ãÜ</mo></msub><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">k_\star = [k(x_\star, x_i)]_{i=1}^n</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mo>‚ãÜ</mo></msub><mo>=</mo><msubsup><mover><mi>k</mi><mo accent="true">ÃÉ</mo></mover><mo>‚ãÜ</mo><mi>‚ä§</mi></msubsup><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">t_\star = \tilde k_\star^\top \alpha</annotation></semantics></math>.
When
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
is multivariate, apply steps component-wise with a shared
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mi>a</mi></msub><annotation encoding="application/x-tex">t_a</annotation></semantics></math>.</p>
<p><strong>In bigPLSR</strong><br>
- Dense path: <code>algorithm="rkhs"</code> builds
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>X</mi></msub><annotation encoding="application/x-tex">\tilde K_X</annotation></semantics></math>
(or an approximation) and runs dual SIMPLS deflation.<br>
- Big-matrix path: block-streamed Gram computations to avoid
materializing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>√ó</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n\times n</annotation></semantics></math>.</p>
<hr>
</div>
<div class="section level2">
<h2 id="streaming-gram-blocks-column--and-row-chunked">Streaming Gram blocks (column- and row-chunked)<a class="anchor" aria-label="anchor" href="#streaming-gram-blocks-column--and-row-chunked"></a>
</h2>
<p>We avoid forming
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>X</mi></msub><annotation encoding="application/x-tex">\tilde K_X</annotation></semantics></math>
explicitly by accumulating blocks. Write
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>X</mi></msub><mo>=</mo><msub><mo>‚àë</mo><mi>B</mi></msub><msub><mi>X</mi><mi>B</mi></msub><msup><mi>X</mi><mi>‚ä§</mi></msup></mrow><annotation encoding="application/x-tex">K_X = \sum_{B} X_B X^\top</annotation></semantics></math>
for blocks
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>B</mi></msub><annotation encoding="application/x-tex">X_B</annotation></semantics></math>
taken by rows (<em>row-chunked/XX·µó</em>) or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>X</mi></msub><mo>=</mo><mi>X</mi><msup><mi>X</mi><mi>‚ä§</mi></msup><mo>=</mo><msub><mo>‚àë</mo><mi>C</mi></msub><mi>X</mi><msup><mi>C</mi><mi>‚ä§</mi></msup></mrow><annotation encoding="application/x-tex">K_X = X X^\top = \sum_{C} X C^\top</annotation></semantics></math>
with <em>column</em> chunks via
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><msup><mi>C</mi><mi>‚ä§</mi></msup></mrow><annotation encoding="application/x-tex">X C^\top</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>
are column submatrices (useful for tall-skinny
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>).</p>
<p><strong>Row-chunked (XX·µó):</strong> 1. For blocks
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>‚äÇ</mo><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mi>n</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">B \subset \{1,\ldots,n\}</annotation></semantics></math>:
compute
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>B</mi></msub><mo>=</mo><msub><mi>X</mi><mi>B</mi></msub><msup><mi>X</mi><mi>‚ä§</mi></msup></mrow><annotation encoding="application/x-tex">G_B = X_B X^\top</annotation></semantics></math>.
2. Accumulate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>‚Üê</mo><mi>K</mi><mo>+</mo><mi>H</mi><msub><mi>G</mi><mi>B</mi></msub><mi>H</mi></mrow><annotation encoding="application/x-tex">K \leftarrow K + H G_B H</annotation></semantics></math>
on the fly when needed in matrix-vector products
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>K</mi><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(K v)</annotation></semantics></math>
without storing full
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>.</p>
<p><strong>Column-chunked:</strong> 1. Partition the feature dimension
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
into blocks
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math>.
2. For each block
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>J</mi></msub><mo>=</mo><msub><mi>X</mi><mrow><mo stretchy="true" form="prefix">[</mo><mo>:</mo><mo>,</mo><mi>J</mi><mo stretchy="true" form="postfix">]</mo></mrow></msub><msubsup><mi>X</mi><mrow><mo stretchy="true" form="prefix">[</mo><mo>:</mo><mo>,</mo><mi>J</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>‚ä§</mi></msubsup></mrow><annotation encoding="application/x-tex">G_J = X_{[:,J]} X_{[:,J]}^\top</annotation></semantics></math>.
3. Use
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>G</mi><mi>J</mi></msub><annotation encoding="application/x-tex">G_J</annotation></semantics></math>
to update
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">K v</annotation></semantics></math>
accumulators and to refresh deflation quantities
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>,</mo><mi>q</mi></mrow><annotation encoding="application/x-tex">t, q</annotation></semantics></math>).</p>
<p><strong>Memory</strong><br>
- Row-chunked:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚ãÖ</mo><mtext mathvariant="normal">chunk_rows</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n \cdot \text{chunk\_rows})</annotation></semantics></math>.<br>
- Column-chunked:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚ãÖ</mo><mtext mathvariant="normal">chunk_cols</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n \cdot \text{chunk\_cols})</annotation></semantics></math>.<br>
Pick based on layout and cache friendliness.</p>
<hr>
</div>
<div class="section level2">
<h2 id="kernel-approximations-nystr√∂m-and-random-fourier-features">Kernel approximations: Nystr√∂m and Random Fourier Features<a class="anchor" aria-label="anchor" href="#kernel-approximations-nystr%C3%B6m-and-random-fourier-features"></a>
</h2>
<p><strong>Nystr√∂m (rank
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>)</strong><br>
Sample a subset
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>,
form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mrow><mi>S</mi><mi>S</mi></mrow></msub><annotation encoding="application/x-tex">K_{SS}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mrow><mi>N</mi><mi>S</mi></mrow></msub><annotation encoding="application/x-tex">K_{NS}</annotation></semantics></math>.
Define the sketch
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi><mo>=</mo><msub><mi>K</mi><mrow><mi>N</mi><mi>S</mi></mrow></msub><msubsup><mi>K</mi><mrow><mi>S</mi><mi>S</mi></mrow><mrow><mi>‚àí</mi><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">Z = K_{NS} K_{SS}^{-1/2}</annotation></semantics></math>,
so
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>‚âà</mo><mi>Z</mi><msup><mi>Z</mi><mi>‚ä§</mi></msup></mrow><annotation encoding="application/x-tex">K \approx Z Z^\top</annotation></semantics></math>.
Center
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>
by subtracting row/column means. Run linear PLS on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>.</p>
<p><strong>RFF (RBF kernels)</strong><br>
Draw
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>œâ</mi><mo>‚Ñì</mo></msub><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mo>‚Ñì</mo><mo>=</mo><mn>1</mn></mrow><mi>r</mi></msubsup><mo>‚àº</mo><mi>ùí©</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mi>Œ≥</mi><mi>I</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\{\omega_\ell\}_{\ell=1}^r \sim \mathcal{N}(0,2\gamma I)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mo>‚Ñì</mo></msub><mo>‚àº</mo><mi>ùí∞</mi><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>2</mn><mi>œÄ</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">b_\ell\sim \mathcal{U}[0,2\pi]</annotation></semantics></math>.
Define features
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œÜ</mi><mo>‚Ñì</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msqrt><mfrac displaystyle="false"><mn>2</mn><mi>r</mi></mfrac></msqrt><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>œâ</mi><mo>‚Ñì</mo><mi>‚ä§</mi></msubsup><mi>x</mi><mo>+</mo><msub><mi>b</mi><mo>‚Ñì</mo></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\varphi_\ell(x)=\sqrt{\tfrac{2}{r}}\cos(\omega_\ell^\top x + b_\ell)</annotation></semantics></math>,
so
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚âà</mo><mi>œÜ</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>‚ä§</mi></msup><mi>œÜ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">k(x,z)\approx \varphi(x)^\top \varphi(z)</annotation></semantics></math>.
Run linear PLS on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÜ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\varphi(X)</annotation></semantics></math>.</p>
<hr>
</div>
<div class="section level2">
<h2 id="kernel-logistic-pls-binary-classification">Kernel Logistic PLS (binary classification)<a class="anchor" aria-label="anchor" href="#kernel-logistic-pls-binary-classification"></a>
</h2>
<p>We first compute KPLS scores
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>n</mi><mo>√ó</mo><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">T\in\mathbb{R}^{n\times a}</annotation></semantics></math>
from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
vs labels
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>‚àà</mo><mo stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">y\in\{0,1\}</annotation></semantics></math>,
then run logistic regression in latent space via IRLS:</p>
<p>Minimize
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Ñì</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ≤</mi><mo>,</mo><msub><mi>Œ≤</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>‚àí</mi><msub><mo>‚àë</mo><mi>i</mi></msub><mo stretchy="false" form="prefix">{</mo><msub><mi>y</mi><mi>i</mi></msub><msub><mi>Œ∑</mi><mi>i</mi></msub><mo>‚àí</mo><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mo>exp</mo><msub><mi>Œ∑</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\ell(\beta, \beta_0) = -\sum_i \{ y_i\eta_i - \log(1+\exp\eta_i)\}</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ∑</mi><mo>=</mo><msub><mi>Œ≤</mi><mn>0</mn></msub><mn>ùüè</mn><mo>+</mo><mi>T</mi><mi>Œ≤</mi></mrow><annotation encoding="application/x-tex">\eta = \beta_0\mathbf{1} + T \beta</annotation></semantics></math>.
IRLS step at iteration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msup><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mspace width="1.0em"></mspace><mi>z</mi><mo>=</mo><msup><mi>Œ∑</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>W</mi><mrow><mi>‚àí</mi><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>‚àí</mo><msup><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mspace width="1.0em"></mspace><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>Œ≤</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>Œ≤</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>X</mi><mi>Œ∑</mi><mi>‚ä§</mi></msubsup><mi>W</mi><msub><mi>X</mi><mi>Œ∑</mi></msub><mo>+</mo><mi>Œª</mi><mi>I</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>‚àí</mi><mn>1</mn></mrow></msup><msubsup><mi>X</mi><mi>Œ∑</mi><mi>‚ä§</mi></msubsup><mi>W</mi><mi>z</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">
W = \mathrm{diag}(p^{(k)}(1-p^{(k)})),\quad
z = \eta^{(k)} + W^{-1}(y - p^{(k)}),\quad
\begin{bmatrix}\beta_0\\ \beta\end{bmatrix}
= (X_\eta^\top W X_\eta + \lambda I)^{-1} X_\eta^\top W z,
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>Œ∑</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>ùüè</mn><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">X_\eta = [\mathbf{1}, T]</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>œÉ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∑</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p^{(k)} = \sigma(\eta^{(k)})</annotation></semantics></math>.
Optionally <strong>alternate</strong>: recompute KPLS scores with
current residuals and re-run a few IRLS steps. Class weights
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>c</mi></msub><annotation encoding="application/x-tex">w_c</annotation></semantics></math>
can be injected by scaling rows of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math>.</p>
<p><strong>In bigPLSR</strong><br><code>algorithm="klogitpls"</code> computes
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>
(dense or streamed) then fits IRLS in latent space.</p>
<hr>
</div>
<div class="section level2">
<h2 id="sparse-kernel-pls-sketch">Sparse Kernel PLS (sketch)<a class="anchor" aria-label="anchor" href="#sparse-kernel-pls-sketch"></a>
</h2>
<p>Promote sparsity in dual or primal weights. In dual form, constrain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ±</mi><mi>a</mi></msub><annotation encoding="application/x-tex">\alpha_a</annotation></semantics></math>
by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mo>‚Ñì</mo><mn>1</mn></msub><annotation encoding="application/x-tex">\ell_1</annotation></semantics></math>
(or group) penalty:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mo>max</mo><mi>Œ±</mi></munder><mspace width="0.222em"></mspace><mrow><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">v</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>Œ±</mi><mo>,</mo><mi>Y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àí</mo><mi>Œª</mi><mo stretchy="false" form="postfix">‚à•</mo><mi>Œ±</mi><msub><mo stretchy="false" form="postfix">‚à•</mo><mn>1</mn></msub><mspace width="1.0em"></mspace><mtext mathvariant="normal">s.t.</mtext><mspace width="1.0em"></mspace><msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>Œ±</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>‚ä§</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><mi>Œ±</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>,</mo><mspace width="0.222em"></mspace><msubsup><mi>t</mi><mi>a</mi><mi>‚ä§</mi></msubsup><msub><mi>t</mi><mi>b</mi></msub><mo>=</mo><mn>0</mn><mspace width="0.222em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mi>b</mi><mo>&lt;</mo><mi>a</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
\max_{\alpha}\ \mathrm{cov}(\tilde K \alpha, Y) - \lambda\|\alpha\|_1
\quad\text{s.t.}\quad (\tilde K\alpha)^\top (\tilde K\alpha) = 1,\ t_a^\top t_b=0\ (b&lt;a).
</annotation></semantics></math></p>
<p>A practical approach uses <em>proximal gradient</em> or
<em>coordinate descent</em> on a smooth surrogate of covariance, with
periodic orthogonalization of the resulting score vectors in the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>K</mi><mo accent="true">ÃÉ</mo></mover><annotation encoding="application/x-tex">\tilde K</annotation></semantics></math>
metric. Early stop by explained covariance. (The current release
provides the scaffolding API.)</p>
<hr>
</div>
<div class="section level2">
<h2 id="pls-in-rkhs-for-x-and-y-double-rkhs">PLS in RKHS for X and Y (double RKHS)<a class="anchor" aria-label="anchor" href="#pls-in-rkhs-for-x-and-y-double-rkhs"></a>
</h2>
<p>Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mi>X</mi></msub><annotation encoding="application/x-tex">K_X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mi>Y</mi></msub><annotation encoding="application/x-tex">K_Y</annotation></semantics></math>
be centered Grams for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
(with small ridge
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œª</mi><mi>X</mi></msub><mo>,</mo><msub><mi>Œª</mi><mi>Y</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_X,\lambda_Y</annotation></semantics></math>
for stability). The cross-covariance operator is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><msub><mi>K</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>K</mi><mi>Y</mi></msub><mo>+</mo><msub><mi>Œª</mi><mi>Y</mi></msub><mi>I</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>K</mi><mi>X</mi></msub></mrow><annotation encoding="application/x-tex">A = K_X (K_Y + \lambda_Y I) K_X</annotation></semantics></math>.
Dual SIMPLS extracts latent directions via the dominant eigenspace of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
with orthogonalization under the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mi>X</mi></msub><annotation encoding="application/x-tex">K_X</annotation></semantics></math>
inner product.</p>
<p>Prediction returns dual coefficients
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.</p>
<p><strong>In bigPLSR</strong><br><code>algorithm="rkhs_xy"</code> wires this in dense mode; a streamed
variant can be built by block Gram accumulations on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mi>X</mi></msub><annotation encoding="application/x-tex">K_X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mi>Y</mi></msub><annotation encoding="application/x-tex">K_Y</annotation></semantics></math>.</p>
<hr>
</div>
<div class="section level2">
<h2 id="kalman-filter-pls-kf-pls-streaming">Kalman-Filter PLS (KF-PLS; streaming)<a class="anchor" aria-label="anchor" href="#kalman-filter-pls-kf-pls-streaming"></a>
</h2>
<p>KF-PLS maintains a state that tracks latent parameters over incoming
mini-batches. Let the state be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mi>w</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo>,</mo><mi>b</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">s = \{w, p, q, b\}</annotation></semantics></math>
for the current component, with state transition
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>s</mi><mi>k</mi></msub><mo>+</mo><msub><mi>œµ</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">s_{k+1} = s_k + \epsilon_k</annotation></semantics></math>
(random walk) and ‚Äúmeasurement‚Äù formed from the current block
cross-covariances
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mrow><msup><mi>X</mi><mi>‚ä§</mi></msup><mi>t</mi></mrow><mo accent="true">ÃÇ</mo></mover><mo>,</mo><mover><mrow><msup><mi>Y</mi><mi>‚ä§</mi></msup><mi>t</mi></mrow><mo accent="true">ÃÇ</mo></mover></mrow><annotation encoding="application/x-tex">\widehat{X^\top t}, \widehat{Y^\top t}</annotation></semantics></math>).
The Kalman update:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">Predict: </mtext><mspace width="0.333em"></mspace></mrow><msub><mover><mi>s</mi><mo accent="true">ÃÇ</mo></mover><mrow><mi>k</mi><mo stretchy="false" form="prefix">|</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover><mi>s</mi><mo accent="true">ÃÇ</mo></mover><mrow><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>,</mo><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><msub><mi>P</mi><mrow><mi>k</mi><mo stretchy="false" form="prefix">|</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>P</mi><mrow><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>Q</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">Innovation: </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>ŒΩ</mi><mi>k</mi></msub><mo>=</mo><msub><mi>z</mi><mi>k</mi></msub><mo>‚àí</mo><msub><mi>H</mi><mi>k</mi></msub><msub><mover><mi>s</mi><mo accent="true">ÃÇ</mo></mover><mrow><mi>k</mi><mo stretchy="false" form="prefix">|</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>,</mo><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><msub><mi>S</mi><mi>k</mi></msub><mo>=</mo><msub><mi>H</mi><mi>k</mi></msub><msub><mi>P</mi><mrow><mi>k</mi><mo stretchy="false" form="prefix">|</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><msubsup><mi>H</mi><mi>k</mi><mi>‚ä§</mi></msubsup><mo>+</mo><mi>R</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">Gain: </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>K</mi><mi>k</mi></msub><mo>=</mo><msub><mi>P</mi><mrow><mi>k</mi><mo stretchy="false" form="prefix">|</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><msubsup><mi>H</mi><mi>k</mi><mi>‚ä§</mi></msubsup><msubsup><mi>S</mi><mi>k</mi><mrow><mi>‚àí</mi><mn>1</mn></mrow></msubsup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">Update: </mtext><mspace width="0.333em"></mspace></mrow><msub><mover><mi>s</mi><mo accent="true">ÃÇ</mo></mover><mi>k</mi></msub><mo>=</mo><msub><mover><mi>s</mi><mo accent="true">ÃÇ</mo></mover><mrow><mi>k</mi><mo stretchy="false" form="prefix">|</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>K</mi><mi>k</mi></msub><msub><mi>ŒΩ</mi><mi>k</mi></msub><mo>,</mo><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><msub><mi>P</mi><mi>k</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>‚àí</mo><msub><mi>K</mi><mi>k</mi></msub><msub><mi>H</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>P</mi><mrow><mi>k</mi><mo stretchy="false" form="prefix">|</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
&amp;\text{Predict: } \hat s_{k|k-1} = \hat s_{k-1},\ \ P_{k|k-1}=P_{k-1}+Q \\
&amp;\text{Innovation: } \nu_k = z_k - H_k \hat s_{k|k-1},\ \ S_k = H_k P_{k|k-1} H_k^\top + R \\
&amp;\text{Gain: } K_k = P_{k|k-1} H_k^\top S_k^{-1} \\
&amp;\text{Update: } \hat s_k = \hat s_{k|k-1} + K_k \nu_k,\ \ P_k = (I - K_k H_k) P_{k|k-1}.
\end{aligned}
</annotation></semantics></math></p>
<p>After convergence (or patience stop), form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mi>X</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">t = X w</annotation></semantics></math>,
normalize, and proceed to next component with deflation compatible with
SIMPLS/NIPALS choice.</p>
<p><strong>In bigPLSR</strong><br><code>algorithm="kf_pls"</code> reuses the existing <em>chunked T
streaming</em> kernel and updates the state per block.</p>
<hr>
</div>
<div class="section level2">
<h2 id="api-quick-start">API quick start<a class="anchor" aria-label="anchor" href="#api-quick-start"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://fbertran.github.io/bigPLSR/">bigPLSR</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Dense RKHS PLS with Nystr√∂m of rank 500 (rbf kernel)</span></span>
<span><span class="va">fit_rkhs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/pls_fit.html">pls_fit</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">Y</span>, ncomp <span class="op">=</span> <span class="fl">5</span>,</span>
<span>                    backend   <span class="op">=</span> <span class="st">"arma"</span>,</span>
<span>                    algorithm <span class="op">=</span> <span class="st">"rkhs"</span>,</span>
<span>                    kernel <span class="op">=</span> <span class="st">"rbf"</span>, gamma <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>                    approx <span class="op">=</span> <span class="st">"nystrom"</span>, approx_rank <span class="op">=</span> <span class="fl">500</span>,</span>
<span>                    scores <span class="op">=</span> <span class="st">"r"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Bigmemory, kernel logistic PLS (streamed scores + IRLS)</span></span>
<span><span class="va">fit_klog</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/pls_fit.html">pls_fit</a></span><span class="op">(</span><span class="va">bmX</span>, <span class="va">bmy</span>, ncomp <span class="op">=</span> <span class="fl">4</span>,</span>
<span>                    backend   <span class="op">=</span> <span class="st">"bigmem"</span>,</span>
<span>                    algorithm <span class="op">=</span> <span class="st">"klogitpls"</span>,</span>
<span>                    kernel <span class="op">=</span> <span class="st">"rbf"</span>, gamma <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span>                    chunk_size <span class="op">=</span> <span class="fl">16384L</span>,</span>
<span>                    scores <span class="op">=</span> <span class="st">"r"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Sparse KPLS (dense scaffold)</span></span>
<span><span class="va">fit_sk</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/pls_fit.html">pls_fit</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">Y</span>, ncomp <span class="op">=</span> <span class="fl">5</span>,</span>
<span>                  backend <span class="op">=</span> <span class="st">"arma"</span>,</span>
<span>                  algorithm <span class="op">=</span> <span class="st">"sparse_kpls"</span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="prediction-in-rkhs-pls">Prediction in RKHS PLS<a class="anchor" aria-label="anchor" href="#prediction-in-rkhs-pls"></a>
</h3>
<p>Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>n</mi><mo>√ó</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X\in\mathbb{R}^{n\times p}</annotation></semantics></math>
be training inputs and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>n</mi><mo>√ó</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">Y\in\mathbb{R}^{n\times m}</annotation></semantics></math>
the responses. With kernel
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>‚ãÖ</mi><mo>,</mo><mi>‚ãÖ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">k(\cdot,\cdot)</annotation></semantics></math>
and training Gram
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>,
the <strong>centered</strong> Gram is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>c</mi></msub><mo>=</mo><mi>K</mi><mo>‚àí</mo><msub><mn>ùüè</mn><mi>n</mi></msub><msup><mover><mi>k</mi><mo accent="true">‚Äæ</mo></mover><mi>‚ä§</mi></msup><mo>‚àí</mo><mover><mi>k</mi><mo accent="true">‚Äæ</mo></mover><mspace width="0.167em"></mspace><msubsup><mn>ùüè</mn><mi>n</mi><mi>‚ä§</mi></msubsup><mo>+</mo><mover><mover><mi>k</mi><mo accent="true">‚Äæ</mo></mover><mo accent="true">‚Äæ</mo></mover><mo>,</mo><mspace width="1.0em"></mspace><mover><mi>k</mi><mo accent="true">‚Äæ</mo></mover><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>K</mi><mrow><mi>i</mi><mo>‚ãÖ</mo></mrow></msub><mo>,</mo><mspace width="1.0em"></mspace><mover><mover><mi>k</mi><mo accent="true">‚Äæ</mo></mover><mo accent="true">‚Äæ</mo></mover><mo>=</mo><mfrac><mn>1</mn><msup><mi>n</mi><mn>2</mn></msup></mfrac><munder><mo>‚àë</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><msub><mi>K</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">
K_c = K - \mathbf{1}_n \bar{k}^\top - \bar{k}\,\mathbf{1}_n^\top + \bar{\bar{k}},
\quad
\bar{k} = \frac{1}{n}\sum_{i=1}^n K_{i\cdot},\quad
\bar{\bar{k}} = \frac{1}{n^2}\sum_{i,j}K_{ij}.
</annotation></semantics></math> KPLS on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>K</mi><mi>c</mi></msub><annotation encoding="application/x-tex">K_c</annotation></semantics></math>
yields <strong>dual coefficients</strong>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>n</mi><mo>√ó</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A\in\mathbb{R}^{n\times m}</annotation></semantics></math>.</p>
<p>For new inputs <span class="math inline">$X_\*$</span>, the
cross-kernel <span class="math inline">$K_\* \in \mathbb{R}^{n_\*\times
n}$</span> has <span class="math inline">$(K_\*)_{i,j} = k(x^\*_i,
x_j)$</span>. The <strong>centered</strong> cross-Gram is <span class="math display">$$
K_{\*,c} = K_\* - \mathbf{1}_{n_\*}\bar{k}^\top -
\bar{k}_\*\mathbf{1}_n^\top + \bar{\bar{k}},
\quad
\bar{k}_\* = \frac{1}{n}K_\*\mathbf{1}_n.
$$</span> Predictions follow <span class="math display">$$
\widehat{Y}_\* = K_{\*,c}\,A + \mathbf{1}_{n_\*}\,\mu_Y^\top,
$$</span> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œº</mi><mi>Y</mi></msub><annotation encoding="application/x-tex">\mu_Y</annotation></semantics></math>
is the vector of training response means.</p>
<p>In <code>bigPLSR</code>, these are stored as: <code>dual_coef</code>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>),
<code>k_colmeans</code>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>k</mi><mo accent="true">‚Äæ</mo></mover><annotation encoding="application/x-tex">\bar{k}</annotation></semantics></math>),
<code>k_mean</code>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mover><mi>k</mi><mo accent="true">‚Äæ</mo></mover><mo accent="true">‚Äæ</mo></mover><annotation encoding="application/x-tex">\bar{\bar{k}}</annotation></semantics></math>),
<code>y_means</code>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œº</mi><mi>Y</mi></msub><annotation encoding="application/x-tex">\mu_Y</annotation></semantics></math>),
and <code>X_ref</code> (dense training inputs). The RKHS branch of
<code><a href="../reference/predict.big_plsr.html">predict.big_plsr()</a></code> uses the same formula.</p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="dependency-overview-wrappers-c-entry-points">Dependency overview (wrappers ‚Üí C++ entry points)<a class="anchor" aria-label="anchor" href="#dependency-overview-wrappers-c-entry-points"></a>
</h2>
<pre><code>pls_fit(algorithm="simpls", backend="arma")
  ‚îî‚îÄ .Call("_bigPLSR_cpp_simpls_from_cross")

pls_fit(algorithm="simpls", backend="bigmem")
  ‚îú‚îÄ .Call("_bigPLSR_cpp_bigmem_cross")
  ‚îú‚îÄ .Call("_bigPLSR_cpp_simpls_from_cross")
  ‚îî‚îÄ .Call("_bigPLSR_cpp_stream_scores_given_W")

pls_fit(algorithm="nipals", backend="arma")
  ‚îî‚îÄ cpp_dense_plsr_nipals()

pls_fit(algorithm="nipals", backend="bigmem")
  ‚îî‚îÄ big_plsr_stream_fit_nipals()

pls_fit(algorithm="kernelpls"/"widekernelpls")
  ‚îî‚îÄ .kernel_pls_core()  (R)

pls_fit(algorithm="rkhs", backend="arma")
  ‚îî‚îÄ .Call("_bigPLSR_cpp_kpls_rkhs_dense")

pls_fit(algorithm="rkhs", backend="bigmem")
  ‚îî‚îÄ .Call("_bigPLSR_cpp_kpls_rkhs_bigmem")

pls_fit(algorithm="klogitpls", backend="arma")
  ‚îî‚îÄ .Call("_bigPLSR_cpp_klogit_pls_dense")

pls_fit(algorithm="klogitpls", backend="bigmem")
  ‚îî‚îÄ .Call("_bigPLSR_cpp_klogit_pls_bigmem")

pls_fit(algorithm="sparse_kpls")
  ‚îî‚îÄ .Call("_bigPLSR_cpp_sparse_kpls_dense")

pls_fit(algorithm="rkhs_xy")
  ‚îî‚îÄ .Call("_bigPLSR_cpp_rkhs_xy_dense")

pls_fit(algorithm="kf_pls")
  ‚îî‚îÄ .Call("_bigPLSR_cpp_kf_pls_stream")</code></pre>
<hr>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<ul>
<li>Dayal, B., &amp; MacGregor, J.F. (1997). Improved PLS algorithms.
<em>Journal of Chemometrics</em>, <strong>11</strong>(1), 73‚Äì85, <a href="doi:10.1002/(SICI)1099-128X(199701)11:1%3C73::AID-CEM446%3E3.0.CO;2-2" class="uri">doi:10.1002/(SICI)1099-128X(199701)11:1%3C73::AID-CEM446%3E3.0.CO;2-2</a>.</li>
<li>Rosipal, R., &amp; Trejo, L.J. (2001). Kernel PLS regression in
RKHS. <em>Journal of Machine Learning Research</em>, <strong>2</strong>,
97‚Äì123, <a href="doi:10.1162/153244302760200687" class="uri">doi:10.1162/153244302760200687</a>.</li>
<li>Tenenhaus et al., Kernel Logistic PLS.</li>
<li>Sparse Kernel Partial Least Squares Regression. In <em>LNCS</em>
Proceedings.</li>
<li>Rosipal et al., RKHS PLS (JMLR) <a href="http://www.jmlr.org/papers/v2/rosipal01a.html" class="external-link uri">http://www.jmlr.org/papers/v2/rosipal01a.html</a>.</li>
<li>Kernel PLS Regression II (double RKHS). <em>IEEE Transactions on
Neural Networks and Learning Systems</em>, <a href="doi:10.1109/TNNLS.2019.2932014" class="uri">doi:10.1109/TNNLS.2019.2932014</a>.</li>
<li>KF-PLS (2024) <a href="doi:10.1016/j.chemolab.2024.104024" class="uri">doi:10.1016/j.chemolab.2024.104024</a>
</li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Frederic Bertrand, Myriam Maumy.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
