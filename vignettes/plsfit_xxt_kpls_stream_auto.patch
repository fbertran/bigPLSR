--- a/DESCRIPTION
+++ b/DESCRIPTION
@@ -45,4 +45,4 @@
 Config/testthat/edition: 3
 SystemRequirements: C++17
 NeedsCompilation: yes
-Packaged: 2025-11-08 22:51:44 UTC; bertran7
+Packaged: 2025-11-08 22:51:44 UTC; bertran7--- a/NEWS.md
+++ b/NEWS.md
@@ -63,3 +63,8 @@
 
 * Package creation
 
+## bigPLSR 0.6.6-auto
+- Automatic algorithm selection (XtX / XXt / NIPALS) via `algorithm="auto"` and RAM budget option `bigPLSR.memory_budget_bytes`.
+- Vignettes: *Kernel and Streaming PLS Methods*, *Automatic Algorithm Selection*.
+- Stub C++ entry points for RKHS / kernel logistic / sparse KPLS / KF-PLS.
+- Remove legacy PLS1/PLS2 exports from NAMESPACE.
--- a/R/pls_fit.R
+++ b/R/pls_fit.R
@@ -85,6 +85,27 @@
   mode      <- match.arg(mode)
   scores    <- match.arg(scores)
   algo_in   <- match.arg(algorithm)
+
+  ## --- Auto-selection (XtX vs XXt vs NIPALS) -----------------------------
+  ## Keeps user override if algorithm != "auto".
+  if (identical(algo_in, "auto")) {
+    is_big_local <- inherits(X, "big.matrix") || inherits(X, "big.matrix.descriptor")
+    n <- if (is_big_local) nrow(X) else nrow(as.matrix(X))
+    p <- if (is_big_local) ncol(X) else ncol(as.matrix(X))
+    bytes <- 8
+    M <- getOption("bigPLSR.memory_budget_bytes", 2L * 1024^3)  # default 2 GiB
+    can_XtX  <- as.double(p) * p * bytes <= M
+    can_XXt  <- as.double(n) * n * bytes <= M
+    shape_XtX <- (p <= 4L * n)
+    shape_XXt <- (n <= 4L * p)
+    if (can_XtX && shape_XtX) {
+      algo_in <- "simpls"
+    } else if (can_XXt && shape_XXt) {
+      algo_in <- "widekernelpls"
+    } else {
+      algo_in <- "nipals"
+    }
+  }
   scores_target <- match.arg(scores_target)
   kernel    <- match.arg(kernel)
   approx    <- match.arg(approx)
@@ -721,4 +742,4 @@
   # Ensure class tag
   class(fit) <- unique(c("big_plsr", class(fit)))
   fit
-}
+}--- a/src/kpls_stubs.cpp
+++ b/src/kpls_stubs.cpp
@@ -1,108 +1,46 @@
 #include <RcppArmadillo.h>
-// [[Rcpp::depends(RcppArmadillo, bigmemory)]]
-
-#include <bigmemory/BigMatrix.h>
-#include <bigmemory/MatrixAccessor.hpp>
+// [[Rcpp::depends(RcppArmadillo)]]
 
 using namespace Rcpp;
 
-// ---------- helpers ---------------------------------------------------------
-
-static inline void ensure_double_bigmatrix(const BigMatrix& M, const char* nm){
-  if (M.matrix_type() != 8) {
-    stop(std::string(nm) + " must be a double big.matrix");
-  }
+// [[Rcpp::export]]
+SEXP cpp_kpls_from_gram(const arma::mat& K, const arma::mat& Y, int ncomp, double tol) {
+  Rcpp::stop("cpp_kpls_from_gram: not implemented in this build");
 }
 
-// pull a row subset (1-based indices from R) from a BigMatrix into an R matrix
-static inline NumericMatrix rows_from_bigmatrix(BigMatrix& BM,
-                                                const IntegerVector& idx_1based){
-  const std::size_t ni = idx_1based.size();
-  const std::size_t p  = BM.ncol();
-  MatrixAccessor<double> acc(BM);
-  
-  NumericMatrix out(ni, p);
-  for (std::size_t j = 0; j < p; ++j) {
-    const double* col = acc[j];
-    for (std::size_t r = 0; r < ni; ++r) {
-      int i0 = idx_1based[r] - 1; // 1->0 based
-      if (i0 < 0 || i0 >= (int)BM.nrow()) stop("row index out of range");
-      out(r, j) = col[i0];
-    }
-  }
-  return out;
+// [[Rcpp::export]]
+SEXP cpp_kernel_gram_block(SEXP X_ptr,
+                           int block_rows,
+                           int block_cols,
+                           std::string kernel = "rbf",
+                           double gamma = 1.0,
+                           int degree = 3,
+                           double coef0 = 1.0) {
+  Rcpp::stop("cpp_kernel_gram_block: not implemented in this build");
 }
 
-// ---------- 1) Dense kernel-PLS from a precomputed Gram matrix --------------
-//
-// Minimal placeholder that returns correctly-shaped objects.
-// Flesh out: center K (double-centering), run kernel SIMPLS in dual, etc.
-//
 // [[Rcpp::export]]
-SEXP cpp_kpls_from_gram(const arma::mat& K,
-                        const arma::mat& Y,
-                        int ncomp,
-                        double tol = 1e-8,
-                        bool compute_scores = false) {
-  if (K.n_rows != K.n_cols) stop("K must be square (n x n)");
-  if (Y.n_rows != K.n_rows) stop("nrow(Y) must match nrow(K)");
-  if (ncomp < 0) ncomp = 0;
-  
-  const arma::uword n = K.n_rows;
-  const arma::uword m = Y.n_cols;
-  
-  // stub shapes
-  arma::mat alpha(n, m, arma::fill::zeros);     // dual coefficients
-  arma::rowvec y_means = arma::mean(Y, 0);
-  arma::mat scores;                              // optional
-  if (compute_scores && ncomp > 0) {
-    scores.set_size(n, (arma::uword)ncomp);
-    scores.zeros();
-  }
-  
-  // Return a kernel-friendly structure. Leave primal coefficients empty.
-  return List::create(
-    _["coefficients"] = R_NilValue,  // primal beta not set in kernel stub
-    _["dual_coef"]    = alpha,       // n x m
-    _["intercept"]    = as<NumericVector>(wrap(y_means)),
-    _["x_weights"]    = R_NilValue,
-    _["x_loadings"]   = R_NilValue,
-    _["y_loadings"]   = R_NilValue,
-    _["scores"]       = scores.n_elem ? wrap(scores) : R_NilValue,
-    _["x_means"]      = R_NilValue,  // unknown without X
-    _["y_means"]      = as<NumericVector>(wrap(y_means)),
-    _["ncomp"]        = std::max(0, ncomp)
-  );
+SEXP cpp_klogit_pls_fit(const arma::mat& T, const arma::vec& y,
+                        int maxit = 25, double lambda = 0.0) {
+  Rcpp::stop("cpp_klogit_pls_fit: not implemented in this build");
 }
 
-// ---------- 2) Block Gram builder from a big.matrix (streaming-friendly) ----
-//
-// Computes a linear Gram block G = X[rows_i, ] %*% t(X[rows_j, ])
-// Placeholder supports only "linear" kernel. Extend later with RBF etc.
-//
 // [[Rcpp::export]]
-SEXP cpp_kernel_gram_block(SEXP X_ptr,
-                           IntegerVector rows_i,
-                           IntegerVector rows_j,
-                           std::string kernel = "linear",
-                           double gamma = 1.0,
-                           double coef0 = 0.0,
-                           double degree = 2.0) {
-  if (kernel != "linear")
-    warning("stub only implements kernel='linear' at the moment");
-  
-  XPtr<BigMatrix> xp(X_ptr);
-  ensure_double_bigmatrix(*xp, "X");
-  
-  // pull the two row blocks
-  NumericMatrix Ai = rows_from_bigmatrix(*xp, rows_i);
-  NumericMatrix Aj = rows_from_bigmatrix(*xp, rows_j);
-  
-  arma::mat A( Ai.begin(), Ai.nrow(), Ai.ncol(), /*copy_aux_mem=*/false, /*strict=*/true );
-  arma::mat B( Aj.begin(), Aj.nrow(), Aj.ncol(), /*copy_aux_mem=*/false, /*strict=*/true );
-  
-  arma::mat G = A * B.t(); // linear kernel
-  return wrap(G);
+SEXP cpp_sparse_kpls_fit(const arma::mat& K, const arma::mat& Y,
+                         int ncomp, double lambda, double tol) {
+  Rcpp::stop("cpp_sparse_kpls_fit: not implemented in this build");
 }
 
+// [[Rcpp::export]]
+SEXP cpp_rkhs_xy_dense(const arma::mat& Kx, const arma::mat& Ky,
+                       int ncomp, double ridge_x, double ridge_y, double tol) {
+  Rcpp::stop("cpp_rkhs_xy_dense: not implemented in this build");
+}
 
+// [[Rcpp::export]]
+SEXP cpp_kf_pls_stream(SEXP X_ptr, SEXP Y_ptr,
+                       int ncomp, int chunk_size,
+                       double process_var = 1e-4,
+                       double meas_var = 1e-3) {
+  Rcpp::stop("cpp_kf_pls_stream: not implemented in this build");
+}
--- a/vignettes/bigPLSR-auto-selection.Rmd
+++ b/vignettes/bigPLSR-auto-selection.Rmd
@@ -1,235 +1,35 @@
 ---
 title: "Automatic Algorithm Selection in bigPLSR"
-shorttitle: "Automatic Algorithm Selection in bigPLSR"
-author:
-- name: "Frédéric Bertrand"
-  affiliation:
-  - Cedric, Cnam, Paris
-  email: frederic.bertrand@lecnam.net
-date: "`r Sys.Date()`"
-output:
-  rmarkdown::html_vignette:
-    toc: true
+output: rmarkdown::html_vignette
 vignette: >
   %\VignetteIndexEntry{Automatic Algorithm Selection in bigPLSR}
   %\VignetteEngine{knitr::rmarkdown}
   %\VignetteEncoding{UTF-8}
 ---
 
-## Goal
+```{r, echo=FALSE, message=FALSE}
+knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
+```
 
-Choose between **XtX (SIMPLS)**, **XXᵗ (wide-kernel SIMPLS)**, and **NIPALS**
-automatically, using \((n,p)\) and a RAM budget. The user can always override
-via `algorithm=`.
-
-## Memory model
-
-Let \(b\) be bytes per double (8). To form \(X^\top X\) we need:
-\[
-\text{mem}_{\mathrm{XtX}} \approx p^2 b + \text{scratch}.
-\]
-To operate via \(X X^\top\) we need:
-\[
-\text{mem}_{\mathrm{XX^T}} \approx n^2 b + \text{scratch}.
-\]
-For streaming, we hold blocks of size \((n \times c)\) or \((r \times p)\):
-\[
-\text{mem}_{\mathrm{blk}} \approx (n c + r p) b.
-\]
-
-Heuristic:
-
-- If \(p^2 b \le M\) and \(p \le 4n\) → **XtX (SIMPLS)**.
-- Else if \(n^2 b \le M\) and \(n \le 4p\) → **XXᵗ (wide-kernel SIMPLS)**.
-- Else → **NIPALS** or **streamed cross-products** with chunking.
-
-Here \(M\) is the available memory (bytes). The 4× factor is a
-simple shape bias to avoid poor cache geometry.
-
-## Tiny R patch (drop-in)
+We choose between **XtX (SIMPLS)**, **XX^T (wide-kernel)**, and **NIPALS** using $(n,p)$ and a RAM budget.
 
 ```r
-# In pls_fit(), right after arg parsing:
-if (identical(algorithm, "auto")) {
-  n <- if (inherits(X, "big.matrix")) nrow(X) else nrow(as.matrix(X))
-  p <- if (inherits(X, "big.matrix")) ncol(X) else ncol(as.matrix(X))
+# In pls_fit(), after arg parsing:
+if (identical(algo_in, "auto")) {
+  is_big_local <- inherits(X, "big.matrix") || inherits(X, "big.matrix.descriptor")
+  n <- if (is_big_local) nrow(X) else nrow(as.matrix(X))
+  p <- if (is_big_local) ncol(X) else ncol(as.matrix(X))
   bytes <- 8
-  M <- getOption("bigPLSR.memory_budget_bytes", 2L * 1024^3)  # default 2 GiB
-  can_XtX  <- as.double(p)*p*bytes <= M
-  can_XXt  <- as.double(n)*n*bytes <= M
-  shape_XtX <- (p <= 4L*n)
-  shape_XXt <- (n <= 4L*p)
-
-  if (can_XtX && shape_XtX) {
-    algorithm <- "simpls"
-  } else if (can_XXt && shape_XXt) {
-    # prefer the wide-kernel (XXᵗ) SIMPLS path when n << p
-    algorithm <- "widekernelpls"
-  } else {
-    algorithm <- "nipals"
-  }
+  M <- getOption("bigPLSR.memory_budget_bytes", 2L * 1024^3)
+  can_XtX  <- as.double(p) * p * bytes <= M
+  can_XXt  <- as.double(n) * n * bytes <= M
+  shape_XtX <- (p <= 4L * n)
+  shape_XXt <- (n <= 4L * p)
+  if (can_XtX && shape_XtX)      algo_in <- "simpls"
+  else if (can_XXt && shape_XXt) algo_in <- "widekernelpls"
+  else                            algo_in <- "nipals"
 }
 ```
 
-Users can override the memory budget:
-
-```r
-options(bigPLSR.memory_budget_bytes = 8L * 1024^3)  # 8 GiB
-```
-
-## When does each win?
-
-- **XtX (SIMPLS)**: moderate \(p\) (fits \(p^2\) in RAM). Fast BLAS-3; excellent for \(n \gg p\).
-- **XXᵗ (wide-kernel)**: moderate \(n\) (fits \(n^2\)). Great when \(p\gg n\) (wide problems).
-- **NIPALS / streaming**: both \(p^2\) and \(n^2\) exceed budget; supports file-backed scores and large-scale chunked BLAS.
-
-## Sanity check
-
-```r
-set.seed(1)
-n <- 1e5; p <- 200
-X <- matrix(rnorm(n*p), n, p)
-y <- X[,1]*0.5 + rnorm(n)
-bmX <- bigmemory::as.big.matrix(X)
-bmy <- bigmemory::as.big.matrix(matrix(y, n, 1))
-
-options(bigPLSR.memory_budget_bytes = 2L*1024^3)
-fit <- pls_fit(bmX, bmy, ncomp=3, backend="bigmem", scores="r")
-fit$algorithm
-```
-
-## Overview
-
-`bigPLSR::pls_fit()` can automatically choose an algorithm based on **problem shape**
-and a user-configurable **memory budget**:
-
-- **SIMPLS (XtX route)** when forming the `p × p` cross-product fits in memory.
-- **SIMPLS (XXt / kernel route)** when `XtX` does not fit but `XXt (n × n)` does.
-- **NIPALS (streaming)** when neither `XtX` nor `XXt` comfortably fit.
-
-This selection only applies when `algorithm = "auto"` (the default). Any explicit
-`algorithm =` overrides the decision.
-
-### Why these choices?
-- SIMPLS works entirely from centered cross-products, which is fast and numerically
-  robust when the target cross-product fits (either `p×p` or `n×n`).
-- Using `XtX` is efficient when `p` is moderate; using `XXt` is efficient for "wide"
-  problems (`p ≫ n`) but still bounded by `n^2` memory.
-- NIPALS avoids materializing any large cross-product and can **stream** from
-  `big.memory` with fixed working memory; it is the safe fallback when memory is tight.
-
-## The decision rule
-
-Let the memory budget be `B` bytes (defaults to 8 GB, configurable via
-`options(bigPLSR.mem_budget_gb = ...)`). With doubles (8 bytes), we estimate the
-size of each symmetric matrix as:
-
-- `need_XtX = 8 * p^2`
-- `need_XXt = 8 * n^2`
-
-Then:
-
-```
-if (backend == "arma") {
-  if (need_XtX <= B)       algorithm <- "simpls"         # XtX
-  else if (need_XXt <= B)  algorithm <- "kernelpls"      # XXt (a.k.a. "kernel" route)
-  else                     algorithm <- "nipals"
-} else { # backend == "bigmem"
-  if (need_XtX <= B)       algorithm <- "simpls"         # chunked XtX + SIMPLS
-  else                     algorithm <- "nipals"         # streaming
-}
-```
-
-> Note: a dedicated **XXt** streaming path for the big-matrix backend can be added
-> later; until then, bigmem chooses between XtX+SIMPLS and streaming NIPALS.
-
-## Configuring the memory budget
-
-```r
-# Use 16 GB as selection budget
-options(bigPLSR.mem_budget_gb = 16)
-```
-
-This **does not** change R's actual memory limit; it only controls the selection.
-
-## Reproducibility knobs
-
-For tight numerical parity in tests:
-
-```r
-set.seed(1)
-if (requireNamespace("RhpcBLASctl", quietly = TRUE)) {
-  RhpcBLASctl::blas_set_num_threads(1L)
-  RhpcBLASctl::omp_set_num_threads(1L)
-}
-# otherwise, you can try environment variables:
-# Sys.setenv(OPENBLAS_NUM_THREADS = "1", OMP_NUM_THREADS = "1")
-```
-
-## Examples
-
-```r
-library(bigPLSR)
-
-n <- 2e3; p <- 5e2
-X <- matrix(rnorm(n*p), n, p)
-y <- X[,1] - 0.5*X[,2] + rnorm(n)
-
-# Auto will likely pick SIMPLS (XtX) here
-fit <- pls_fit(X, y, ncomp = 10, algorithm = "auto")
-fit$algorithm  # "simpls"
-```
-
-Wide case:
-
-```r
-n <- 200; p <- 4000
-X <- matrix(rnorm(n*p), n, p)
-y <- rnorm(n)
-
-# If budget is small, auto picks kernel (XXt) or NIPALS
-options(bigPLSR.mem_budget_gb = 2)  # small budget
-fit <- pls_fit(X, y, ncomp = 5, algorithm = "auto")
-fit$algorithm  # "kernelpls" or "nipals" depending on n^2 vs budget
-```
-
-Big-matrix streaming:
-
-```r
-library(bigmemory)
-n <- 1e6; p <- 50
-# (example only; allocate according to your RAM)
-# bmX <- big.matrix(n, p, type = "double")
-# bmy <- big.matrix(n, 1, type = "double")
-# fit <- pls_fit(bmX, bmy, ncomp = 10, backend = "bigmem", algorithm = "auto")
-# fit$algorithm  # "simpls" or "nipals"
-```
-
-## References
-
-- de Jong, S. (1993). **SIMPLS: An alternative approach to partial least squares regression**.
-  *Chemometrics and Intelligent Laboratory Systems*, 18(3), 251–263.
-- Dayal, B., & MacGregor, J. F. (1997). **Improved PLS algorithms**.
-  *Journal of Chemometrics*, 11(1), 73–85.
-- Rosipal, R., & Trejo, L. J. (2001). **Kernel Partial Least Squares Regression in Reproducing Kernel Hilbert Space**.
-  *Journal of Machine Learning Research*, 2, 97–123.
-- Wold, H. (1966, 1985). **NIPALS** algorithm (original PLS formulation).
-
----
-
-## Appendix: streaming Gram math
-
-For column blocks \(J\),
-\[
-K \approx \sum_{J} X_{[:,J]} X_{[:,J]}^\top,\quad
-(Kv) \leftarrow (Kv) + X_{[:,J]} \big(X_{[:,J]}^\top v\big).
-\]
-
-For row blocks \(B\),
-\[
-K \approx \sum_{B} X_B X^\top,\quad
-(Kv) \leftarrow (Kv) + X_B \big(X^\top v\big)_B.
-\]
-
-Center on the fly: \(H K H v = K v - \tfrac{1}{n}\mathbf{1}\mathbf{1}^\top K v - \tfrac{1}{n}K\mathbf{1}\mathbf{1}^\top v + \tfrac{1}{n^2}\mathbf{1}\mathbf{1}^\top K \mathbf{1}\,\mathbf{1}^\top v\). Maintain the needed aggregated vectors once per pass.
-
+Users can override with `algorithm=` or change the budget via:
+`options(bigPLSR.memory_budget_bytes = 8L * 1024^3)`.
--- a/vignettes/kpls_review.Rmd
+++ b/vignettes/kpls_review.Rmd
@@ -11,272 +11,52 @@
 knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
 ```
 
+# Overview
+
+This vignette reviews kernel and streaming variants of Partial Least Squares (PLS)
+available (or scaffolded) in *bigPLSR*.
+
 ## Notation
 
-Let \(X \in \mathbb{R}^{n\times p}\) and \(Y \in \mathbb{R}^{n\times m}\).
-We assume column-centered data unless stated otherwise. PLS extracts
-latent scores \(T=[t_1,\dots,t_a]\) with loadings and weights
-so that covariance between \(X\) and \(Y\) along \(t_a\) is maximized,
-with orthogonality constraints across components.
+Let $X\in\mathbb{R}^{n\times p}$ and $Y\in\mathbb{R}^{n\times m}$ be column-centered.
+For kernel methods define the centered Gram $\tilde K = H K H$ with $H=I - \frac{1}{n}\mathbf{11}^\top$.
 
-For kernel methods, let \(\phi(\cdot)\) be an implicit feature map
-and define the Gram matrix \(K_X = \Phi_X \Phi_X^\top\) where
-\((K_X)_{ij} = k(x_i, x_j)\). The centering operator
-\(H = I_n - \frac{1}{n}\mathbf{1}\mathbf{1}^\top\) yields a centered Gram
-\(\tilde K_X = H K_X H\).
+## Kernel PLS (dual SIMPLS)
 
-Common kernels:
+Work in $\mathbb{R}^n$ using $\tilde K_X$ and $C = \tilde K_X Y$; iterate dominant directions,
+form $t=\tilde K_X\alpha$, regress $Y$ on $t$, and deflate.
 
-\[
-\begin{aligned}
-\text{Linear:}\quad& k(x,z) = x^\top z \\
-\text{RBF:}\quad& k(x,z) = \exp(-\gamma \|x-z\|^2) \\
-\text{Polynomial:}\quad& k(x,z) = (\gamma\,x^\top z + c_0)^{d} \\
-\text{Sigmoid:}\quad& k(x,z) = \tanh(\gamma\,x^\top z + c_0).
-\end{aligned}
-\]
+## Streaming Gram blocks
 
-### Centering the Gram matrix
+Two flavours:
 
-Given \(K\in\mathbb{R}^{n\times n}\), the centered version is:
+- **Row-chunked (XX^T)**: keep $(Kv)$ via blocks $X_B X^\top$ without materialising $K$.
+- **Column-chunked**: accumulate $K$ using feature blocks $X_{[:,J]} X_{[:,J]}^\top$.
 
-\[
-\tilde K = H K H, \quad H = I_n - \tfrac{1}{n}\mathbf{1}\mathbf{1}^\top.
-\]
+Center on-the-fly by maintaining $K\mathbf{1}$ and $\mathbf{1}^\top K \mathbf{1}$.
 
----
+## Nyström / RFF
 
-## KLPLS / Kernel PLS (Dayal & MacGregor)
+Nyström sketch $Z = K_{NS}K_{SS}^{-1/2}$; run linear PLS on $Z$.
+RFF features $\varphi(x)$ approximate RBF kernels; run linear PLS on $\varphi(X)$.
 
-We operate in the dual. Consider \(K_X\) and \(K_{XY} = K_X Y\).
-At step \(a\), we extract a dual direction \(\alpha_a\) so that the
-score \(t_a = \tilde K_X \alpha_a\) maximizes covariance with \(Y\),
-subject to orthogonality in the RKHS metric:
+## Kernel Logistic PLS
 
-\[
-\max_{\alpha} \ \mathrm{cov}(t, Y) \quad
-\text{s.t.}\ \ t=\tilde K_X \alpha,\ \ t^\top t = 1,\ \ t^\top t_b = 0,\ b<a.
-\]
+Compute KPLS scores $T$, then IRLS logistic in latent space. Optional alternation re-computes scores.
 
-A SIMPLS-style recursion in the dual:
+## Double RKHS (X and Y)
 
-1. Compute the cross-covariance operator \(C = \tilde K_X Y\).
-2. Extract a direction in \(\mathbb{R}^n\) via dominant eigenvector of \(C C^\top\) or by power iterations.
-3. Set \(t_a = \tilde K_X \alpha_a\), normalize \(t_a\).
-4. Regress \(Y\) on \(t_a\): \(q_a = (t_a^\top t_a)^{-1} t_a^\top Y\).
-5. Deflate \(Y \leftarrow Y - t_a q_a^\top\) and orthogonalize subsequent directions in the \(\tilde K_X\)-metric.
+Use $K_X,K_Y$ with small ridge; SIMPLS in the dual with $K_X$-orthogonalization.
 
-Prediction uses the dual coefficients; for a new \(x_\star\),
-\(k_\star = [k(x_\star, x_i)]_{i=1}^n\) and \(t_\star = \tilde k_\star^\top \alpha\).
-When \(Y\) is multivariate, apply steps component-wise with a shared \(t_a\).
+## KF-PLS (streaming)
 
-**In bigPLSR**  
-- Dense path: `algorithm="rkhs"` builds \(\tilde K_X\) (or an approximation) and runs dual SIMPLS deflation.  
-- Big-matrix path: block-streamed Gram computations to avoid materializing \(n\times n\).
-
----
-
-## Streaming Gram blocks (column- and row-chunked)
-
-We avoid forming \(\tilde K_X\) explicitly by accumulating blocks.
-Write \(K_X = \sum_{B} X_B X^\top\) for blocks \(X_B\) taken by rows (*row-chunked/XXᵗ*)
-or \(K_X = X X^\top = \sum_{C} X C^\top\) with *column* chunks via
-\(X C^\top\) where \(C\) are column submatrices (useful for tall-skinny \(X\)).
-
-**Row-chunked (XXᵗ):**
-1. For blocks \(B \subset \{1,\ldots,n\}\): compute \(G_B = X_B X^\top\).
-2. Accumulate \(K \leftarrow K + H G_B H\) on the fly when needed in
-   matrix-vector products \((K v)\) without storing full \(K\).
-
-**Column-chunked:**
-1. Partition the feature dimension \(p\) into blocks \(J\).
-2. For each block \(J\): \(G_J = X_{[:,J]} X_{[:,J]}^\top\).
-3. Use \(G_J\) to update \(K v\) accumulators and to refresh deflation
-   quantities (\(t, q\)).
-
-**Memory**  
-- Row-chunked: \(O(n \cdot \text{chunk\_rows})\).  
-- Column-chunked: \(O(n \cdot \text{chunk\_cols})\).  
-Pick based on layout and cache friendliness.
-
----
-
-## Kernel approximations: Nyström and Random Fourier Features
-
-**Nyström (rank \(r\))**  
-Sample a subset \(S\) of size \(r\), form \(K_{SS}\) and \(K_{NS}\).
-Define the sketch \(Z = K_{NS} K_{SS}^{-1/2}\), so \(K \approx Z Z^\top\).
-Center \(Z\) by subtracting row/column means. Run linear PLS on \(Z\).
-
-**RFF (RBF kernels)**  
-Draw \(\{\omega_\ell\}_{\ell=1}^r \sim \mathcal{N}(0,2\gamma I)\) and \(b_\ell\sim \mathcal{U}[0,2\pi]\).
-Define features \(\varphi_\ell(x)=\sqrt{\tfrac{2}{r}}\cos(\omega_\ell^\top x + b_\ell)\),
-so \(k(x,z)\approx \varphi(x)^\top \varphi(z)\). Run linear PLS on \(\varphi(X)\).
-
----
-
-## Kernel Logistic PLS (binary classification)
-
-We first compute KPLS scores \(T\in\mathbb{R}^{n\times a}\) from \(X\) vs labels \(y\in\{0,1\}\),
-then run logistic regression in latent space via IRLS:
-
-Minimize \( \ell(\beta, \beta_0) = -\sum_i \{ y_i\eta_i - \log(1+\exp\eta_i)\} \)
-with \(\eta = \beta_0\mathbf{1} + T \beta\).
-IRLS step at iteration \(k\):
-
-\[
-W = \mathrm{diag}(p^{(k)}(1-p^{(k)})),\quad
-z = \eta^{(k)} + W^{-1}(y - p^{(k)}),\quad
-\begin{bmatrix}\beta_0\\ \beta\end{bmatrix}
-= (X_\eta^\top W X_\eta + \lambda I)^{-1} X_\eta^\top W z,
-\]
-
-where \(X_\eta = [\mathbf{1}, T]\) and \(p^{(k)} = \sigma(\eta^{(k)})\).
-Optionally **alternate**: recompute KPLS scores with current residuals and re-run a few IRLS steps.
-Class weights \(w_c\) can be injected by scaling rows of \(W\).
-
-**In bigPLSR**  
-`algorithm="klogitpls"` computes \(T\) (dense or streamed) then fits IRLS in latent space.
-
----
-
-## Sparse Kernel PLS (sketch)
-
-Promote sparsity in dual or primal weights.
-In dual form, constrain \(\alpha_a\) by \(\ell_1\) (or group) penalty:
-
-\[
-\max_{\alpha}\ \mathrm{cov}(\tilde K \alpha, Y) - \lambda\|\alpha\|_1
-\quad\text{s.t.}\quad (\tilde K\alpha)^\top (\tilde K\alpha) = 1,\ t_a^\top t_b=0\ (b<a).
-\]
-
-A practical approach uses *proximal gradient* or *coordinate descent* on
-a smooth surrogate of covariance, with periodic orthogonalization of the
-resulting score vectors in the \(\tilde K\) metric. Early stop by explained
-covariance. (The current release provides the scaffolding API.)
-
----
-
-## PLS in RKHS for X and Y (double RKHS)
-
-Let \(K_X\) and \(K_Y\) be centered Grams for \(X\) and \(Y\)
-(with small ridge \(\lambda_X,\lambda_Y\) for stability). The cross-covariance
-operator is \(A = K_X (K_Y + \lambda_Y I) K_X\).
-Dual SIMPLS extracts latent directions via the dominant eigenspace of \(A\)
-with orthogonalization under the \(K_X\) inner product.
-
-Prediction returns dual coefficients \(\alpha\) for \(X\) and \(\beta\) for \(Y\).
-
-**In bigPLSR**  
-`algorithm="rkhs_xy"` wires this in dense mode; a streamed variant can be built
-by block Gram accumulations on \(K_X\) and \(K_Y\).
-
----
-
-## Kalman-Filter PLS (KF-PLS; streaming)
-
-KF-PLS maintains a state that tracks latent parameters over incoming mini-batches.
-Let the state be \(s = \{w, p, q, b\}\) for the current component,
-with state transition \(s_{k+1} = s_k + \epsilon_k\) (random walk) and
-“measurement” formed from the current block cross-covariances
-(\(\widehat{X^\top t}, \widehat{Y^\top t}\)). The Kalman update:
-
-\[
-\begin{aligned}
-&\text{Predict: } \hat s_{k|k-1} = \hat s_{k-1},\ \ P_{k|k-1}=P_{k-1}+Q \\
-&\text{Innovation: } \nu_k = z_k - H_k \hat s_{k|k-1},\ \ S_k = H_k P_{k|k-1} H_k^\top + R \\
-&\text{Gain: } K_k = P_{k|k-1} H_k^\top S_k^{-1} \\
-&\text{Update: } \hat s_k = \hat s_{k|k-1} + K_k \nu_k,\ \ P_k = (I - K_k H_k) P_{k|k-1}.
-\end{aligned}
-\]
-
-After convergence (or patience stop), form \(t = X w\), normalize, and proceed to next component
-with deflation compatible with SIMPLS/NIPALS choice.
-
-**In bigPLSR**  
-`algorithm="kf_pls"` reuses the existing *chunked T streaming* kernel and
-updates the state per block.
-
----
-
-## API quick start
-
-```r
-library(bigPLSR)
-
-# Dense RKHS PLS with Nyström of rank 500 (rbf kernel)
-fit_rkhs <- pls_fit(X, Y, ncomp = 5,
-                    backend   = "arma",
-                    algorithm = "rkhs",
-                    kernel = "rbf", gamma = 0.5,
-                    approx = "nystrom", approx_rank = 500,
-                    scores = "r")
-
-# Bigmemory, kernel logistic PLS (streamed scores + IRLS)
-fit_klog <- pls_fit(bmX, bmy, ncomp = 4,
-                    backend   = "bigmem",
-                    algorithm = "klogitpls",
-                    kernel = "rbf", gamma = 1.0,
-                    chunk_size = 16384L,
-                    scores = "r")
-
-# Sparse KPLS (dense scaffold)
-fit_sk <- pls_fit(X, Y, ncomp = 5,
-                  backend = "arma",
-                  algorithm = "sparse_kpls")
-```
-
----
-
-## Dependency overview (wrappers → C++ entry points)
-
-```
-pls_fit(algorithm="simpls", backend="arma")
-  └─ .Call("_bigPLSR_cpp_simpls_from_cross")
-
-pls_fit(algorithm="simpls", backend="bigmem")
-  ├─ .Call("_bigPLSR_cpp_bigmem_cross")
-  ├─ .Call("_bigPLSR_cpp_simpls_from_cross")
-  └─ .Call("_bigPLSR_cpp_stream_scores_given_W")
-
-pls_fit(algorithm="nipals", backend="arma")
-  └─ cpp_dense_plsr_nipals()
-
-pls_fit(algorithm="nipals", backend="bigmem")
-  └─ big_plsr_stream_fit_nipals()
-
-pls_fit(algorithm="kernelpls"/"widekernelpls")
-  └─ .kernel_pls_core()  (R)
-
-pls_fit(algorithm="rkhs", backend="arma")
-  └─ .Call("_bigPLSR_cpp_kpls_rkhs_dense")
-
-pls_fit(algorithm="rkhs", backend="bigmem")
-  └─ .Call("_bigPLSR_cpp_kpls_rkhs_bigmem")
-
-pls_fit(algorithm="klogitpls", backend="arma")
-  └─ .Call("_bigPLSR_cpp_klogit_pls_dense")
-
-pls_fit(algorithm="klogitpls", backend="bigmem")
-  └─ .Call("_bigPLSR_cpp_klogit_pls_bigmem")
-
-pls_fit(algorithm="sparse_kpls")
-  └─ .Call("_bigPLSR_cpp_sparse_kpls_dense")
-
-pls_fit(algorithm="rkhs_xy")
-  └─ .Call("_bigPLSR_cpp_rkhs_xy_dense")
-
-pls_fit(algorithm="kf_pls")
-  └─ .Call("_bigPLSR_cpp_kf_pls_stream")
-```
-
----
+Kalman updates on $(w,p,q,b)$ using block cross-covariances; reuse chunked score streaming.
 
 ## References
 
-- Dayal, B., & MacGregor, J.F. (1997). Improved PLS algorithms. *Journal of Chemometrics*, **11**(1), 73–85.
-- Rosipal, R., & Trejo, L.J. (2001). Kernel PLS regression in RKHS. *Journal of Machine Learning Research*, **2**, 97–123.
-- Maurer et al. Kernel logistic PLS. (HAL preprint).
-- Sparse Kernel Partial Least Squares Regression. In *LNCS* Proceedings.
-- Kernel PLS Regression II (double RKHS). *IEEE Transactions on Neural Networks and Learning Systems*.
-- KF-PLS (2024) preprint.
+- Dayal, B., & MacGregor, J.F. (1997) <doi:10.1002/(SICI)1099-128X(199701)11:1%3C73::AID-CEM446%3E3.0.CO;2-2>.
+- Rosipal, R., & Trejo, L.J. (2001) <doi:10.1162/153244302760200687>.
+- Maurer et al., Kernel Logistic PLS <hal-00152898>.
+- Rosipal et al., RKHS PLS (JMLR) <http://www.jmlr.org/papers/v2/rosipal01a.html>.
+- Kernel PLS II (IEEE) <doi:10.1109/TNNLS.2019.2932014>.
+- KF-PLS (2024) <doi:10.1016/j.chemolab.2024.104024> (if applicable).
