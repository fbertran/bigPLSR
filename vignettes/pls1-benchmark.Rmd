---
title: "Benchmarking PLS1 Implementations"
shorttitle: "Benchmarking PLS1 Implementations"
author:
- name: "Frédéric Bertrand"
  affiliation:
  - Cedric, Cnam, Paris
  email: frederic.bertrand@lecnam.net
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Benchmarking PLS1 Implementations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup_ops, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "figures/benchmarking-",
  fig.width = 7,
  fig.height = 5,
  dpi = 150,
  message = FALSE,
  warning = FALSE
)

LOCAL <- identical(Sys.getenv("LOCAL"), "TRUE")
```

```{r setup, message=FALSE}
library(bigPLSR)
library(bigmemory)
library(bench)
set.seed(123)
```

## Overview

The unified `pls_fit()` interface now drives both the dense and streaming
implementations of single-response partial least squares regression. This
vignette revisits the benchmarking workflow with the modern API and
introduces two complementary perspectives:

1. **Internal comparisons** that contrast the dense (in-memory) and
   streaming (big-memory) backends of `pls_fit()`.
2. **External references** recorded against popular packages such as
   `pls` and `mixOmics`. These results are stored in the package to keep
   the vignette lightweight while still documenting performance relative
   to the wider ecosystem.

The chunks tagged with `eval = LOCAL` are only executed when the
environment variable `LOCAL` is set to `TRUE`, allowing CRAN checks to
skip the more time-consuming benchmarks.

## Simulated data

We create a synthetic regression problem with a modest latent structure
and keep both dense and big-memory versions of the predictors and
response so they can be reused in the benchmarking chunks.

Here is an example with `n=4000` en `p=50`

```{r data-generation}
n <- 1500
p <- 80
ncomp <- 6

X <- bigmemory::big.matrix(nrow = n, ncol = p, type = "double")
X[,] <- matrix(rnorm(n * p), nrow = n)

y_vec <- scale(X[,] %*% rnorm(p) + rnorm(n))

y <- bigmemory::big.matrix(nrow = n, ncol = 1, type = "double")
y[,] <- y_vec

X[1:6, 1:6]
y[1:6,]
```

## Internal benchmarks

The following chunk compares dense vs. streaming fits for both SIMPLS and
NIPALS. The dense backend receives base R matrices, while the streaming
backend consumes the `big.matrix` objects directly.

```{r internal-benchmark, eval=LOCAL, cache=TRUE}
internal_bench <- bench::mark(
  dense_simpls = pls_fit(as.matrix(X[]), y_vec, ncomp = ncomp,
                         backend = "arma", algorithm = "simpls"),
  streaming_simpls = pls_fit(X, y, ncomp = ncomp, backend = "bigmem",
                             algorithm = "simpls", chunk_size = 512L),
  dense_nipals = pls_fit(as.matrix(X[]), y_vec, ncomp = ncomp,
                         backend = "arma", algorithm = "nipals"),
  streaming_nipals = pls_fit(X, y, ncomp = ncomp, backend = "bigmem",
                             algorithm = "nipals", chunk_size = 512L),
  iterations = 20,
  check = FALSE
)
internal_bench
```
```
The results highlight the trade-off between throughput and memory usage:
SIMPLS shines on dense matrices, whereas the streaming backend scales to
larger-than-memory inputs thanks to block processing.

## External references

To avoid heavy dependencies at build time we ship a pre-computed
benchmark dataset that contrasts `bigPLSR` with implementations from the
`pls` and `mixOmics` packages. The dataset was generated with the helper
script stored in `inst/scripts/external_pls_benchmarks.R`.

```{r external-benchmark}
data("external_pls_benchmarks", package = "bigPLSR")
subset(external_pls_benchmarks, task == "pls1")
```

The table reports median execution times (in milliseconds) and the number
of iterations per second for a representative single-response scenario.
The notes column indicates the additional packages required to reproduce
those measurements.

## Takeaways

* Use the dense backend when the full predictor matrix fits comfortably
  in RAM—the SIMPLS path delivers the best raw performance.
* Switch to the streaming backend for file-backed or
  larger-than-memory workloads; the cost in throughput is compensated by
  the ability to scale.
* Refer to the external benchmark table when choosing between bigPLSR
  and third-party implementations.
  